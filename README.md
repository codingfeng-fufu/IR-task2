# å­¦æœ¯æ ‡é¢˜åˆ†ç±»ç³»ç»Ÿ / Scholar Title Classification System

[![Python](https://img.shields.io/badge/Python-3.7+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.9+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ğŸ“‹ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªåŸºäºæœºå™¨å­¦ä¹ çš„å­¦æœ¯è®ºæ–‡æ ‡é¢˜åˆ†ç±»ç³»ç»Ÿï¼Œæ—¨åœ¨è¯†åˆ«ä» **CiteSeer æ•°æ®åº“**ä¸­é”™è¯¯æå–çš„å­¦æœ¯è®ºæ–‡æ ‡é¢˜ã€‚ç³»ç»Ÿé‡‡ç”¨å¤šæ¨¡å‹å¯¹æ¯”æ–¹æ³•ï¼Œå®ç°äº†ä»ä¼ ç»Ÿæœºå™¨å­¦ä¹ åˆ°æ·±åº¦å­¦ä¹ çš„å®Œæ•´æŠ€æœ¯æ ˆã€‚

### é—®é¢˜èƒŒæ™¯

CiteSeer æ˜¯ä¸€ä¸ªå­¦æœ¯è®ºæ–‡æœç´¢å¼•æ“ï¼Œåœ¨è‡ªåŠ¨æå–è®ºæ–‡æ ‡é¢˜æ—¶å¯èƒ½ä¼šå‡ºç°é”™è¯¯ï¼Œä¾‹å¦‚ï¼š
- âŒ **é”™è¯¯æå–**: `"Abstract......Introduction......References"`
- âŒ **å…ƒæ•°æ®æ··å…¥**: `"IEEE Transactions on Pattern Analysis, Vol. 25, pp. 1-15"`
- âŒ **æ ‡é¢˜ç‰‡æ®µ**: `"Table of Contents......Chapter 1"`
- âœ… **æ­£ç¡®æ ‡é¢˜**: `"Deep Learning for Computer Vision Applications"`

### æ ¸å¿ƒåŠŸèƒ½

- ğŸ¯ **äºŒåˆ†ç±»ä»»åŠ¡**: åŒºåˆ†æ­£ç¡®æ ‡é¢˜ (Label=1) å’Œé”™è¯¯æ ‡é¢˜ (Label=0)
- ğŸ”¬ **ä¸‰ç§æ¨¡å‹å¯¹æ¯”**: æœ´ç´ è´å¶æ–¯ã€Word2Vec+SVMã€BERT
- ğŸ“Š **å…¨é¢è¯„ä¼°**: å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰å¤šç»´åº¦æŒ‡æ ‡
- ğŸ“ˆ **å¯è§†åŒ–åˆ†æ**: æ€§èƒ½å¯¹æ¯”å›¾ã€æ··æ·†çŸ©é˜µã€t-SNEé™ç»´å¯è§†åŒ–
- âš¡ **ä¼˜åŒ–æŠ€æœ¯**: åŒ…å« 8 ç§ BERT ä¼˜åŒ–æŠ€æœ¯çš„é«˜çº§ç‰ˆæœ¬

---

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
IR-task2/
â”‚
â”œâ”€â”€ core/                           # æ ¸å¿ƒæ¨¡å—ç›®å½•
â”‚   â”œâ”€â”€ data_loader.py             # æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
â”‚   â”œâ”€â”€ naive_bayes_classifier.py  # æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ (TF-IDF)
â”‚   â”œâ”€â”€ word2vec_svm_classifier.py # Word2Vec + SVM åˆ†ç±»å™¨
â”‚   â”œâ”€â”€ bert_classifier.py         # æ ‡å‡† BERT åˆ†ç±»å™¨
â”‚   â”œâ”€â”€ evaluator.py               # æ¨¡å‹è¯„ä¼°æ¨¡å—
â”‚   â”œâ”€â”€ visualizer.py              # ç»“æœå¯è§†åŒ–æ¨¡å—
â”‚   â””â”€â”€ main_pipeline.py           # ä¸»æ‰§è¡Œæµç¨‹ (å®Œæ•´æµæ°´çº¿)
â”‚
â”œâ”€â”€ optimized_BERT.py              # ä¼˜åŒ–ç‰ˆ BERT (8ç§ä¼˜åŒ–æŠ€æœ¯)
â”œâ”€â”€ train_optimized_bert.py        # ä¼˜åŒ– BERT è®­ç»ƒè„šæœ¬
â”‚
â”œâ”€â”€ data/                          # æ•°æ®ç›®å½• (éœ€è‡ªè¡Œå‡†å¤‡)
â”‚   â”œâ”€â”€ positive.txt               # æ­£æ ·æœ¬ (æ­£ç¡®æ ‡é¢˜)
â”‚   â”œâ”€â”€ negative.txt               # è´Ÿæ ·æœ¬ (é”™è¯¯æ ‡é¢˜)
â”‚   â””â”€â”€ testSet-1000.xlsx          # æµ‹è¯•é›†
â”‚
â”œâ”€â”€ output/                        # è¾“å‡ºç›®å½• (è‡ªåŠ¨ç”Ÿæˆ)
â”‚   â”œâ”€â”€ model_comparison.png       # æ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾
â”‚   â”œâ”€â”€ confusion_matrices.png     # æ··æ·†çŸ©é˜µçƒ­å›¾
â”‚   â”œâ”€â”€ tsne_*.png                 # t-SNE å¯è§†åŒ–
â”‚   â”œâ”€â”€ evaluation_results.txt     # è¯„ä¼°ç»“æœ
â”‚   â””â”€â”€ predictions.json           # é¢„æµ‹ç»“æœ
â”‚
â”œâ”€â”€ README.md                      # é¡¹ç›®æ–‡æ¡£ (æœ¬æ–‡ä»¶)
â””â”€â”€ .gitignore                     # Git å¿½ç•¥é…ç½®
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè¦æ±‚

- **Python**: 3.7 æˆ–æ›´é«˜ç‰ˆæœ¬
- **æ“ä½œç³»ç»Ÿ**: Windows / Linux / macOS
- **ç¡¬ä»¶**:
  - CPU: 4 æ ¸ä»¥ä¸Šæ¨è
  - å†…å­˜: 8GB ä»¥ä¸Š
  - GPU: å¯é€‰ (ç”¨äºåŠ é€Ÿ BERT è®­ç»ƒï¼ŒCUDA æ”¯æŒ)

### 2. å®‰è£…ä¾èµ–

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd IR-task2

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ (æ¨è)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ–
venv\Scripts\activate     # Windows

# å®‰è£…ä¾èµ–åŒ…
pip install numpy pandas scikit-learn
pip install torch transformers  # PyTorch å’Œ Transformers
pip install gensim              # Word2Vec
pip install matplotlib seaborn  # å¯è§†åŒ–
pip install tqdm openpyxl       # å·¥å…·åº“
```

### 3. å‡†å¤‡æ•°æ®

åœ¨ `data/` ç›®å½•ä¸‹å‡†å¤‡ä»¥ä¸‹æ–‡ä»¶ï¼š

#### (1) è®­ç»ƒæ•°æ®
- **positive.txt**: æ­£ç¡®çš„å­¦æœ¯æ ‡é¢˜ï¼Œæ¯è¡Œä¸€ä¸ªæ ‡é¢˜
  ```
  Deep Learning for Computer Vision Applications
  Natural Language Processing with Transformer Models
  Introduction to Machine Learning Algorithms
  ...
  ```

- **negative.txt**: é”™è¯¯æå–çš„æ ‡é¢˜ï¼Œæ¯è¡Œä¸€ä¸ª
  ```
  Abstract......Introduction......References
  IEEE Transactions on Pattern Analysis, Vol. 25
  Table of Contents......Chapter 1
  ...
  ```

#### (2) æµ‹è¯•æ•°æ®
- **testSet-1000.xlsx**: Excel æ–‡ä»¶ï¼ŒåŒ…å«ä¸¤åˆ—
  - `title given by manchine`: å¾…åˆ†ç±»çš„æ ‡é¢˜
  - `Y/N`: æ ‡ç­¾ (Y=æ­£ç¡®æ ‡é¢˜, N=é”™è¯¯æ ‡é¢˜)

> **æ³¨æ„**: å¦‚æœæ²¡æœ‰æ•°æ®æ–‡ä»¶ï¼Œç¨‹åºä¼šè‡ªåŠ¨ä½¿ç”¨ç¤ºä¾‹æ•°æ®è¿›è¡Œæ¼”ç¤º

### 4. è¿è¡Œç¨‹åº

#### æ–¹å¼ä¸€ï¼šå®Œæ•´æµæ°´çº¿ (æ¨èæ–°æ‰‹)

è¿è¡Œæ‰€æœ‰ä¸‰ä¸ªæ¨¡å‹å¹¶å¯¹æ¯”ï¼š

```bash
cd core
python main_pipeline.py
```

**é…ç½®é€‰é¡¹** (åœ¨ `main_pipeline.py` ä¸­ä¿®æ”¹):
```python
USE_SAMPLE_DATA = False     # æ˜¯å¦ä½¿ç”¨ç¤ºä¾‹æ•°æ®
MAX_TRAIN_SAMPLES = None    # è®­ç»ƒæ ·æœ¬æ•°é™åˆ¶ (None=å…¨éƒ¨)
TRAIN_ONLY_BERT = False     # æ˜¯å¦åªè®­ç»ƒ BERT
BERT_EPOCHS = 5             # BERT è®­ç»ƒè½®æ•°
OUTPUT_DIR = 'output'       # è¾“å‡ºç›®å½•
```

#### æ–¹å¼äºŒï¼šä¼˜åŒ–ç‰ˆ BERT (æ¨èè¿›é˜¶)

åªè®­ç»ƒä¼˜åŒ–ç‰ˆ BERTï¼Œè·å¾—æœ€ä½³æ€§èƒ½ï¼š

```bash
python train_optimized_bert.py
```

#### æ–¹å¼ä¸‰ï¼šå•ç‹¬æµ‹è¯•æ¨¡å‹

æµ‹è¯•å•ä¸ªåˆ†ç±»å™¨ï¼š

```bash
cd core
python naive_bayes_classifier.py      # æµ‹è¯•æœ´ç´ è´å¶æ–¯
python word2vec_svm_classifier.py     # æµ‹è¯• Word2Vec+SVM
python bert_classifier.py             # æµ‹è¯• BERT
```

---

## ğŸ¯ ä¸‰ç§åˆ†ç±»æ–¹æ³•è¯¦è§£

### æ–¹æ³• 1: æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ (Naive Bayes)

**æ–‡ä»¶**: `core/naive_bayes_classifier.py`

#### ç®—æ³•åŸç†
- **ç‰¹å¾æå–**: TF-IDF (Term Frequency - Inverse Document Frequency)
  - è¯é¢‘ (TF): è¯åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„é¢‘ç‡
  - é€†æ–‡æ¡£é¢‘ç‡ (IDF): è¯çš„åŒºåˆ†èƒ½åŠ› (ç½•è§è¯æƒé‡æ›´é«˜)
  - å…¬å¼: `TF-IDF = TF Ã— log(N / df)`
- **åˆ†ç±»å™¨**: MultinomialNB (å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯)
  - åŸºäºè´å¶æ–¯å®šç†: `P(ç±»åˆ«|æ–‡æ¡£) âˆ P(æ–‡æ¡£|ç±»åˆ«) Ã— P(ç±»åˆ«)`
  - Laplace å¹³æ»‘é¿å…é›¶æ¦‚ç‡

#### æŠ€æœ¯ç‰¹ç‚¹
- âœ… **ä¼˜åŠ¿**: è®­ç»ƒé€Ÿåº¦å¿«ã€å¯è§£é‡Šæ€§å¼ºã€é€‚åˆæ–‡æœ¬åˆ†ç±»
- âŒ **åŠ£åŠ¿**: å‡è®¾ç‰¹å¾ç‹¬ç«‹ (å®é™…ä¸Šè¯ä¹‹é—´æœ‰ä¾èµ–å…³ç³»)
- âš™ï¸ **å‚æ•°**:
  - `max_features=5000`: æœ€å¤š 5000 ä¸ªç‰¹å¾
  - `ngram_range=(1,2)`: ä½¿ç”¨ 1-gram å’Œ 2-gram
  - `alpha=1.0`: Laplace å¹³æ»‘å‚æ•°

#### å…³é”®ä»£ç 
```python
# TF-IDF å‘é‡åŒ–
vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))
X_train = vectorizer.fit_transform(titles)

# è®­ç»ƒæœ´ç´ è´å¶æ–¯
classifier = MultinomialNB(alpha=1.0)
classifier.fit(X_train, labels)
```

---

### æ–¹æ³• 2: Word2Vec + SVM åˆ†ç±»å™¨

**æ–‡ä»¶**: `core/word2vec_svm_classifier.py`

#### ç®—æ³•åŸç†
1. **Word2Vec è¯åµŒå…¥**:
   - å°†æ¯ä¸ªè¯æ˜ å°„åˆ° 100 ç»´å‘é‡ç©ºé—´
   - è¯­ä¹‰ç›¸ä¼¼çš„è¯åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»æ›´è¿‘
   - ä½¿ç”¨ CBOW (Continuous Bag of Words) æ¨¡å‹è®­ç»ƒ

2. **æ ‡é¢˜å‘é‡åŒ–**:
   - æ–¹æ³•: å¯¹æ ‡é¢˜ä¸­æ‰€æœ‰è¯å‘é‡å–å¹³å‡
   - å…¬å¼: `title_vec = (1/n) Ã— Î£ word_vec_i`

3. **ç‰¹å¾å·¥ç¨‹** (8 ä¸ªç»Ÿè®¡ç‰¹å¾):
   - æ ‡é¢˜é•¿åº¦ã€å¹³å‡è¯é•¿
   - å¤§å†™å­—æ¯æ¯”ä¾‹ã€æ•°å­—æ¯”ä¾‹
   - ç‰¹æ®Šå­—ç¬¦æ•°ã€æ˜¯å¦å«æ•°å­—ã€å…¨å¤§å†™/å°å†™æ ‡å¿—

4. **SVM åˆ†ç±»**:
   - ä½¿ç”¨ RBF æ ¸å‡½æ•° (éçº¿æ€§åˆ†ç±»)
   - æ‰¾åˆ°æœ€ä¼˜è¶…å¹³é¢åˆ†éš”ä¸¤ç±»æ ·æœ¬

#### æŠ€æœ¯ç‰¹ç‚¹
- âœ… **ä¼˜åŠ¿**:
  - æ•æ‰è¯­ä¹‰ä¿¡æ¯ (Word2Vec)
  - æ‰‹å·¥ç‰¹å¾å¢å¼ºæ€§èƒ½
  - SVM åœ¨é«˜ç»´ç©ºé—´è¡¨ç°ä¼˜ç§€
- âŒ **åŠ£åŠ¿**:
  - è®­ç»ƒæ—¶é—´è¾ƒé•¿
  - éœ€è¦æ‰‹å·¥è®¾è®¡ç‰¹å¾
- âš™ï¸ **å‚æ•°**:
  - `vector_size=100`: è¯å‘é‡ç»´åº¦
  - `window=5`: ä¸Šä¸‹æ–‡çª—å£
  - `use_linear_svm=False`: ä½¿ç”¨ RBF æ ¸
  - `add_features=True`: å¯ç”¨ç»Ÿè®¡ç‰¹å¾

#### å…³é”®ä»£ç 
```python
# è®­ç»ƒ Word2Vec
w2v_model = Word2Vec(sentences=tokenized_titles, vector_size=100, window=5)

# æ ‡é¢˜å‘é‡åŒ–
def title_to_vector(title):
    word_vecs = [w2v_model.wv[word] for word in title.split() if word in w2v_model.wv]
    avg_vec = np.mean(word_vecs, axis=0) if word_vecs else np.zeros(100)
    stat_features = extract_statistical_features(title)  # 8 ç»´
    return np.concatenate([avg_vec, stat_features])      # 108 ç»´

# è®­ç»ƒ SVM
svm = SVC(kernel='rbf', probability=True)
svm.fit(X_train, labels)
```

---

### æ–¹æ³• 3: BERT åˆ†ç±»å™¨ (Transformer)

**æ–‡ä»¶**: `core/bert_classifier.py` (æ ‡å‡†ç‰ˆ) å’Œ `optimized_BERT.py` (ä¼˜åŒ–ç‰ˆ)

#### ç®—æ³•åŸç†
- **BERT** (Bidirectional Encoder Representations from Transformers)
  - é¢„è®­ç»ƒæ¨¡å‹: `bert-base-uncased` (12å±‚, 768ç»´, 110Må‚æ•°)
  - åŒå‘ Transformer: åŒæ—¶è€ƒè™‘ä¸Šä¸‹æ–‡ä¿¡æ¯
  - [CLS] token: å¥å­çº§åˆ«çš„è¡¨ç¤ºå‘é‡

- **å¾®è°ƒ** (Fine-tuning):
  - åœ¨é¢„è®­ç»ƒ BERT åŸºç¡€ä¸Šæ·»åŠ åˆ†ç±»å±‚
  - ä½¿ç”¨æ ‡é¢˜æ•°æ®è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒ
  - è‡ªåŠ¨å­¦ä¹ ä»»åŠ¡ç›¸å…³çš„ç‰¹å¾è¡¨ç¤º

#### æ ‡å‡†ç‰ˆ BERT

**æ–‡ä»¶**: `core/bert_classifier.py`

**ç‰¹ç‚¹**:
- åŸºç¡€ BERT å¾®è°ƒ
- AdamW ä¼˜åŒ–å™¨ + çº¿æ€§å­¦ä¹ ç‡é¢„çƒ­
- 5 è½®è®­ç»ƒï¼Œbatch_size=32

**å…³é”®ä»£ç **:
```python
# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# è®­ç»ƒå¾ªç¯
optimizer = AdamW(model.parameters(), lr=2e-5)
scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps=500)

for epoch in range(epochs):
    for batch in dataloader:
        outputs = model(input_ids, attention_mask, labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        scheduler.step()
```

#### ä¼˜åŒ–ç‰ˆ BERT â­

**æ–‡ä»¶**: `optimized_BERT.py`

**8 ç§ä¼˜åŒ–æŠ€æœ¯**:

1. **FGM å¯¹æŠ—è®­ç»ƒ** (Fast Gradient Method)
   - åœ¨åµŒå…¥å±‚æ·»åŠ å¯¹æŠ—æ‰°åŠ¨ï¼Œæé«˜é²æ£’æ€§
   - `r_adv = Îµ Ã— grad / ||grad||`

2. **EMA æŒ‡æ•°ç§»åŠ¨å¹³å‡** (Exponential Moving Average)
   - å¹³æ»‘æ¨¡å‹å‚æ•°ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›
   - `Î¸_ema = 0.999 Ã— Î¸_ema + 0.001 Ã— Î¸`

3. **å·®å¼‚åŒ–å­¦ä¹ ç‡**
   - åˆ†ç±»å±‚å­¦ä¹ ç‡ = 10 Ã— BERT åŸºç¡€å­¦ä¹ ç‡
   - å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡

4. **Warmup + Cosine å­¦ä¹ ç‡è°ƒåº¦**
   - å‰ 10% æ­¥æ•°çº¿æ€§å¢åŠ å­¦ä¹ ç‡
   - åç»­æŒ‰ä½™å¼¦æ›²çº¿è¡°å‡

5. **æ—©åœæœºåˆ¶** (Early Stopping)
   - ç›‘æ§éªŒè¯é›† F1 åˆ†æ•°
   - Patience=3ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

6. **æ•°æ®å¢å¼º**
   - éšæœºåˆ é™¤è¯ (10% æ¦‚ç‡)
   - éšæœºäº¤æ¢ç›¸é‚»è¯ (10% æ¦‚ç‡)

7. **Focal Loss** (å¯é€‰)
   - å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
   - `FL = -Î±(1-p)^Î³ Ã— log(p)`

8. **æ¢¯åº¦è£å‰ª**
   - é™åˆ¶æ¢¯åº¦èŒƒæ•° â‰¤ 1.0
   - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

**å…³é”®ä»£ç **:
```python
# å¯¹æŠ—è®­ç»ƒ (FGM)
fgm = FGM(model, epsilon=1.0)
loss.backward()
fgm.attack()           # æ·»åŠ å¯¹æŠ—æ‰°åŠ¨
loss_adv.backward()    # å¯¹æŠ—æ ·æœ¬çš„æ¢¯åº¦
fgm.restore()          # æ¢å¤åŸå§‹å‚æ•°

# EMA
ema = EMA(model, decay=0.999)
ema.update()           # æ¯æ­¥æ›´æ–° EMA å‚æ•°
ema.apply_shadow()     # éªŒè¯æ—¶ä½¿ç”¨ EMA å‚æ•°
```

#### æŠ€æœ¯ç‰¹ç‚¹
- âœ… **ä¼˜åŠ¿**:
  - æœ€å…ˆè¿›çš„ NLP æ¨¡å‹
  - è‡ªåŠ¨ç‰¹å¾å­¦ä¹ 
  - ä¼˜åŒ–ç‰ˆæ€§èƒ½æœ€ä½³
- âŒ **åŠ£åŠ¿**:
  - è®­ç»ƒæ—¶é—´é•¿ (GPU æ¨è)
  - æ¨¡å‹å‚æ•°å¤š (110M)
  - éœ€è¦å¤§é‡æ•°æ®
- âš™ï¸ **å‚æ•°**:
  - `max_length=64`: æœ€å¤§åºåˆ—é•¿åº¦
  - `epochs=10`: è®­ç»ƒè½®æ•° (ä¼˜åŒ–ç‰ˆ)
  - `batch_size=16`: æ‰¹æ¬¡å¤§å°
  - `learning_rate=2e-5`: åŸºç¡€å­¦ä¹ ç‡

---

## ğŸ“Š æ¨¡å‹è¯„ä¼°

### è¯„ä¼°æŒ‡æ ‡

ç³»ç»Ÿä½¿ç”¨å¤šç»´åº¦æŒ‡æ ‡å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼š

| æŒ‡æ ‡ | è¯´æ˜ | å…¬å¼ |
|------|------|------|
| **å‡†ç¡®ç‡** (Accuracy) | é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ¯”ä¾‹ | `(TP + TN) / Total` |
| **ç²¾ç¡®ç‡** (Precision) | é¢„æµ‹ä¸ºæ­£ä¾‹ä¸­çœŸæ­£ä¾‹çš„æ¯”ä¾‹ | `TP / (TP + FP)` |
| **å¬å›ç‡** (Recall) | çœŸæ­£ä¾‹ä¸­è¢«é¢„æµ‹å‡ºçš„æ¯”ä¾‹ | `TP / (TP + FN)` |
| **F1 åˆ†æ•°** | ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡ | `2 Ã— P Ã— R / (P + R)` |
| **F1 å®å¹³å‡** | å„ç±»åˆ« F1 çš„ç®—æœ¯å¹³å‡ | `(F1_0 + F1_1) / 2` |
| **F1 å¾®å¹³å‡** | å…¨å±€è®¡ç®— F1 (ç­‰äºå‡†ç¡®ç‡) | `2 Ã— TP / (2Ã—TP + FP + FN)` |

**æ··æ·†çŸ©é˜µ**:
```
                é¢„æµ‹ä¸ºè´Ÿ   é¢„æµ‹ä¸ºæ­£
å®é™…ä¸ºè´Ÿ (0)      TN        FP
å®é™…ä¸ºæ­£ (1)      FN        TP
```

### æ€§èƒ½å¯¹æ¯” (å‚è€ƒ)

åŸºäº 1000 æ ·æœ¬æµ‹è¯•é›†çš„å…¸å‹ç»“æœï¼š

| æ¨¡å‹ | å‡†ç¡®ç‡ | ç²¾ç¡®ç‡ | å¬å›ç‡ | F1 åˆ†æ•° | è®­ç»ƒæ—¶é—´ |
|------|--------|--------|--------|---------|----------|
| Naive Bayes | 0.89 | 0.88 | 0.90 | 0.89 | ~1 åˆ†é’Ÿ |
| Word2Vec+SVM | 0.92 | 0.91 | 0.93 | 0.92 | ~5 åˆ†é’Ÿ |
| BERT (æ ‡å‡†) | 0.95 | 0.94 | 0.96 | 0.95 | ~30 åˆ†é’Ÿ |
| BERT (ä¼˜åŒ–) | **0.97** | **0.96** | **0.98** | **0.97** | ~45 åˆ†é’Ÿ |

> æ³¨: å®é™…æ€§èƒ½å–å†³äºæ•°æ®è´¨é‡ã€æ•°æ®é‡å’Œç¡¬ä»¶é…ç½®

---

## ğŸ“ˆ å¯è§†åŒ–è¾“å‡º

ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆä»¥ä¸‹å¯è§†åŒ–å›¾è¡¨ (ä¿å­˜åœ¨ `output/` ç›®å½•):

### 1. æ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾
**æ–‡ä»¶**: `model_comparison.png`

6 ä¸ªå­å›¾å¯¹æ¯”æ‰€æœ‰æ¨¡å‹çš„æŒ‡æ ‡ï¼š
- å‡†ç¡®ç‡ (Accuracy)
- ç²¾ç¡®ç‡ (Precision)
- å¬å›ç‡ (Recall)
- F1 åˆ†æ•°
- F1 å®å¹³å‡
- F1 å¾®å¹³å‡

### 2. æ··æ·†çŸ©é˜µçƒ­å›¾
**æ–‡ä»¶**: `confusion_matrices.png`

æ˜¾ç¤ºæ¯ä¸ªæ¨¡å‹çš„åˆ†ç±»ç»†èŠ‚ï¼š
- çœŸè´Ÿä¾‹ (TN)ã€å‡æ­£ä¾‹ (FP)
- å‡è´Ÿä¾‹ (FN)ã€çœŸæ­£ä¾‹ (TP)

### 3. t-SNE é™ç»´å¯è§†åŒ–
**æ–‡ä»¶**: `tsne_*.png`

å°†é«˜ç»´ç‰¹å¾å‘é‡æŠ•å½±åˆ° 2D å¹³é¢ï¼š
- çº¢è‰²ç‚¹: è´Ÿæ ·æœ¬ (é”™è¯¯æ ‡é¢˜)
- ç»¿è‰²ç‚¹: æ­£æ ·æœ¬ (æ­£ç¡®æ ‡é¢˜)
- æ˜¾ç¤ºæ¨¡å‹çš„ç‰¹å¾ç©ºé—´åˆ†å¸ƒ

---

## ğŸ› ï¸ æ ¸å¿ƒæ¨¡å—è¯´æ˜

### 1. æ•°æ®åŠ è½½å™¨ (data_loader.py)

**åŠŸèƒ½**:
- ä»æ–‡æœ¬æ–‡ä»¶åŠ è½½è®­ç»ƒæ•°æ®
- ä» Excel åŠ è½½æµ‹è¯•æ•°æ®
- æ–‡æœ¬é¢„å¤„ç† (å°å†™ã€å»ç‰¹æ®Šå­—ç¬¦)
- åˆ›å»ºç¤ºä¾‹æ•°æ®

**å…³é”®å‡½æ•°**:
```python
# åŠ è½½æ•°æ®é›†
train_titles, train_labels, test_titles, test_labels = DataLoader.prepare_dataset(
    'data/positive.txt',
    'data/negative.txt',
    'data/testSet-1000.xlsx'
)

# é¢„å¤„ç†å•ä¸ªæ ‡é¢˜
clean_title = DataLoader.preprocess_title("Deep Learning for CV!")
# è¾“å‡º: "deep learning for cv"
```

### 2. æ¨¡å‹è¯„ä¼°å™¨ (evaluator.py)

**åŠŸèƒ½**:
- è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡
- ç”Ÿæˆæ··æ·†çŸ©é˜µ
- æ¨¡å‹æ€§èƒ½å¯¹æ¯”
- é”™è¯¯åˆ†æ

**å…³é”®å‡½æ•°**:
```python
evaluator = ModelEvaluator()

# è¯„ä¼°å•ä¸ªæ¨¡å‹
result = evaluator.evaluate_model(y_true, y_pred, "Model Name")

# æ¯”è¾ƒå¤šä¸ªæ¨¡å‹
evaluator.compare_models([result1, result2, result3])

# é”™è¯¯åˆ†æ
error_analysis = evaluator.calculate_error_analysis(y_true, y_pred, titles)
```

### 3. ç»“æœå¯è§†åŒ–å™¨ (visualizer.py)

**åŠŸèƒ½**:
- ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾
- ç»˜åˆ¶æ··æ·†çŸ©é˜µçƒ­å›¾
- t-SNE é™ç»´å¯è§†åŒ–

**å…³é”®å‡½æ•°**:
```python
visualizer = ResultVisualizer()

# æ€§èƒ½å¯¹æ¯”å›¾
visualizer.plot_comparison(results, save_path='comparison.png')

# æ··æ·†çŸ©é˜µ
visualizer.plot_confusion_matrices(results, save_path='confusion.png')

# t-SNE å¯è§†åŒ–
visualizer.visualize_embeddings_tsne(vectors, labels, "Model", save_path='tsne.png')
```

---

## âš™ï¸ é…ç½®ä¸å‚æ•°è°ƒä¼˜

### æœ´ç´ è´å¶æ–¯å‚æ•°

```python
classifier = NaiveBayesClassifier(
    max_features=5000,      # TF-IDF ç‰¹å¾æ•° (â†‘ å¢åŠ å¯èƒ½æé«˜å‡†ç¡®ç‡ä½†å¢åŠ è®¡ç®—é‡)
    ngram_range=(1, 2)      # n-gram èŒƒå›´ (1,1)=ä»…unigram, (1,2)=uni+bigram, (1,3)=uni+bi+trigram
)
```

**è°ƒä¼˜å»ºè®®**:
- å°æ•°æ®é›†: `max_features=3000`, `ngram_range=(1,1)`
- å¤§æ•°æ®é›†: `max_features=10000`, `ngram_range=(1,3)`

### Word2Vec + SVM å‚æ•°

```python
classifier = Word2VecSVMClassifier(
    vector_size=100,        # è¯å‘é‡ç»´åº¦ (50/100/200ï¼Œâ†‘ å¢åŠ æ›´ä¸°å¯Œä½†è®­ç»ƒæ…¢)
    window=5,               # ä¸Šä¸‹æ–‡çª—å£ (3-10ï¼ŒçŸ­æ ‡é¢˜ç”¨å°å€¼)
    min_count=2,            # æœ€å°è¯é¢‘ (1-5ï¼Œå°æ•°æ®é›†ç”¨1)
    epochs=10,              # Word2Vec è®­ç»ƒè½®æ•° (5-20)
    use_linear_svm=False,   # True=LinearSVC(å¿«), False=RBFæ ¸(å‡†)
    add_features=True       # æ˜¯å¦æ·»åŠ ç»Ÿè®¡ç‰¹å¾ (æ¨èTrue)
)
```

**è°ƒä¼˜å»ºè®®**:
- å¿«é€ŸåŸå‹: `use_linear_svm=True`, `epochs=5`
- æœ€ä½³æ€§èƒ½: `use_linear_svm=False`, `add_features=True`, `epochs=20`

### BERT å‚æ•°

#### æ ‡å‡†ç‰ˆ
```python
classifier = BERTClassifier(
    model_name='bert-base-uncased',  # é¢„è®­ç»ƒæ¨¡å‹
    max_length=64                     # æœ€å¤§åºåˆ—é•¿åº¦ (32-128)
)

classifier.train(
    train_titles, train_labels,
    epochs=5,                         # è®­ç»ƒè½®æ•° (3-10)
    batch_size=32,                    # æ‰¹æ¬¡å¤§å° (8/16/32/64)
    learning_rate=2e-5,               # å­¦ä¹ ç‡ (1e-5 ~ 5e-5)
    warmup_steps=500                  # é¢„çƒ­æ­¥æ•°
)
```

#### ä¼˜åŒ–ç‰ˆ
```python
classifier = BERTClassifierOptimized(
    model_name='bert-base-uncased',
    max_length=64,
    use_fgm=True,                     # å¯¹æŠ—è®­ç»ƒ
    use_ema=True                      # æŒ‡æ•°ç§»åŠ¨å¹³å‡
)

classifier.train(
    train_titles, train_labels,
    epochs=10,                        # è®­ç»ƒè½®æ•° (5-15)
    batch_size=16,                    # æ‰¹æ¬¡å¤§å° (â†“ å‡å°‘æ˜¾å­˜)
    learning_rate=2e-5,
    warmup_ratio=0.1,                 # é¢„çƒ­æ¯”ä¾‹ (0.05-0.15)
    weight_decay=0.01,                # æƒé‡è¡°å‡ (æ­£åˆ™åŒ–)
    patience=3,                       # æ—©åœè€å¿ƒå€¼
    use_focal_loss=False,             # Focal Loss (ç±»åˆ«ä¸å¹³è¡¡æ—¶ç”¨True)
    augment_data=True                 # æ•°æ®å¢å¼º
)
```

**è°ƒä¼˜å»ºè®®**:
- **å°æ•°æ®é›†** (<1000 æ ·æœ¬):
  - `epochs=15`, `batch_size=8`, `warmup_ratio=0.15`
  - `augment_data=True` (é‡è¦!)

- **ä¸­æ•°æ®é›†** (1000-10000):
  - `epochs=10`, `batch_size=16`, `warmup_ratio=0.1`

- **å¤§æ•°æ®é›†** (>10000):
  - `epochs=5`, `batch_size=32`, `warmup_ratio=0.05`

- **ç±»åˆ«ä¸å¹³è¡¡**:
  - `use_focal_loss=True`

### ç¡¬ä»¶é…ç½®å»ºè®®

| é…ç½® | CPU | GPU | å†…å­˜ | æ¨èæ¨¡å‹ |
|------|-----|-----|------|----------|
| æœ€ä½ | 2æ ¸ | æ—  | 4GB | Naive Bayes |
| æ¨è | 4æ ¸ | æ—  | 8GB | Word2Vec+SVM |
| é«˜æ€§èƒ½ | 8æ ¸ | GTX 1060+ (6GB) | 16GB | BERT (æ ‡å‡†) |
| é¡¶é… | 16æ ¸ | RTX 3080+ (10GB) | 32GB | BERT (ä¼˜åŒ–) |

---

## ğŸ”§ å¸¸è§é—®é¢˜ (FAQ)

### Q1: å¦‚ä½•å¤„ç†æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨çš„æƒ…å†µï¼Ÿ

**A**: ç¨‹åºä¼šè‡ªåŠ¨ä½¿ç”¨ç¤ºä¾‹æ•°æ®ã€‚å¦‚éœ€ä½¿ç”¨çœŸå®æ•°æ®ï¼š
```python
# åœ¨ main_pipeline.py ä¸­è®¾ç½®
USE_SAMPLE_DATA = False

# ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨
data/
  â”œâ”€â”€ positive.txt
  â”œâ”€â”€ negative.txt
  â””â”€â”€ testSet-1000.xlsx
```

### Q2: BERT è®­ç»ƒå¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ

**A**: å‡ ç§åŠ é€Ÿæ–¹æ³•ï¼š
1. **å‡å°‘æ•°æ®**: `MAX_TRAIN_SAMPLES = 5000`
2. **å‡å°‘è½®æ•°**: `epochs=3`
3. **å¢å¤§æ‰¹æ¬¡**: `batch_size=32` (éœ€è¦æ›´å¤šæ˜¾å­˜)
4. **åªè®­ç»ƒBERT**: `TRAIN_ONLY_BERT = True`
5. **ä½¿ç”¨GPU**: å®‰è£… CUDA ç‰ˆ PyTorch

### Q3: æ˜¾å­˜ä¸è¶³ (CUDA out of memory)

**A**:
```python
# å‡å°æ‰¹æ¬¡å¤§å°
batch_size = 8  # æˆ–æ›´å° (4)

# å‡å°åºåˆ—é•¿åº¦
max_length = 32  # åŸæ¥æ˜¯ 64

# ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ (æœªå®ç°ï¼Œéœ€æ‰‹åŠ¨æ·»åŠ )
```

### Q4: å¦‚ä½•æé«˜æ¨¡å‹å‡†ç¡®ç‡ï¼Ÿ

**A**: æŒ‰ä¼˜å…ˆçº§å°è¯•ï¼š
1. **å¢åŠ è®­ç»ƒæ•°æ®** (æœ€é‡è¦!)
2. **ä½¿ç”¨ä¼˜åŒ–ç‰ˆBERT** (`train_optimized_bert.py`)
3. **è°ƒæ•´è¶…å‚æ•°** (å­¦ä¹ ç‡ã€è½®æ•°)
4. **æ•°æ®æ¸…æ´—** (ç§»é™¤å™ªå£°æ ·æœ¬)
5. **ç‰¹å¾å·¥ç¨‹** (Word2Vec+SVM çš„ç»Ÿè®¡ç‰¹å¾)

### Q5: å¦‚ä½•ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Ÿ

**A**:
```python
# BERT æ¨¡å‹ä¼šè‡ªåŠ¨ä¿å­˜
# æ–‡ä»¶: best_bert_model.pt

# åŠ è½½æ¨¡å‹
model.load_state_dict(torch.load('best_bert_model.pt'))

# å…¶ä»–æ¨¡å‹éœ€æ‰‹åŠ¨ä¿å­˜
import pickle
with open('nb_model.pkl', 'wb') as f:
    pickle.dump(classifier, f)
```

### Q6: å¯ä»¥ç”¨äºå…¶ä»–åˆ†ç±»ä»»åŠ¡å—ï¼Ÿ

**A**: å¯ä»¥! åªéœ€ï¼š
1. å‡†å¤‡æ•°æ® (æ–‡æœ¬ + äºŒåˆ†ç±»æ ‡ç­¾)
2. ä¿®æ”¹æ•°æ®åŠ è½½éƒ¨åˆ†
3. æ— éœ€ä¿®æ”¹æ¨¡å‹ä»£ç 

### Q7: æ”¯æŒå¤šåˆ†ç±»å—ï¼Ÿ

**A**: å½“å‰æ˜¯äºŒåˆ†ç±»ï¼Œæ”¹ä¸ºå¤šåˆ†ç±»éœ€ä¿®æ”¹ï¼š
```python
# BERT
num_labels = 5  # æ”¹ä¸ºç±»åˆ«æ•°

# æœ´ç´ è´å¶æ–¯å’ŒSVMè‡ªåŠ¨æ”¯æŒå¤šåˆ†ç±»
```

---

## ğŸ“š æŠ€æœ¯æ ˆ

### æ ¸å¿ƒåº“

| åº“ | ç‰ˆæœ¬ | ç”¨é€” |
|---|------|------|
| Python | 3.7+ | ç¼–ç¨‹è¯­è¨€ |
| NumPy | 1.19+ | æ•°å€¼è®¡ç®— |
| Pandas | 1.2+ | æ•°æ®å¤„ç† |
| Scikit-learn | 0.24+ | ä¼ ç»Ÿæœºå™¨å­¦ä¹  |
| PyTorch | 1.9+ | æ·±åº¦å­¦ä¹ æ¡†æ¶ |
| Transformers | 4.10+ | BERT æ¨¡å‹ |
| Gensim | 4.0+ | Word2Vec |
| Matplotlib | 3.3+ | å¯è§†åŒ– |
| Seaborn | 0.11+ | é«˜çº§å¯è§†åŒ– |
| tqdm | 4.60+ | è¿›åº¦æ¡ |

### ç®—æ³•ä¸æ¨¡å‹

- **æœ´ç´ è´å¶æ–¯**: MultinomialNB
- **TF-IDF**: TfidfVectorizer
- **Word2Vec**: CBOW æ¨¡å‹
- **SVM**: RBF æ ¸ / Linear
- **BERT**: bert-base-uncased (Hugging Face)
- **ä¼˜åŒ–å™¨**: AdamW
- **å­¦ä¹ ç‡è°ƒåº¦**: Linear / Cosine with Warmup

---

## ğŸ“ ç®—æ³•ç†è®º

### 1. TF-IDF åŸç†

**TF (Term Frequency)**: è¯é¢‘
```
TF(t, d) = count(t in d) / total_words(d)
```

**IDF (Inverse Document Frequency)**: é€†æ–‡æ¡£é¢‘ç‡
```
IDF(t) = log(N / df(t))
```
å…¶ä¸­ N æ˜¯æ–‡æ¡£æ€»æ•°ï¼Œdf(t) æ˜¯åŒ…å«è¯ t çš„æ–‡æ¡£æ•°

**TF-IDF**:
```
TF-IDF(t, d) = TF(t, d) Ã— IDF(t)
```

**æ„ä¹‰**:
- é«˜ TF: è¯åœ¨å½“å‰æ–‡æ¡£ä¸­é‡è¦
- é«˜ IDF: è¯åœ¨æ‰€æœ‰æ–‡æ¡£ä¸­ç½•è§ (åŒºåˆ†åº¦é«˜)

### 2. æœ´ç´ è´å¶æ–¯åŸç†

**è´å¶æ–¯å®šç†**:
```
P(C|X) = P(X|C) Ã— P(C) / P(X)
```

**æœ´ç´ å‡è®¾**: ç‰¹å¾ç›¸äº’ç‹¬ç«‹
```
P(X|C) = P(x1|C) Ã— P(x2|C) Ã— ... Ã— P(xn|C)
```

**åˆ†ç±»å†³ç­–**:
```
C* = argmax_C P(C|X) = argmax_C P(X|C) Ã— P(C)
```

### 3. Word2Vec åŸç†

**CBOW (Continuous Bag of Words)**:
- è¾“å…¥: ä¸Šä¸‹æ–‡è¯
- è¾“å‡º: ä¸­å¿ƒè¯
- ç›®æ ‡: æœ€å¤§åŒ– `P(ä¸­å¿ƒè¯ | ä¸Šä¸‹æ–‡)`

**Skip-gram**:
- è¾“å…¥: ä¸­å¿ƒè¯
- è¾“å‡º: ä¸Šä¸‹æ–‡è¯
- ç›®æ ‡: æœ€å¤§åŒ– `P(ä¸Šä¸‹æ–‡ | ä¸­å¿ƒè¯)`

**è´Ÿé‡‡æ ·** (Negative Sampling):
- åªæ›´æ–°å°‘é‡è´Ÿæ ·æœ¬ï¼ŒåŠ é€Ÿè®­ç»ƒ

### 4. SVM åŸç†

**ç›®æ ‡**: æ‰¾åˆ°æœ€å¤§é—´éš”è¶…å¹³é¢

**çº¿æ€§SVM**:
```
minimize: 1/2 ||w||Â² + C Î£ Î¾i
subject to: yi(wÂ·xi + b) â‰¥ 1 - Î¾i
```

**RBF æ ¸**:
```
K(xi, xj) = exp(-Î³ ||xi - xj||Â²)
```
å°†æ•°æ®æ˜ å°„åˆ°é«˜ç»´ç©ºé—´ï¼Œå®ç°éçº¿æ€§åˆ†ç±»

### 5. BERT åŸç†

**Transformer æ¶æ„**:
- Self-Attention: `Attention(Q, K, V) = softmax(QK^T / âˆšdk) V`
- Multi-Head Attention: å¤šä¸ª attention å¹¶è¡Œ
- Feed-Forward: ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œ

**é¢„è®­ç»ƒä»»åŠ¡**:
1. **Masked LM**: éšæœºé®ç›– 15% çš„è¯ï¼Œé¢„æµ‹è¢«é®ç›–çš„è¯
2. **Next Sentence Prediction**: åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦è¿ç»­

**å¾®è°ƒ**:
- æ·»åŠ åˆ†ç±»å±‚ `Linear(768 â†’ 2)`
- ç«¯åˆ°ç«¯è®­ç»ƒ

### 6. ä¼˜åŒ–æŠ€æœ¯åŸç†

**å¯¹æŠ—è®­ç»ƒ (FGM)**:
```
r_adv = Îµ Ã— âˆ‡_emb L / ||âˆ‡_emb L||
emb_adv = emb + r_adv
L_total = L(emb) + L(emb_adv)
```

**EMA**:
```
Î¸_ema^(t) = Î± Ã— Î¸_ema^(t-1) + (1-Î±) Ã— Î¸^(t)
```

**Warmup**:
```
lr(t) = lr_max Ã— min(t/warmup_steps, 1)
```

**Cosine Annealing**:
```
lr(t) = lr_min + 0.5(lr_max - lr_min)(1 + cos(Ï€t/T))
```

---

## ğŸ“ è¾“å‡ºæ–‡ä»¶è¯´æ˜

### 1. evaluation_results.txt

åŒ…å«æ‰€æœ‰æ¨¡å‹çš„è¯¦ç»†è¯„ä¼°æŒ‡æ ‡ï¼š
```
æ¨¡å‹: Naive Bayes
  å‡†ç¡®ç‡: 0.8900
  ç²¾ç¡®ç‡: 0.8800
  å¬å›ç‡: 0.9000
  F1åˆ†æ•°: 0.8899
  ...
```

### 2. predictions.json

æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼š
```json
{
  "Naive Bayes": [1, 0, 1, 0, ...],
  "Word2Vec+SVM": [1, 0, 1, 1, ...],
  "BERT": [1, 0, 1, 0, ...]
}
```

### 3. å¯è§†åŒ–å›¾è¡¨

- **model_comparison.png**: 6 ä¸ªå­å›¾å¯¹æ¯”æ‰€æœ‰æŒ‡æ ‡
- **confusion_matrices.png**: æ··æ·†çŸ©é˜µçƒ­å›¾
- **tsne_*.png**: æ¯ä¸ªæ¨¡å‹çš„ t-SNE é™ç»´å›¾

---

## ğŸš€ è¿›é˜¶ä½¿ç”¨

### è‡ªå®šä¹‰æ•°æ®å¢å¼º

```python
# åœ¨ optimized_BERT.py çš„ TitleDataset ç±»ä¸­ä¿®æ”¹
def augment_text(self, text: str) -> str:
    words = text.split()

    # è‡ªå®šä¹‰å¢å¼ºç­–ç•¥
    # 1. åŒä¹‰è¯æ›¿æ¢
    # 2. å›è¯‘ (Back Translation)
    # 3. è¯åºæ‰“ä¹±

    return ' '.join(words)
```

### é›†æˆå­¦ä¹ 

```python
# ç»„åˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹
def ensemble_predict(models, titles):
    predictions = []
    for model in models:
        pred = model.predict(titles)
        predictions.append(pred)

    # æŠ•ç¥¨æ³•
    final_pred = np.round(np.mean(predictions, axis=0))
    return final_pred
```

### è¶…å‚æ•°æœç´¢

```python
from sklearn.model_selection import GridSearchCV

# å¯¹SVMè¿›è¡Œç½‘æ ¼æœç´¢
param_grid = {
    'C': [0.1, 1, 10],
    'gamma': ['scale', 'auto', 0.1, 1]
}
grid_search = GridSearchCV(SVC(), param_grid, cv=5)
grid_search.fit(X_train, y_train)
```

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **TF-IDF**:
   - Salton, G., & Buckley, C. (1988). Term-weighting approaches in automatic text retrieval.

2. **æœ´ç´ è´å¶æ–¯**:
   - McCallum, A., & Nigam, K. (1998). A comparison of event models for naive bayes text classification.

3. **Word2Vec**:
   - Mikolov, T., et al. (2013). Efficient estimation of word representations in vector space. ICLR.

4. **BERT**:
   - Devlin, J., et al. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. NAACL.

5. **å¯¹æŠ—è®­ç»ƒ**:
   - Miyato, T., et al. (2017). Adversarial training methods for semi-supervised text classification. ICLR.

6. **EMA**:
   - Polyak, B. T., & Juditsky, A. B. (1992). Acceleration of stochastic approximation by averaging.

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ä»£ç ! è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

### ä»£ç è§„èŒƒ

- éµå¾ª PEP 8 é£æ ¼
- æ·»åŠ è¯¦ç»†çš„æ³¨é‡Šå’Œæ–‡æ¡£å­—ç¬¦ä¸²
- ä¿æŒå‡½æ•°ç®€æ´ (<50 è¡Œ)
- ä½¿ç”¨æœ‰æ„ä¹‰çš„å˜é‡å

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

---

## ğŸ“® è”ç³»æ–¹å¼

- **é¡¹ç›®ç»´æŠ¤è€…**: [Your Name]
- **Email**: your.email@example.com
- **é¡¹ç›®ä¸»é¡µ**: https://github.com/yourusername/IR-task2

---

## ğŸ™ è‡´è°¢

- Hugging Face Transformers å›¢é˜Ÿ
- Scikit-learn ç¤¾åŒº
- PyTorch å¼€å‘å›¢é˜Ÿ
- æ‰€æœ‰è´¡çŒ®è€…

---

## ğŸ“Š æ›´æ–°æ—¥å¿—

### v2.0.0 (2024-12-02)
- âœ¨ æ·»åŠ ä¼˜åŒ–ç‰ˆ BERT (8ç§ä¼˜åŒ–æŠ€æœ¯)
- âœ¨ å¢å¼ºæ•°æ®å¢å¼ºåŠŸèƒ½
- ğŸ“ å®Œå–„æ–‡æ¡£å’Œæ³¨é‡Š
- ğŸ› ä¿®å¤å·²çŸ¥ bug

### v1.0.0 (2024-11-01)
- ğŸ‰ åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- âœ… å®ç°ä¸‰ç§åˆ†ç±»æ–¹æ³•
- ğŸ“Š æ·»åŠ å®Œæ•´è¯„ä¼°å’Œå¯è§†åŒ–

---

**ç¥ä½ ä½¿ç”¨æ„‰å¿«! å¦‚æœ‰é—®é¢˜è¯·æ Issue æˆ– PR ğŸ‰**
