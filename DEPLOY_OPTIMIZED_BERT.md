# 🚀 部署优化版 BERT 分类器

## 📋 优化版 BERT 的核心特性

### **8 大优化技巧：**

1. **对抗训练 (FGM)** ⭐⭐⭐⭐⭐
   - 生成对抗样本，提升模型鲁棒性
   - 防止过拟合

2. **指数移动平均 (EMA)** ⭐⭐⭐⭐
   - 平滑参数更新
   - 提升模型稳定性

3. **差异化学习率** ⭐⭐⭐⭐
   - 分类层用 10 倍学习率
   - BERT 层用标准学习率
   - 更快收敛

4. **Cosine 学习率调度** ⭐⭐⭐⭐
   - Warmup + Cosine 衰减
   - 比线性衰减更平滑

5. **梯度裁剪** ⭐⭐⭐
   - 防止梯度爆炸
   - 更稳定的训练

6. **数据增强** ⭐⭐⭐
   - 随机删除词、交换词
   - 扩充训练数据

7. **Focal Loss** ⭐⭐⭐
   - 处理类别不平衡
   - 专注于难分类样本

8. **验证集早停** ⭐⭐⭐⭐⭐
   - 基于验证 F1 分数
   - 防止过拟合

---

## 📊 预期性能

### **对比：**

| 模型 | 准确率 | F1 分数 | 说明 |
|------|--------|---------|------|
| 原始 BERT | 87.81% | 89.38% | 基线 |
| 优化 v2 BERT | 88-91% | 89-92% | 简单优化 |
| **优化版 BERT** | **90-93%** | **91-94%** | **全优化** ✅ |

---

## 📤 部署步骤

### **第一步：上传文件到服务器**

在本地电脑执行：

```bash
# 上传优化版 BERT 和训练脚本
scp -P 22900 E:/task2/optimized_BERT.py u2023312337@10.3.50.20:~/task2/task2/
scp -P 22900 E:/task2/train_optimized_bert.py u2023312337@10.3.50.20:~/task2/task2/
scp -P 22900 E:/task2/DEPLOY_OPTIMIZED_BERT.md u2023312337@10.3.50.20:~/task2/task2/
```

---

### **第二步：在服务器上运行训练**

SSH 登录服务器后执行：

```bash
# 进入项目目录
cd ~/task2/task2
source .venv/bin/activate

# 清理输出目录
rm -rf output/*

# 后台运行优化版 BERT 训练
nohup python train_optimized_bert.py > training_optimized_bert_full.log 2>&1 &
echo "优化版 BERT 训练已启动！"

# 查看日志
tail -f training_optimized_bert_full.log
```

---

## ⏱️ 预计训练时间

### **时间估计：**

| 阶段 | 时间 |
|------|------|
| 数据加载 | 1 分钟 |
| **BERT 训练（10 轮）** | **~8-10 小时** |
| 评估 | 5 分钟 |
| **总计** | **~8-10 小时** |

**注意：** 优化版 BERT 比 v2 多了对抗训练和 EMA，所以训练时间更长。

---

## 🔍 监控训练

### **查看实时日志：**

```bash
cd ~/task2/task2
tail -f training_optimized_bert_full.log
```

### **预期输出：**

```
================================================================================
  优化版 BERT 分类器 - 学术标题分类
================================================================================

[步骤 1/3] 加载数据集
--------------------------------------------------------------------------------
✓ 训练集: 232402 样本
  - 正样本: 118079 (50.8%)
  - 负样本: 114323 (49.2%)
✓ 测试集: 976 样本

[步骤 2/3] 训练优化版 BERT 分类器
--------------------------------------------------------------------------------

使用设备: cuda
GPU: NVIDIA GeForce RTX 4090
显存: 24.00 GB

加载BERT模型: bert-base-uncased

训练优化版BERT分类器
======================================================================
模型: bert-base-uncased
训练样本: 185921 (自动划分20%验证集)
轮数: 10
批次大小: 16
学习率: 2e-05

优化技巧:
  ✓ 对抗训练(FGM): True
  ✓ 指数移动平均(EMA): True
  ✓ 差异化学习率: 是
  ✓ Cosine学习率调度: 是
  ✓ 梯度裁剪: 是
  ✓ Focal Loss: False
  ✓ 数据增强: True
  ✓ 验证早停: 是

自动划分验证集:
  训练集: 185921 样本
  验证集: 46481 样本

学习率调度:
  总步数: 116200
  Warmup步数: 11620
  BERT层学习率: 2e-05
  分类层学习率: 2e-04

======================================================================
Epoch 1/10
======================================================================
训练: 100%|████████| 11613/11613 [1:15:32<00:00, lr=1.9e-05]

Epoch 1 结果:
  训练 - Loss: 0.3245 | Acc: 0.8756 (87.56%)
  验证 - Loss: 0.2987 | Acc: 0.8834 (88.34%) | F1: 0.8823
  ✓ 最佳模型已保存! (F1: 0.8823)
```

---

## 📈 训练进度时间线

假设现在是 **20:00** 启动：

| 时间 | 阶段 | 说明 |
|------|------|------|
| 20:00 | 启动 | 加载数据 |
| 20:01 | Epoch 1 | 约 75 分钟/轮 |
| 21:16 | Epoch 2 | |
| 22:31 | Epoch 3 | |
| 23:46 | Epoch 4 | |
| 01:01 | Epoch 5 | |
| 02:16 | Epoch 6 | |
| 03:31 | Epoch 7 | |
| 04:46 | Epoch 8 | |
| 06:01 | Epoch 9 | |
| 07:16 | Epoch 10 | |
| **08:31** | **完成** | **评估和保存** |

---

## 🎯 成功指标

### **训练成功的标志：**

1. **验证 F1 逐轮提升**
   ```
   Epoch 1: F1 = 0.8823
   Epoch 2: F1 = 0.8945
   Epoch 3: F1 = 0.9012
   Epoch 4: F1 = 0.9078
   ...
   ```

2. **最终测试集性能**
   ```
   准确率 > 90%
   F1 分数 > 91%
   ```

3. **没有 CUDA 错误或内存溢出**

---

## 📥 训练完成后

### **下载结果：**

在本地电脑执行：

```bash
# 下载输出结果
scp -P 22900 -r u2023312337@10.3.50.20:~/task2/task2/output E:/task2/output_optimized_bert_full

# 下载训练日志
scp -P 22900 u2023312337@10.3.50.20:~/task2/task2/training_optimized_bert_full.log E:/task2/

# 下载最佳模型
scp -P 22900 u2023312337@10.3.50.20:~/task2/task2/best_bert_model.pt E:/task2/
```

---

## 🔧 配置调整

### **如果显存不足（< 24GB）：**

修改 `train_optimized_bert.py` 第 50 行：

```python
batch_size=8,  # 从 16 改为 8
```

**影响：**
- 训练时间增加 2 倍
- 显存占用减少 50%

---

### **如果想更快完成（牺牲性能）：**

修改 `train_optimized_bert.py` 第 48 行：

```python
epochs=5,  # 从 10 改为 5
```

**影响：**
- 训练时间减少 50%
- 性能可能下降 1-2%

---

## 💡 优化版 BERT 的优势

### **为什么这个版本更强？**

1. **对抗训练 (FGM)**
   - 模型学会对抗扰动
   - 更鲁棒，泛化能力强
   - 预期提升 1-2%

2. **指数移动平均 (EMA)**
   - 参数更新更平滑
   - 避免剧烈波动
   - 预期提升 0.5-1%

3. **差异化学习率**
   - 分类层快速适应
   - BERT 层缓慢微调
   - 预期提升 1-2%

4. **Cosine 学习率调度**
   - 比线性衰减更平滑
   - 更好的收敛
   - 预期提升 0.5-1%

5. **数据增强**
   - 扩充训练数据
   - 防止过拟合
   - 预期提升 1-2%

6. **验证集早停**
   - 基于验证 F1
   - 防止过拟合
   - 自动找到最佳轮数

---

## 📊 性能对比

### **三个版本对比：**

| 特性 | 原始 BERT | 优化 v2 | 优化版 BERT |
|------|-----------|---------|------------|
| **准确率** | 87.81% | 88-91% | **90-93%** |
| **F1 分数** | 89.38% | 89-92% | **91-94%** |
| **训练时间** | 3.5 小时 | 4.5 小时 | **8-10 小时** |
| **对抗训练** | ❌ | ❌ | ✅ |
| **EMA** | ❌ | ❌ | ✅ |
| **差异化学习率** | ❌ | ❌ | ✅ |
| **数据增强** | ❌ | ❌ | ✅ |
| **验证早停** | ❌ | ❌ | ✅ |

---

## 🎓 课程作业亮点

### **展示的高级技巧：**

1. ✅ **对抗训练**：深度学习安全性
2. ✅ **指数移动平均**：参数平滑技巧
3. ✅ **差异化学习率**：分层微调
4. ✅ **Cosine 调度**：高级学习率策略
5. ✅ **数据增强**：数据处理技巧
6. ✅ **验证集早停**：防止过拟合

---

**预计 8-10 小时后完成，性能应该达到 90-93%！** 🚀

