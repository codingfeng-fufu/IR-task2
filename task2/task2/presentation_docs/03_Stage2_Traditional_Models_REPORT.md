# Stage2_Traditional_Models é˜¶æ®µæŠ¥å‘Š

## ğŸ“‹ é˜¶æ®µæ¦‚è§ˆ

**é˜¶æ®µåç§°**: Stage2_Traditional_Models - ä¼ ç»Ÿæ¨¡å‹å®Œæ•´å®ç°
**å®ç°æ—¶é—´**: 2024å¹´11æœˆ15æ—¥
**é˜¶æ®µå®šä½**: åœ¨Stage1åŸºç¡€è®¾æ–½ä¸Š,å®ç°ä¸‰ä¸ªå®Œæ•´çš„åˆ†ç±»æ¨¡å‹
**ä»£ç è§„æ¨¡**: çº¦1,400è¡Œï¼ˆ3ä¸ªæ¨¡å‹æ–‡ä»¶ï¼‰

## ğŸ¯ æ ¸å¿ƒç›®æ ‡ä¸å®šä½

**ä¸Baselineçš„å…³ç³»**:
- Baseline: ç®€å•åŸå‹,éªŒè¯å¯è¡Œæ€§
- **Stage2**: ç”Ÿäº§çº§å®ç°,å®Œå–„çš„æ¥å£å’Œé”™è¯¯å¤„ç†

**ä¸Stage1çš„å…³ç³»**:
- Stage1: æä¾›åŸºç¡€è®¾æ–½(data_loader, evaluator, visualizer)
- **Stage2**: ä½¿ç”¨Stage1çš„åŸºç¡€è®¾æ–½,ä¸“æ³¨äºæ¨¡å‹å®ç°

**æ ¸å¿ƒä»·å€¼**: å»ºç«‹æ€§èƒ½åŸºå‡†,ä¸ºåç»­ä¼˜åŒ–æä¾›å¯¹æ¯”å¯¹è±¡ã€‚

## ğŸ“Š å®éªŒç»“æœ

åŸºäºå®é™…è®­ç»ƒ(è®­ç»ƒé›†232,402æ ·æœ¬,æµ‹è¯•é›†1,000æ ·æœ¬):

| æ¨¡å‹ | å‡†ç¡®ç‡ | ç²¾ç¡®ç‡ | å¬å›ç‡ | F1åˆ†æ•° |
|------|--------|--------|--------|--------|
| **Naive Bayes** | 73.46% | 73.59% | 84.86% | 78.82% |
| **Word2Vec+SVM** | 74.39% | 79.66% | 75.18% | 77.36% |
| **BERT** | 86.99% | 92.16% | 84.86% | 88.36% |

### ä¸Baselineå¯¹æ¯”

| æ¨¡å‹ | Baseline | Stage2 | å·®å¼‚ | åŸå›  |
|------|----------|--------|------|------|
| Naive Bayes | 73.36% | 73.46% | +0.1% | å®ç°ç›¸åŒ,æ­£å¸¸æ³¢åŠ¨ |
| Word2Vec+SVM | 75.61% | 74.39% | -1.2% | å‚æ•°ç•¥æœ‰ä¸åŒ |
| BERT | 88.32% | 86.99% | -1.3% | è®­ç»ƒè½®æ•°å·®å¼‚ |

**ç»“è®º**: Stage2ä¸Baselineæ€§èƒ½åŸºæœ¬ä¸€è‡´,éªŒè¯äº†å®ç°çš„æ­£ç¡®æ€§ã€‚å¾®å°å·®å¼‚æ¥è‡ªéšæœºæ€§å’Œå‚æ•°è°ƒæ•´ã€‚

## ğŸ”§ æ ¸å¿ƒåˆ›æ–°ç‚¹

### 1. æ¨¡å—åŒ–è®¾è®¡

**Baselineé—®é¢˜**: æ‰€æœ‰åŠŸèƒ½æ··åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­,éš¾ä»¥ç»´æŠ¤ã€‚

**Stage2æ”¹è¿›**:
```
baseline_simple/main.py (200è¡Œ)
â†’ Stage2/naive_bayes_classifier.py (273è¡Œ, ä¸“æ³¨NB)
â†’ Stage2/word2vec_svm_classifier.py (450è¡Œ, ä¸“æ³¨W2V)
â†’ Stage2/bert_classifier.py (348è¡Œ, ä¸“æ³¨BERT)
```

æ¯ä¸ªæ¨¡å‹ç‹¬ç«‹æ–‡ä»¶,èŒè´£å•ä¸€,ä¾¿äº:
- å•ç‹¬æµ‹è¯•
- ç‹¬ç«‹ä¼˜åŒ–
- å¹¶è¡Œå¼€å‘

### 2. ç»Ÿä¸€æ¥å£è§„èŒƒ

æ‰€æœ‰ä¸‰ä¸ªæ¨¡å‹å®ç°ç›¸åŒæ¥å£:

```python
class Classifier:
    def train(self, titles: List[str], labels: List[int]) -> None:
        """è®­ç»ƒæ¨¡å‹"""

    def predict(self, titles: List[str]) -> np.ndarray:
        """é¢„æµ‹æ ‡ç­¾"""

    def save_model(self, path: str) -> None:
        """ä¿å­˜æ¨¡å‹"""

    def load_model(self, path: str) -> None:
        """åŠ è½½æ¨¡å‹"""

    def get_feature_vectors(self, titles: List[str]) -> np.ndarray:
        """è·å–ç‰¹å¾å‘é‡(ç”¨äºt-SNE)"""
```

**å¥½å¤„**:
- å¯ä»¥æ— ç¼æ›¿æ¢ä¸åŒæ¨¡å‹
- ä¾¿äºæ¨¡å‹å¯¹æ¯”å®éªŒ
- ä¾¿äºåç»­æ‰©å±•æ–°æ¨¡å‹

### 3. å®Œå–„çš„é”™è¯¯å¤„ç†

**Baseline**: åŸºç¡€çš„try-except
**Stage2**: ç»†ç²’åº¦çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—

```python
# naive_bayes_classifier.py:125
try:
    self.vectorizer.fit(titles)
except ValueError as e:
    print(f"é”™è¯¯: TF-IDFç‰¹å¾æå–å¤±è´¥ - {str(e)}")
    raise
except Exception as e:
    print(f"æœªçŸ¥é”™è¯¯: {str(e)}")
    raise
```

### 4. å¤ç”¨Stage1åŸºç¡€è®¾æ–½

**è®­ç»ƒè„šæœ¬ç¤ºä¾‹** (`train.py:168`):
```python
import sys
sys.path.append('../Stage1_Foundation')

from data_loader import DataLoader  # Stage1
from evaluator import ModelEvaluator  # Stage1
from visualizer import ResultVisualizer  # Stage1
from config import get_data_path, get_output_path  # Stage1

from naive_bayes_classifier import NaiveBayesClassifier  # Stage2
from word2vec_svm_classifier import Word2VecSVMClassifier  # Stage2
from bert_classifier import BERTClassifier  # Stage2
```

**é¿å…é‡å¤é€ è½®å­**: æ•°æ®åŠ è½½ã€è¯„ä¼°ã€å¯è§†åŒ–å…¨éƒ¨å¤ç”¨Stage1ã€‚

## ğŸ” ä¸‰ä¸ªæ¨¡å‹çš„æŠ€æœ¯ç»†èŠ‚

### 1. Naive Bayes (`naive_bayes_classifier.py:273è¡Œ`)

**æ ¸å¿ƒå®ç°**:
- TF-IDFç‰¹å¾: 5,000ç»´, (1,2)-gram
- åˆ†ç±»å™¨: MultinomialNB(alpha=1.0)
- ç®€å•ç›´æ¥,æ— ç‰¹æ®Šä¼˜åŒ–

**æ€§èƒ½**: 73.46% (ä¸­ç­‰)
**ä¼˜ç‚¹**: è®­ç»ƒå¿«(2åˆ†é’Ÿ),å¯è§£é‡Šæ€§å¼º
**ç¼ºç‚¹**: å‡è®¾ç‰¹å¾ç‹¬ç«‹,æ€§èƒ½å—é™

**åç»­ä¼˜åŒ–æ–¹å‘** (åœ¨Stage3å®ç°):
- å¢åŠ å­—ç¬¦çº§ç‰¹å¾ â†’ æ•æ‰æ ¼å¼æ¨¡å¼
- å¢åŠ ç»Ÿè®¡ç‰¹å¾ â†’ æ•æ‰é•¿åº¦ã€æ ‡ç‚¹ç­‰
- ä½¿ç”¨ComplementNB â†’ æ›´é€‚åˆæ–‡æœ¬åˆ†ç±»
- **æœ€ç»ˆæå‡**: 73.46% â†’ 79.20% (+5.74%)

### 2. Word2Vec + SVM (`word2vec_svm_classifier.py:450è¡Œ`)

**æ ¸å¿ƒå®ç°**:
- Word2Vec: 100ç»´è¯å‘é‡, window=5
- å¥å­è¡¨ç¤º: ç®€å•å¹³å‡è¯å‘é‡
- åˆ†ç±»å™¨: LinearSVC (çº¿æ€§æ ¸,é€Ÿåº¦å¿«)

**æ€§èƒ½**: 74.39% (ä¸­ç­‰)
**ä¼˜ç‚¹**: æ•æ‰è¯­ä¹‰ä¿¡æ¯,æ€§èƒ½å¹³è¡¡
**ç¼ºç‚¹**: å¹³å‡æ± åŒ–ä¸¢å¤±è¯åºå’Œé‡è¦æ€§

**ä¸ºä»€ä¹ˆä¸ç»§ç»­ä¼˜åŒ–**: æ€§èƒ½æå‡ç©ºé—´æœ‰é™,BERTå·²ç»è¶³å¤Ÿå¥½ã€‚

### 3. BERT (`bert_classifier.py:348è¡Œ`)

**æ ¸å¿ƒå®ç°**:
- é¢„è®­ç»ƒæ¨¡å‹: bert-base-uncased (110Må‚æ•°)
- åºåˆ—é•¿åº¦: 64 tokens
- è®­ç»ƒ: 3 epochs, batch_size=16, lr=2e-5
- ä¼˜åŒ–å™¨: AdamW + çº¿æ€§warmup

**æ€§èƒ½**: 86.99% (æœ€ä¼˜)
**ä¼˜ç‚¹**: ä¸Šä¸‹æ–‡ç†è§£å¼º,å‡†ç¡®ç‡é«˜
**ç¼ºç‚¹**: è®­ç»ƒæ…¢(1å°æ—¶),æ¨ç†æ…¢

**åç»­ä¼˜åŒ–æ–¹å‘** (åœ¨Stage4å®ç°):
- ä½¿ç”¨SciBERT â†’ å­¦æœ¯é¢†åŸŸä¸“ç”¨æ¨¡å‹
- Focal Loss â†’ å¤„ç†éš¾åˆ†æ ·æœ¬
- å¯¹æŠ—è®­ç»ƒ â†’ æå‡é²æ£’æ€§
- å¢åŠ åºåˆ—é•¿åº¦ â†’ 64â†’96 tokens
- **æœ€ç»ˆæå‡**: 86.99% â†’ 89-91%

## ğŸ“ˆ ä¸å…¶ä»–é˜¶æ®µçš„å¯¹æ¯”

### Stage2 vs Baseline

| ç»´åº¦ | Baseline | Stage2 |
|------|----------|--------|
| **ä»£ç ç»„ç»‡** | å•æ–‡ä»¶æ··æ‚ | å¤šæ–‡ä»¶æ¨¡å—åŒ– |
| **æ¥å£è®¾è®¡** | ä¸ç»Ÿä¸€ | ç»Ÿä¸€æ¥å£ |
| **é”™è¯¯å¤„ç†** | åŸºç¡€ | å®Œå–„ |
| **å¯ç»´æŠ¤æ€§** | ä½ | é«˜ |
| **æ€§èƒ½** | 73-88% | 73-87% (ç›¸å½“) |

### Stage2 vs Stage3/4 (åç»­ä¼˜åŒ–)

| æ¨¡å‹ | Stage2åŸºå‡† | Stage3/4ä¼˜åŒ–å | æå‡å¹…åº¦ |
|------|------------|----------------|----------|
| **Naive Bayes** | 73.46% | **79.20%** | +5.74% |
| **Word2Vec+SVM** | 74.39% | 82.99% | +8.6% |
| **BERT** | 86.99% | **89-91%** | +2-4% |

### Stage1 + Stage2 = å®Œæ•´çš„ä¼ ç»Ÿæ¨¡å‹ç³»ç»Ÿ

```
Stage1 (åŸºç¡€è®¾æ–½)
    |
    â”œâ”€ data_loader.py
    â”œâ”€ evaluator.py
    â””â”€ visualizer.py
         â†“ (è¢«ä½¿ç”¨)
Stage2 (æ¨¡å‹å®ç°)
    |
    â”œâ”€ naive_bayes_classifier.py
    â”œâ”€ word2vec_svm_classifier.py
    â””â”€ bert_classifier.py
         â†“ (æä¾›åŸºå‡†)
Stage3/4 (æ¨¡å‹ä¼˜åŒ–)
```

## ğŸ’¡ æŠ€æœ¯è¦ç‚¹è¯´æ˜

### 1. ä¸ºä»€ä¹ˆéœ€è¦Stage2?

**é—®é¢˜**: Baselineå·²ç»å®ç°äº†ä¸‰ä¸ªæ¨¡å‹,ä¸ºä»€ä¹ˆè¿˜è¦Stage2?

**ç­”æ¡ˆ**:
- Baselineæ˜¯**åŸå‹éªŒè¯**,è¯æ˜æ–¹æ¡ˆå¯è¡Œ
- Stage2æ˜¯**ç”Ÿäº§å®ç°**,ä»£ç è´¨é‡æ›´é«˜,ä¾¿äºç»´æŠ¤å’Œæ‰©å±•
- Stage2å»ºç«‹äº†**æ ‡å‡†æ¥å£**,åç»­ä¼˜åŒ–å¯ä»¥æ— ç¼æ›¿æ¢

### 2. ä¸ºä»€ä¹ˆä¸‰ä¸ªæ¨¡å‹æ€§èƒ½ç•¥æœ‰å·®å¼‚?

**ä¸Baselineå¯¹æ¯”**: Stage2çš„BERTæ¯”Baselineä½1.3%

**åŸå› **:
1. **éšæœºæ€§**: ç¥ç»ç½‘ç»œè®­ç»ƒæœ‰éšæœºåˆå§‹åŒ–
2. **è®­ç»ƒæ—¶é—´**: å¯èƒ½è®­ç»ƒè½®æ•°ç•¥æœ‰ä¸åŒ
3. **æ•°æ®é¡ºåº**: shuffleé¡ºåºä¸åŒ

**ç»“è®º**: 1-2%çš„å·®å¼‚åœ¨æ­£å¸¸èŒƒå›´å†…,ä¸å½±å“ç»“è®ºã€‚

### 3. ä¸ºä»€ä¹ˆWord2Vec+SVMç”¨LinearSVC?

**å¯¹æ¯”**:
- **RBFæ ¸SVC**: éçº¿æ€§,æ€§èƒ½å¯èƒ½æ›´å¥½,ä½†è®­ç»ƒææ…¢(6.8å°æ—¶)
- **LinearSVC**: çº¿æ€§,æ€§èƒ½ç•¥ä½,ä½†è®­ç»ƒå¿«(10åˆ†é’Ÿ)

**é€‰æ‹©LinearSVCçš„åŸå› **:
1. é€Ÿåº¦å¿«100å€+
2. æ€§èƒ½åªé™ä½1-2%
3. æ›´é€‚åˆå¤§è§„æ¨¡æ•°æ®

baseline_simpleç”¨çš„æ˜¯RBF,Stage2æ”¹ç”¨LinearSVCæ˜¯æƒè¡¡ä¹‹ä¸¾ã€‚

### 4. ç»Ÿä¸€æ¥å£çš„ä»·å€¼

**åœºæ™¯**: å¯¹æ¯”å¤šä¸ªæ¨¡å‹

**æ²¡æœ‰ç»Ÿä¸€æ¥å£**:
```python
# æ¯ä¸ªæ¨¡å‹è°ƒç”¨æ–¹å¼ä¸åŒ
nb_preds = nb_model.classify(test_data)  # æ–¹æ³•åä¸åŒ
w2v_preds = w2v_model.inference(test_data)  # æ–¹æ³•åä¸åŒ
bert_preds = bert_model.forward(test_data)  # æ–¹æ³•åä¸åŒ
```

**æœ‰ç»Ÿä¸€æ¥å£**:
```python
# æ‰€æœ‰æ¨¡å‹è°ƒç”¨æ–¹å¼ç›¸åŒ
for model in [nb, w2v, bert]:
    preds = model.predict(test_data)  # ç»Ÿä¸€æ–¹æ³•å
    result = evaluator.evaluate_model(test_labels, preds, model.name)
```

ä¾¿äºæ‰¹é‡å®éªŒå’Œå¯¹æ¯”ã€‚

## ğŸš€ ä½¿ç”¨æŒ‡å—

### å¿«é€Ÿå¼€å§‹

```bash
# 1. è¿›å…¥Stage2ç›®å½•
cd /home/u2023312337/task2/task2/stages/Stage2_Traditional_Models

# 2. è®­ç»ƒæ‰€æœ‰æ¨¡å‹
python train.py

# æˆ–å•ç‹¬è®­ç»ƒæŸä¸ªæ¨¡å‹
python train.py --model nb    # æœ´ç´ è´å¶æ–¯
python train.py --model w2v   # Word2Vec+SVM
python train.py --model bert  # BERT

# 3. è¯„ä¼°å·²ä¿å­˜çš„æ¨¡å‹
python evaluate_stage2.py
```

### åœ¨ä»£ç ä¸­ä½¿ç”¨

```python
import sys
sys.path.append('/home/u2023312337/task2/task2/stages/Stage2_Traditional_Models')

from naive_bayes_classifier import NaiveBayesClassifier
from word2vec_svm_classifier import Word2VecSVMClassifier
from bert_classifier import BERTClassifier

# é€‰æ‹©ä¸€ä¸ªæ¨¡å‹
model = NaiveBayesClassifier(max_features=5000, ngram_range=(1,2))
# model = Word2VecSVMClassifier(vector_size=100, window=5)
# model = BERTClassifier(model_name='bert-base-uncased', max_length=64)

# è®­ç»ƒ
model.train(train_titles, train_labels)

# é¢„æµ‹
predictions = model.predict(test_titles)

# ä¿å­˜
model.save_model('path/to/model')
```

## ğŸ“ˆ å·¥ä½œé‡ç»Ÿè®¡

### ä»£ç è§„æ¨¡

| æ–‡ä»¶ | è¡Œæ•° | åŠŸèƒ½ |
|------|------|------|
| naive_bayes_classifier.py | 273 | NBå®ç° |
| word2vec_svm_classifier.py | 450 | W2V+SVMå®ç° |
| bert_classifier.py | 348 | BERTå®ç° |
| train.py | 300 | è®­ç»ƒè„šæœ¬ |
| evaluate_stage2.py | 150 | è¯„ä¼°è„šæœ¬ |
| **æ€»è®¡** | **~1,520è¡Œ** | - |

### å¼€å‘æ—¶é—´ä¼°è®¡

- æœ´ç´ è´å¶æ–¯å®ç°: 4å°æ—¶
- Word2Vec+SVMå®ç°: 6å°æ—¶
- BERTå®ç°: 8å°æ—¶
- è®­ç»ƒè„šæœ¬: 3å°æ—¶
- æµ‹è¯•å’Œè°ƒè¯•: 6å°æ—¶
- æ–‡æ¡£: 3å°æ—¶
- **æ€»è®¡**: çº¦30å°æ—¶ (4ä¸ªå·¥ä½œæ—¥)

**æ³¨**: å› ä¸ºå¤ç”¨äº†Stage1çš„åŸºç¡€è®¾æ–½,å¤§å¤§èŠ‚çœäº†å¼€å‘æ—¶é—´ã€‚

## âœ… å®Œæˆæƒ…å†µ

- âœ… æœ´ç´ è´å¶æ–¯å®ç° (100%)
- âœ… Word2Vec+SVMå®ç° (100%)
- âœ… BERTå®ç° (100%)
- âœ… ç»Ÿä¸€æ¥å£è®¾è®¡ (100%)
- âœ… è®­ç»ƒå’Œè¯„ä¼°è„šæœ¬ (100%)
- âœ… æ¨¡å‹å¯¹æ¯”å®éªŒ (100%)
- âœ… æ–‡æ¡£ (100%)

**å®Œæˆåº¦**: 100%
**ä»£ç è´¨é‡**: â­â­â­â­
**æ€§èƒ½**: 73-87% (è¾¾åˆ°é¢„æœŸ)

## ğŸ“ æ€»ç»“

Stage2_Traditional_ModelsæˆåŠŸåœ°:

1. âœ… **æ¨¡å—åŒ–é‡æ„** - å°†Baselineçš„æ··æ‚ä»£ç æ‹†åˆ†ä¸ºç‹¬ç«‹æ¨¡å—
2. âœ… **ç»Ÿä¸€æ¥å£è®¾è®¡** - æ‰€æœ‰æ¨¡å‹å®ç°ç›¸åŒæ¥å£,ä¾¿äºå¯¹æ¯”å’Œæ›¿æ¢
3. âœ… **å»ºç«‹æ€§èƒ½åŸºå‡†** - ä¸‰ä¸ªæ¨¡å‹73-87%çš„å‡†ç¡®ç‡,ä¸ºåç»­ä¼˜åŒ–æä¾›å‚è€ƒ
4. âœ… **å¤ç”¨åŸºç¡€è®¾æ–½** - å……åˆ†åˆ©ç”¨Stage1çš„data_loader/evaluator/visualizer
5. âœ… **è¯†åˆ«ä¼˜åŒ–æ–¹å‘** - æœ´ç´ è´å¶æ–¯éœ€è¦ç‰¹å¾å·¥ç¨‹,BERTéœ€è¦é«˜çº§è®­ç»ƒæŠ€æœ¯

**æ ¸å¿ƒä»·å€¼**: ä»"åŸå‹"åˆ°"äº§å“"çš„è½¬å˜,ä»£ç è´¨é‡å’Œå¯ç»´æŠ¤æ€§å¤§å¹…æå‡ã€‚

**æ€§èƒ½æ€»ç»“**:
- æœ´ç´ è´å¶æ–¯: 73.46% (å¿«ä½†ä¸å¤Ÿå‡†)
- Word2Vec+SVM: 74.39% (å¹³è¡¡)
- BERT: 86.99% (æ…¢ä½†å‡†)

**åç»­å·¥ä½œ**:
- Stage3å°†ä¼˜åŒ–æœ´ç´ è´å¶æ–¯ â†’ 79.20%
- Stage4å°†ä¼˜åŒ–BERT â†’ 89-91%
- Stage5å°†å¼•å…¥LLMå®éªŒ â†’ æ¢ç´¢æ–°æ–¹å‘

---

**æŠ¥å‘Šå®Œæˆæ—¶é—´**: 2025-12-08
**æŠ¥å‘Šä½œè€…**: Task2é¡¹ç›®ç»„
**ä¸Šä¸€é˜¶æ®µ**: Stage1_Foundation - åŸºç¡€æ¡†æ¶æ­å»º
**ä¸‹ä¸€é˜¶æ®µ**: Stage3_NaiveBayes_Optimization - æœ´ç´ è´å¶æ–¯æ·±åº¦ä¼˜åŒ–
