"""
predownload_models.py
=====================
é¢„ä¸‹è½½æ‰€æœ‰BERTå®éªŒéœ€è¦çš„æ¨¡å‹åˆ°æœ¬åœ°ç¼“å­˜
è¿™æ ·è¿è¡Œå®éªŒæ—¶å°±ä¸éœ€è¦ä»ç½‘ç»œä¸‹è½½äº†

æ”¯æŒçš„æ¨¡å‹:
1. bert-base-uncased (BERT baseline)
2. allenai/scibert_scivocab_uncased (SciBERT - å­¦æœ¯ä¸“ç”¨)
3. roberta-base (RoBERTa)
"""

import os
import sys
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification
import torch


# æ‰€æœ‰éœ€è¦çš„æ¨¡å‹
REQUIRED_MODELS = {
    'bert-base': 'bert-base-uncased',
    'scibert': 'allenai/scibert_scivocab_uncased',
    'roberta': 'roberta-base',
}


def download_model(model_name: str, model_path: str, retry_count: int = 3):
    """ä¸‹è½½å•ä¸ªæ¨¡å‹(tokenizer + model)"""

    print(f"\n{'='*80}")
    print(f" ğŸ“¥ ä¸‹è½½æ¨¡å‹: {model_name}")
    print(f" è·¯å¾„: {model_path}")
    print(f"{'='*80}")

    for attempt in range(1, retry_count + 1):
        try:
            print(f"\nå°è¯• {attempt}/{retry_count}...")

            # 1. ä¸‹è½½tokenizer
            print(f"  [1/3] ä¸‹è½½tokenizer...")
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                force_download=False,  # ä½¿ç”¨ç¼“å­˜
                resume_download=True   # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
            )
            print(f"  âœ“ Tokenizerä¸‹è½½å®Œæˆ")

            # 2. ä¸‹è½½åŸºç¡€æ¨¡å‹(ç”¨äºç‰¹å¾æå–)
            print(f"  [2/3] ä¸‹è½½åŸºç¡€æ¨¡å‹...")
            base_model = AutoModel.from_pretrained(
                model_path,
                force_download=False,
                resume_download=True
            )
            print(f"  âœ“ åŸºç¡€æ¨¡å‹ä¸‹è½½å®Œæˆ")

            # 3. ä¸‹è½½åˆ†ç±»æ¨¡å‹
            print(f"  [3/3] ä¸‹è½½åˆ†ç±»æ¨¡å‹...")
            classification_model = AutoModelForSequenceClassification.from_pretrained(
                model_path,
                num_labels=2,
                force_download=False,
                resume_download=True
            )
            print(f"  âœ“ åˆ†ç±»æ¨¡å‹ä¸‹è½½å®Œæˆ")

            # æ£€æŸ¥æ¨¡å‹å¤§å°
            model_size_mb = sum(
                p.numel() * p.element_size()
                for p in classification_model.parameters()
            ) / (1024 * 1024)
            print(f"  ğŸ“Š æ¨¡å‹å¤§å°: {model_size_mb:.2f} MB")
            print(f"  ğŸ“Š å‚æ•°é‡: {sum(p.numel() for p in classification_model.parameters()):,}")

            print(f"\nâœ… {model_name} ä¸‹è½½æˆåŠŸ!")
            return True

        except Exception as e:
            print(f"\nâŒ ä¸‹è½½å¤±è´¥ (å°è¯• {attempt}/{retry_count}): {str(e)}")
            if attempt < retry_count:
                print(f"  â³ 5ç§’åé‡è¯•...")
                import time
                time.sleep(5)
            else:
                print(f"\nâŒ {model_name} ä¸‹è½½å¤±è´¥,å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°")
                return False

    return False


def check_model_cached(model_path: str) -> bool:
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç¼“å­˜"""
    try:
        # å°è¯•åŠ è½½tokenizer (æœ€å¿«çš„æ£€æŸ¥æ–¹å¼)
        AutoTokenizer.from_pretrained(model_path, local_files_only=True)
        return True
    except:
        return False


def get_cache_info():
    """è·å–HuggingFaceç¼“å­˜ä¿¡æ¯"""
    cache_dir = os.environ.get('HF_HOME') or os.path.join(os.path.expanduser('~'), '.cache', 'huggingface')
    print(f"\nğŸ“ HuggingFaceç¼“å­˜ç›®å½•: {cache_dir}")

    if os.path.exists(cache_dir):
        # è®¡ç®—ç¼“å­˜å¤§å°
        total_size = 0
        for dirpath, dirnames, filenames in os.walk(cache_dir):
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                try:
                    total_size += os.path.getsize(filepath)
                except:
                    pass

        print(f"ğŸ“Š å½“å‰ç¼“å­˜å¤§å°: {total_size / (1024**3):.2f} GB")
    else:
        print(f"âš ï¸  ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")


def main():
    """ä¸»å‡½æ•°"""

    print("\n" + "="*80)
    print(" ğŸš€ BERTæ¨¡å‹é¢„ä¸‹è½½å·¥å…·")
    print("="*80)

    # æ˜¾ç¤ºç¼“å­˜ä¿¡æ¯
    get_cache_info()

    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        print(f"\nâœ“ GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
        print(f"âœ“ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    else:
        print(f"\nâš ï¸  æœªæ£€æµ‹åˆ°GPU,ä½¿ç”¨CPU")

    print(f"\nğŸ“‹ éœ€è¦ä¸‹è½½çš„æ¨¡å‹:")
    for idx, (name, path) in enumerate(REQUIRED_MODELS.items(), 1):
        cached = check_model_cached(path)
        status = "âœ“ å·²ç¼“å­˜" if cached else "âœ— æœªç¼“å­˜"
        print(f"  {idx}. {name:12s} -> {path:40s} [{status}]")

    # è¯¢é—®ç”¨æˆ·
    print(f"\n{'='*80}")
    choice = input("æ˜¯å¦å¼€å§‹ä¸‹è½½? (y/n, é»˜è®¤y): ").strip().lower()
    if choice and choice != 'y':
        print("å–æ¶ˆä¸‹è½½")
        return

    # ä¸‹è½½æ‰€æœ‰æ¨¡å‹
    results = {}
    success_count = 0
    total_count = len(REQUIRED_MODELS)

    for model_name, model_path in REQUIRED_MODELS.items():
        # æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜
        if check_model_cached(model_path):
            print(f"\nâœ“ {model_name} å·²ç¼“å­˜,è·³è¿‡ä¸‹è½½")
            results[model_name] = 'cached'
            success_count += 1
            continue

        # ä¸‹è½½æ¨¡å‹
        success = download_model(model_name, model_path, retry_count=3)
        results[model_name] = 'success' if success else 'failed'
        if success:
            success_count += 1

    # æ‰“å°æ€»ç»“
    print(f"\n\n{'='*80}")
    print(" ğŸ“Š ä¸‹è½½æ€»ç»“")
    print(f"{'='*80}")

    for model_name, status in results.items():
        status_icon = {
            'cached': 'âœ“ å·²ç¼“å­˜',
            'success': 'âœ“ ä¸‹è½½æˆåŠŸ',
            'failed': 'âœ— ä¸‹è½½å¤±è´¥'
        }[status]
        print(f"  {model_name:12s}: {status_icon}")

    print(f"\næˆåŠŸ: {success_count}/{total_count}")

    if success_count == total_count:
        print("\nğŸ‰ æ‰€æœ‰æ¨¡å‹å‡†å¤‡å°±ç»ª!")
        print("ç°åœ¨å¯ä»¥è¿è¡Œå®éªŒäº†: python run_bert_experiments.py")
    else:
        print(f"\nâš ï¸  æœ‰ {total_count - success_count} ä¸ªæ¨¡å‹ä¸‹è½½å¤±è´¥")
        print("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥åé‡è¯•")

    # æ˜¾ç¤ºæœ€ç»ˆç¼“å­˜å¤§å°
    get_cache_info()


if __name__ == "__main__":
    main()
