"""
train_optimized_bert.py
=======================
ä½¿ç”¨ä¼˜åŒ–ç‰ˆBERTåˆ†ç±»å™¨è®­ç»ƒå­¦æœ¯æ ‡é¢˜åˆ†ç±»æ¨¡å‹
åŒ…å«å®Œæ•´çš„è¯„ä¼°æŠ¥å‘Šå’Œå¯è§†åŒ–è¾“å‡º
"""

import sys
import os

# æ·»åŠ  core ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'core'))

from data_loader import DataLoader
from optimized_BERT import BERTClassifierOptimized
from evaluator import ModelEvaluator
from visualizer import ResultVisualizer
import json


def main():
    print("=" * 80)
    print("  ä¼˜åŒ–ç‰ˆ BERT åˆ†ç±»å™¨ - å­¦æœ¯æ ‡é¢˜åˆ†ç±»")
    print("=" * 80)
    
    # ========== åŠ è½½æ•°æ® ==========
    print("\n[æ­¥éª¤ 1/5] åŠ è½½æ•°æ®é›†")
    print("-" * 80)

    try:
        train_titles, train_labels, test_titles, test_labels = DataLoader.prepare_dataset(
            'data/positive.txt',
            'data/negative.txt',
            'data/testSet-1000.xlsx'
        )

        print(f"âœ“ è®­ç»ƒé›†: {len(train_titles)} æ ·æœ¬")
        print(f"  - æ­£æ ·æœ¬: {sum(train_labels)} ({sum(train_labels)/len(train_labels)*100:.1f}%)")
        print(f"  - è´Ÿæ ·æœ¬: {len(train_labels)-sum(train_labels)} ({(len(train_labels)-sum(train_labels))/len(train_labels)*100:.1f}%)")
        print(f"âœ“ æµ‹è¯•é›†: {len(test_titles)} æ ·æœ¬")

    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # ========== åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹ ==========
    print("\n[æ­¥éª¤ 2/5] è®­ç»ƒä¼˜åŒ–ç‰ˆ BERT åˆ†ç±»å™¨")
    print("-" * 80)
    
    try:
        classifier = BERTClassifierOptimized(
            model_name='bert-base-uncased',
            max_length=64,
            use_fgm=True,
            use_ema=True
        )
        
        best_val_f1 = classifier.train(
            train_titles,
            train_labels,
            val_titles=None,
            val_labels=None,
            epochs=10,
            batch_size=16,
            learning_rate=2e-5,
            warmup_ratio=0.1,
            weight_decay=0.01,
            patience=3,
            use_focal_loss=False,
            augment_data=True
        )
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # ========== è¯„ä¼°æ¨¡å‹ ==========
    print("\n[æ­¥éª¤ 3/5] è¯„ä¼°æ¨¡å‹")
    print("-" * 80)
    
    try:
        # åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹
        print("\nåœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹...")
        predictions = classifier.predict(test_titles, batch_size=16)
        probabilities = classifier.predict_proba(test_titles, batch_size=16)
        
        # æ˜¾ç¤ºé¢„æµ‹ç»“æœç¤ºä¾‹
        print("\né¢„æµ‹ç»“æœç¤ºä¾‹:")
        print(f"{'æ ‡é¢˜':<50} {'çœŸå®':<8} {'é¢„æµ‹':<8} {'ç½®ä¿¡åº¦':<10}")
        print("-" * 80)
        
        for i in range(min(10, len(test_titles))):
            title = test_titles[i][:47] + "..." if len(test_titles[i]) > 50 else test_titles[i]
            true_label = "æ­£ç¡®" if test_labels[i] == 1 else "é”™è¯¯"
            pred_label = "æ­£ç¡®" if predictions[i] == 1 else "é”™è¯¯"
            confidence = probabilities[i][predictions[i]]
            
            print(f"{title:<50} {true_label:<8} {pred_label:<8} {confidence:.3f}")
        
        # ä½¿ç”¨ ModelEvaluator è¿›è¡Œè¯¦ç»†è¯„ä¼°
        print("\n" + "=" * 80)
        print("  è¯¦ç»†è¯„ä¼°æŠ¥å‘Š")
        print("=" * 80)
        
        evaluator = ModelEvaluator()
        result = evaluator.evaluate_model(
            test_labels, 
            predictions, 
            'BERT (Optimized)',
            verbose=True
        )
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # ========== ç”Ÿæˆå¯è§†åŒ– ==========
    print("\n[æ­¥éª¤ 4/5] ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
    print("-" * 80)
    
    try:
        output_dir = 'output'
        os.makedirs(output_dir, exist_ok=True)
        
        visualizer = ResultVisualizer()
        
        # 1. ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾
        print("\nç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾...")
        visualizer.plot_comparison(
            [result],
            save_path=os.path.join(output_dir, 'bert_performance.png')
        )
        
        # 2. ç”Ÿæˆæ··æ·†çŸ©é˜µ
        print("\nç”Ÿæˆæ··æ·†çŸ©é˜µ...")
        visualizer.plot_confusion_matrices(
            [result],
            save_path=os.path.join(output_dir, 'bert_confusion_matrix.png')
        )
        
        # 3. ç”Ÿæˆ t-SNE å¯è§†åŒ–
        print("\nç”Ÿæˆ t-SNE å¯è§†åŒ–å›¾ï¼ˆæå–ç‰¹å¾å‘é‡ï¼‰...")
        feature_vectors = classifier.get_feature_vectors(test_titles, batch_size=16)
        visualizer.visualize_embeddings_tsne(
            feature_vectors,
            test_labels,
            'BERT (Optimized)',
            save_path=os.path.join(output_dir, 'bert_tsne_visualization.png'),
            perplexity=30,
            n_iter=1000
        )
        
        print(f"\nâœ“ æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ° {output_dir}/ ç›®å½•")
        
    except Exception as e:
        print(f"âš ï¸  å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # ========== ä¿å­˜ç»“æœ ==========
    print("\n[æ­¥éª¤ 5/5] ä¿å­˜ç»“æœ")
    print("-" * 80)
    
    try:
        # ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœ
        results_file = os.path.join(output_dir, 'bert_evaluation_results.txt')
        with open(results_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write(" ä¼˜åŒ–ç‰ˆ BERT åˆ†ç±»å™¨ - è¯„ä¼°ç»“æœ\n")
            f.write("=" * 80 + "\n\n")
            
            f.write("ã€è®­ç»ƒä¿¡æ¯ã€‘\n")
            f.write(f"  è®­ç»ƒæ ·æœ¬æ•°: {len(train_titles)}\n")
            f.write(f"  æµ‹è¯•æ ·æœ¬æ•°: {len(test_titles)}\n")
            f.write(f"  éªŒè¯é›†æœ€ä½³ F1: {best_val_f1:.4f}\n\n")
            
            f.write("ã€æµ‹è¯•é›†æ€§èƒ½ã€‘\n")
            f.write(f"  å‡†ç¡®ç‡ (Accuracy):     {result['accuracy']:.4f} ({result['accuracy']*100:.2f}%)\n")
            f.write(f"  ç²¾ç¡®ç‡ (Precision):    {result['precision']:.4f}\n")
            f.write(f"  å¬å›ç‡ (Recall):       {result['recall']:.4f}\n")
            f.write(f"  F1åˆ†æ•° (F1-Score):     {result['f1']:.4f}\n")
            f.write(f"  F1å®å¹³å‡ (F1-Macro):   {result['f1_macro']:.4f}\n")
            f.write(f"  F1å¾®å¹³å‡ (F1-Micro):   {result['f1_micro']:.4f}\n\n")
            
            f.write("ã€æ··æ·†çŸ©é˜µã€‘\n")
            cm = result['confusion_matrix']
            f.write(f"  çœŸè´Ÿä¾‹ (TN): {result['tn']}\n")
            f.write(f"  å‡æ­£ä¾‹ (FP): {result['fp']}\n")
            f.write(f"  å‡è´Ÿä¾‹ (FN): {result['fn']}\n")
            f.write(f"  çœŸæ­£ä¾‹ (TP): {result['tp']}\n")
            f.write(f"  ç‰¹å¼‚åº¦ (Specificity):  {result['specificity']:.4f}\n")
            f.write(f"  æ•æ„Ÿåº¦ (Sensitivity):  {result['sensitivity']:.4f}\n")
        
        print(f"âœ“ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        # ä¿å­˜ JSON æ ¼å¼ç»“æœ
        json_results = {
            'model': 'BERT (Optimized)',
            'training_info': {
                'train_samples': len(train_titles),
                'test_samples': len(test_titles),
                'best_val_f1': float(best_val_f1)
            },
            'test_performance': {
                'accuracy': float(result['accuracy']),
                'precision': float(result['precision']),
                'recall': float(result['recall']),
                'f1_score': float(result['f1']),
                'f1_macro': float(result['f1_macro']),
                'f1_micro': float(result['f1_micro'])
            },
            'confusion_matrix': {
                'tn': int(result['tn']),
                'fp': int(result['fp']),
                'fn': int(result['fn']),
                'tp': int(result['tp']),
                'specificity': float(result['specificity']),
                'sensitivity': float(result['sensitivity'])
            }
        }
        
        json_file = os.path.join(output_dir, 'bert_results.json')
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ“ JSON ç»“æœå·²ä¿å­˜åˆ°: {json_file}")
        
    except Exception as e:
        print(f"âš ï¸  ç»“æœä¿å­˜å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # ========== æœ€ç»ˆæ€»ç»“ ==========
    print("\n" + "=" * 80)
    print("  è®­ç»ƒå®Œæˆï¼")
    print("=" * 80)
    print(f"\nğŸ“Š æ¨¡å‹æ€§èƒ½:")
    print(f"  éªŒè¯é›†æœ€ä½³ F1:   {best_val_f1:.4f}")
    print(f"  æµ‹è¯•é›†å‡†ç¡®ç‡:     {result['accuracy']:.4f} ({result['accuracy']*100:.2f}%)")
    print(f"  æµ‹è¯•é›† F1 åˆ†æ•°:   {result['f1']:.4f}")
    
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"  1. {output_dir}/bert_performance.png - æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”å›¾")
    print(f"  2. {output_dir}/bert_confusion_matrix.png - æ··æ·†çŸ©é˜µå›¾")
    print(f"  3. {output_dir}/bert_tsne_visualization.png - t-SNE å¯è§†åŒ–å›¾")
    print(f"  4. {output_dir}/bert_evaluation_results.txt - è¯¦ç»†è¯„ä¼°æŠ¥å‘Š")
    print(f"  5. {output_dir}/bert_results.json - JSON æ ¼å¼ç»“æœ")
    
    print("\n" + "=" * 80)
    print("  æ„Ÿè°¢ä½¿ç”¨ä¼˜åŒ–ç‰ˆ BERT åˆ†ç±»å™¨ï¼")
    print("=" * 80 + "\n")


if __name__ == "__main__":
    main()
