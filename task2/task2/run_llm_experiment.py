"""
run_llm_experiment.py
=====================
çµæ´»çš„LLMåˆ†ç±»å®éªŒè„šæœ¬
é€šè¿‡ä¿®æ”¹é…ç½®æ–‡ä»¶å³å¯åˆ‡æ¢æ¨¡å‹ï¼Œæ— éœ€ä¿®æ”¹ä»£ç 

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ é…ç½®è¯´æ˜ï¼šæ‰€æœ‰æ¨¡å‹é…ç½®éƒ½åœ¨å¤–éƒ¨JSONæ–‡ä»¶ä¸­
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1ï¸âƒ£ é…ç½®æ–‡ä»¶ä½ç½®ï¼š
   - æ–‡ä»¶åï¼šllm_config.jsonï¼ˆéœ€è‡ªè¡Œåˆ›å»ºï¼‰
   - æ¨¡æ¿ï¼šllm_config_template.json
   - ä½ç½®ï¼šä¸æœ¬è„šæœ¬åŒç›®å½•

2ï¸âƒ£ å¦‚ä½•æ·»åŠ /ä¿®æ”¹æ¨¡å‹ï¼š
   æ­¥éª¤1ï¼šæ‰“å¼€ llm_config.json
   æ­¥éª¤2ï¼šåœ¨ "llms" éƒ¨åˆ†æ·»åŠ æˆ–ä¿®æ”¹æ¨¡å‹é…ç½®
   æ­¥éª¤3ï¼šå¡«å†™ api_keyã€model ç­‰å‚æ•°
   æ­¥éª¤4ï¼šè®¾ç½® "enabled": true å¯ç”¨æ¨¡å‹

3ï¸âƒ£ é…ç½®ç¤ºä¾‹ï¼š
   {
     "llms": {
       "my-model": {                          // æ¨¡å‹åç§°ï¼ˆè‡ªå®šä¹‰ï¼‰
         "provider": "openai",                // APIç±»å‹ï¼ˆopenai/anthropicï¼‰
         "model": "gpt-3.5-turbo",           // å®é™…æ¨¡å‹å
         "api_key": "sk-xxxx",               // ğŸ”‘ APIå¯†é’¥ï¼ˆå¿…å¡«ï¼‰
         "base_url": "https://api.xxx.com",  // APIç«¯ç‚¹ï¼ˆå¯é€‰ï¼‰
         "temperature": 0.0,                  // ç”Ÿæˆæ¸©åº¦
         "max_tokens": 150,                   // æœ€å¤§è¾“å‡ºtoken
         "enabled": true                      // âœ… æ˜¯å¦å¯ç”¨
       }
     }
   }

4ï¸âƒ£ ä¸éœ€è¦ä¿®æ”¹æœ¬Pythonæ–‡ä»¶ä¸­çš„ä»»ä½•ä»£ç ï¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ä½¿ç”¨æ–¹æ³•:
1. ç¼–è¾‘ llm_config.jsonï¼Œé…ç½®æƒ³è¦ä½¿ç”¨çš„æ¨¡å‹
2. è¿è¡Œ: python run_llm_experiment.py
3. æˆ–è€…æŒ‡å®šæ¨¡å‹: python run_llm_experiment.py --model glm-4.6
4. æˆ–è€…è¿è¡Œæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹: python run_llm_experiment.py --all

ç‰¹æ€§:
- æ”¯æŒå•æ¨¡å‹/å¤šæ¨¡å‹å¯¹æ¯”å®éªŒ
- çµæ´»çš„é…ç½®ç³»ç»Ÿ
- è‡ªåŠ¨ä¿å­˜è¯¦ç»†ç»“æœ
- æˆæœ¬ä¼°ç®—
- é”™è¯¯å¤„ç†å’Œæ–­ç‚¹ç»­ä¼ 
"""

import os
import sys
import json
import time
import argparse
import numpy as np
from typing import List, Dict, Optional, Tuple
from tqdm import tqdm
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

from data_loader import DataLoader
from evaluator import ModelEvaluator


class LLMClassifier:
    """ç»Ÿä¸€çš„LLMåˆ†ç±»å™¨ï¼Œæ”¯æŒå¤šç§provider"""

    def __init__(self, config: Dict, model_name: str = "unnamed"):
        """
        åˆå§‹åŒ–LLMåˆ†ç±»å™¨

        Args:
            config: æ¨¡å‹é…ç½®å­—å…¸
            model_name: æ¨¡å‹æ˜¾ç¤ºåç§°
        """
        self.config = config
        self.model_name = model_name
        self.provider = config["provider"]
        self.model = config["model"]
        self.temperature = config.get("temperature", 0.0)
        self.max_tokens = config.get("max_tokens", 150)
        self.api_key = config["api_key"]
        self.base_url = config.get("base_url", None)

        # Few-shoté…ç½®
        self.few_shot_count = config.get("few_shot_examples", 8)
        self.examples = []

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_calls": 0,
            "total_tokens": 0,
            "total_input_tokens": 0,
            "total_output_tokens": 0,
            "errors": 0,
            "total_time": 0.0,
            "failed_indices": []
        }

        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self._init_client()

        print(f"âœ“ {self.model_name} åˆå§‹åŒ–æˆåŠŸ")
        print(f"  æ¨¡å‹: {self.model}")
        print(f"  Temperature: {self.temperature}")
        print(f"  Max tokens: {self.max_tokens}")

    def _init_client(self):
        """åˆå§‹åŒ–å¯¹åº”çš„APIå®¢æˆ·ç«¯"""
        if self.provider == "openai":
            try:
                from openai import OpenAI
                client_params = {"api_key": self.api_key}
                if self.base_url:
                    client_params["base_url"] = self.base_url
                self.client = OpenAI(**client_params)
            except ImportError:
                raise ImportError("è¯·å®‰è£…openai: pip install openai>=1.0.0")
        elif self.provider == "anthropic":
            try:
                import anthropic
                self.client = anthropic.Anthropic(api_key=self.api_key)
            except ImportError:
                raise ImportError("è¯·å®‰è£…anthropic: pip install anthropic")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„provider: {self.provider}")

    def set_few_shot_examples(self, examples: List[Dict]):
        """è®¾ç½®Few-shotç¤ºä¾‹"""
        self.examples = examples[:self.few_shot_count]
        print(f"âœ“ å·²åŠ è½½ {len(self.examples)} ä¸ªFew-shotç¤ºä¾‹")

    def _create_prompt(self, title: str) -> str:
        """åˆ›å»ºåˆ†ç±»Prompt"""
        prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡æ ‡é¢˜è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ¤æ–­ç»™å®šçš„æ ‡é¢˜æ˜¯å¦ä¸º**æ­£ç¡®æå–**çš„å­¦æœ¯è®ºæ–‡æ ‡é¢˜ã€‚

ã€åˆ†ç±»æ ‡å‡†ã€‘
âœ“ æ­£ç¡®æ ‡é¢˜ï¼ˆ1ï¼‰ï¼š
  - å®Œæ•´ã€æ¸…æ™°çš„å­¦æœ¯è®ºæ–‡æ ‡é¢˜
  - è¯­æ³•æ­£ç¡®ï¼Œè¡¨è¾¾å‡†ç¡®
  - ä¸åŒ…å«é¡µç ã€æ‘˜è¦ã€ç« èŠ‚ç¼–å·ç­‰éæ ‡é¢˜å†…å®¹

âœ— é”™è¯¯æ ‡é¢˜ï¼ˆ0ï¼‰ï¼š
  - åŒ…å«é¡µç ï¼ˆå¦‚"pp. 123-145"ï¼‰
  - åŒ…å«æ‘˜è¦ç‰‡æ®µï¼ˆå¦‚"Abstract: ..."ï¼‰
  - åŒ…å«ç« èŠ‚æ ‡è®°ï¼ˆå¦‚"1. Introduction"ï¼‰
  - åŒ…å«æœŸåˆŠä¿¡æ¯ï¼ˆå¦‚"Vol. 25, No. 3"ï¼‰
  - åŒ…å«æ ¼å¼é”™è¯¯ï¼ˆå¦‚"......"è¿ç»­ç‚¹å·ï¼‰
  - åŒ…å«"Reference"ã€"Appendix"ç­‰å…³é”®è¯

ã€å‚è€ƒç¤ºä¾‹ã€‘

"""
        # æ·»åŠ Few-shotç¤ºä¾‹
        for i, ex in enumerate(self.examples, 1):
            label_symbol = "âœ“" if ex["label"] == 1 else "âœ—"
            label_text = "æ­£ç¡®æ ‡é¢˜" if ex["label"] == 1 else "é”™è¯¯æ ‡é¢˜"
            prompt += f"ç¤ºä¾‹ {i}:\n"
            prompt += f"æ ‡é¢˜ï¼šã€Œ{ex['title']}ã€\n"
            prompt += f"åˆ¤æ–­ï¼š{label_symbol} {label_text}\n"
            if "reason" in ex:
                prompt += f"ç†ç”±ï¼š{ex['reason']}\n"
            prompt += "\n"

        # å¾…åˆ†ç±»æ ‡é¢˜
        prompt += "=" * 60 + "\n"
        prompt += "ã€ç°åœ¨è¯·åˆ¤æ–­ä»¥ä¸‹æ ‡é¢˜ã€‘\n\n"
        prompt += f"æ ‡é¢˜ï¼šã€Œ{title}ã€\n"
        prompt += f"åˆ¤æ–­ï¼š\n\n"
        prompt += "è¯·åªå›ç­”\"âœ“ æ­£ç¡®æ ‡é¢˜\"æˆ–\"âœ— é”™è¯¯æ ‡é¢˜\"ã€‚"

        return prompt

    def _parse_response(self, response: str) -> int:
        """è§£æLLMå“åº”ä¸ºæ ‡ç­¾"""
        response_lower = response.lower()

        if "âœ“" in response or "æ­£ç¡®æ ‡é¢˜" in response or "correct" in response_lower:
            return 1
        elif "âœ—" in response or "é”™è¯¯æ ‡é¢˜" in response or "incorrect" in response_lower:
            return 0
        else:
            # å°è¯•ä»æ•°å­—è§£æ
            if "1" in response[:10]:
                return 1
            elif "0" in response[:10]:
                return 0
            else:
                return 0  # é»˜è®¤ä¿å®ˆç­–ç•¥

    def _call_api(self, prompt: str) -> Dict:
        """è°ƒç”¨API"""
        start_time = time.time()

        try:
            if self.provider == "openai":
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‡†ç¡®ã€ä¸“ä¸šçš„å­¦æœ¯æ ‡é¢˜åˆ†ç±»ä¸“å®¶ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=self.temperature,
                    max_tokens=self.max_tokens
                )

                return {
                    "response": response.choices[0].message.content.strip(),
                    "tokens": response.usage.total_tokens if response.usage else 0,
                    "input_tokens": response.usage.prompt_tokens if response.usage else 0,
                    "output_tokens": response.usage.completion_tokens if response.usage else 0,
                    "time": time.time() - start_time
                }

            elif self.provider == "anthropic":
                message = self.client.messages.create(
                    model=self.model,
                    max_tokens=self.max_tokens,
                    temperature=self.temperature,
                    messages=[{"role": "user", "content": prompt}]
                )

                return {
                    "response": message.content[0].text,
                    "tokens": message.usage.input_tokens + message.usage.output_tokens,
                    "input_tokens": message.usage.input_tokens,
                    "output_tokens": message.usage.output_tokens,
                    "time": time.time() - start_time
                }

        except Exception as e:
            return {
                "response": "",
                "tokens": 0,
                "input_tokens": 0,
                "output_tokens": 0,
                "time": time.time() - start_time,
                "error": str(e)
            }

    def predict(
        self,
        titles: List[str],
        delay: float = 0.5,
        verbose: bool = False,
        save_checkpoints: bool = True,
        checkpoint_interval: int = 100
    ) -> Tuple[np.ndarray, List[Dict]]:
        """
        æ‰¹é‡é¢„æµ‹

        Args:
            titles: æ ‡é¢˜åˆ—è¡¨
            delay: APIè°ƒç”¨é—´éš”ï¼ˆç§’ï¼‰
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
            save_checkpoints: æ˜¯å¦ä¿å­˜æ£€æŸ¥ç‚¹
            checkpoint_interval: æ£€æŸ¥ç‚¹é—´éš”

        Returns:
            (predictions, details)
        """
        print(f"\n{'='*80}")
        print(f"å¼€å§‹é¢„æµ‹: {self.model_name}")
        print(f"{'='*80}")
        print(f"æ ·æœ¬æ•°: {len(titles)}")
        print(f"Few-shotç¤ºä¾‹æ•°: {len(self.examples)}")
        print(f"Temperature: {self.temperature}")
        print(f"{'='*80}\n")

        predictions = []
        details = []

        for i, title in enumerate(tqdm(titles, desc=f"{self.model_name} é¢„æµ‹è¿›åº¦")):
            # åˆ›å»ºprompt
            prompt = self._create_prompt(title)

            # è°ƒç”¨API
            result = self._call_api(prompt)

            # è§£æå“åº”
            if "error" not in result:
                label = self._parse_response(result["response"])
                self.stats["total_calls"] += 1
                self.stats["total_tokens"] += result["tokens"]
                self.stats["total_input_tokens"] += result.get("input_tokens", 0)
                self.stats["total_output_tokens"] += result.get("output_tokens", 0)
                self.stats["total_time"] += result["time"]
            else:
                label = 0  # é”™è¯¯æ—¶é»˜è®¤ä¸º0
                self.stats["errors"] += 1
                self.stats["failed_indices"].append(i)
                if verbose:
                    print(f"\nâš ï¸  ç¬¬ {i+1} ä¸ªæ ·æœ¬è°ƒç”¨å¤±è´¥: {result['error']}")

            predictions.append(label)

            # ä¿å­˜è¯¦ç»†ä¿¡æ¯
            details.append({
                "index": i,
                "title": title,
                "pred_label": label,
                "response": result["response"],
                "tokens": result.get("tokens", 0),
                "input_tokens": result.get("input_tokens", 0),
                "output_tokens": result.get("output_tokens", 0),
                "time": result.get("time", 0.0),
                "error": result.get("error", None)
            })

            # ä¿å­˜æ£€æŸ¥ç‚¹
            if save_checkpoints and (i + 1) % checkpoint_interval == 0:
                checkpoint_file = f"checkpoints/{self.model_name}_checkpoint_{i+1}.json"
                os.makedirs("checkpoints", exist_ok=True)
                with open(checkpoint_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        "predictions": predictions,
                        "details": details,
                        "stats": self.stats
                    }, f, ensure_ascii=False, indent=2)
                print(f"\nâœ“ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_file}")

            # APIé™æµå»¶è¿Ÿ
            if i < len(titles) - 1:
                time.sleep(delay)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\n{'='*80}")
        print("é¢„æµ‹å®Œæˆç»Ÿè®¡")
        print(f"{'='*80}")
        print(f"æ€»è°ƒç”¨æ¬¡æ•°: {self.stats['total_calls']}")
        print(f"æˆåŠŸæ¬¡æ•°: {self.stats['total_calls'] - self.stats['errors']}")
        print(f"å¤±è´¥æ¬¡æ•°: {self.stats['errors']}")
        print(f"æ€»Tokenæ¶ˆè€—: {self.stats['total_tokens']}")
        print(f"  è¾“å…¥Token: {self.stats['total_input_tokens']}")
        print(f"  è¾“å‡ºToken: {self.stats['total_output_tokens']}")
        print(f"æ€»æ—¶é—´: {self.stats['total_time']:.2f}ç§’")
        print(f"å¹³å‡æ—¶é—´: {self.stats['total_time']/self.stats['total_calls']:.2f}ç§’/æ ·æœ¬" if self.stats['total_calls'] > 0 else "N/A")
        print(f"é¢„æµ‹ä¸ºæ­£ç±»: {sum(predictions)} ({sum(predictions)/len(predictions)*100:.1f}%)")
        print(f"é¢„æµ‹ä¸ºè´Ÿç±»: {len(predictions)-sum(predictions)} ({(len(predictions)-sum(predictions))/len(predictions)*100:.1f}%)")
        print(f"{'='*80}\n")

        return np.array(predictions), details


def get_default_few_shot_examples() -> List[Dict]:
    """
    è·å–é»˜è®¤çš„Few-shotç¤ºä¾‹

    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    ğŸ“š Few-shotç¤ºä¾‹é…ç½®ä½ç½®ï¼ˆå¦‚éœ€ä¿®æ”¹ç¤ºä¾‹å†…å®¹ï¼‰
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    å¦‚æœéœ€è¦ä¿®æ”¹Few-shotç¤ºä¾‹çš„å†…å®¹ï¼ˆæ­£è´Ÿä¾‹æ ·æœ¬ï¼‰ï¼Œè¯·ä¿®æ”¹ä¸‹æ–¹çš„åˆ—è¡¨ã€‚

    ç¤ºä¾‹æ•°é‡åœ¨é…ç½®æ–‡ä»¶ä¸­æ§åˆ¶ï¼š
        "experiment": {
            "few_shot_examples": 8  // ä½¿ç”¨å‰8ä¸ªç¤ºä¾‹
        }

    ç¤ºä¾‹æ ¼å¼ï¼š
        {
            "title": "æ ‡é¢˜æ–‡æœ¬",
            "label": 1,  // 1=æ­£ç¡®, 0=é”™è¯¯
            "reason": "åˆ¤æ–­ç†ç”±"
        }

    âš ï¸ æ³¨æ„ï¼šä¸€èˆ¬æƒ…å†µä¸‹ä¸éœ€è¦ä¿®æ”¹æ­¤å¤„ï¼Œä½¿ç”¨é»˜è®¤ç¤ºä¾‹å³å¯
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    """
    return [
        {
            "title": "Deep Learning for Natural Language Processing: A Survey",
            "label": 1,
            "reason": "å®Œæ•´è§„èŒƒçš„å­¦æœ¯è®ºæ–‡æ ‡é¢˜"
        },
        {
            "title": "Machine Translation Using Neural Networks",
            "label": 1,
            "reason": "æ¸…æ™°å‡†ç¡®çš„ç ”ç©¶æ ‡é¢˜"
        },
        {
            "title": "A Comparative Study of Sentiment Analysis Methods",
            "label": 1,
            "reason": "æ ‡å‡†çš„å­¦æœ¯è®ºæ–‡æ ‡é¢˜æ ¼å¼"
        },
        {
            "title": "pp. 123-145 Introduction to Machine Learning",
            "label": 0,
            "reason": "åŒ…å«é¡µç ä¿¡æ¯ï¼Œéæ ‡å‡†æ ‡é¢˜"
        },
        {
            "title": "Abstract: This paper presents a new method",
            "label": 0,
            "reason": "åŒ…å«'Abstract'ï¼Œæ˜¯æ‘˜è¦ç‰‡æ®µ"
        },
        {
            "title": "A Novel Approach to Deep Learning......",
            "label": 0,
            "reason": "åŒ…å«è¿ç»­ç‚¹å·ï¼Œç–‘ä¼¼æå–é”™è¯¯"
        },
        {
            "title": "Vol. 25, No. 3, 2024 - Neural Networks",
            "label": 0,
            "reason": "åŒ…å«æœŸåˆŠå·å·ä¿¡æ¯"
        },
        {
            "title": "1. Introduction Recent advances in deep learning",
            "label": 0,
            "reason": "åŒ…å«ç« èŠ‚ç¼–å·"
        },
    ]


def load_config(config_file: str = "llm_config.json") -> Dict:
    """
    åŠ è½½é…ç½®æ–‡ä»¶

    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    ğŸ”§ é…ç½®æ–‡ä»¶åŠ è½½ä½ç½®ï¼ˆé‡è¦ï¼ï¼‰
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    é»˜è®¤é…ç½®æ–‡ä»¶ï¼šllm_config.json

    å¦‚æœè¦ä½¿ç”¨ä¸åŒçš„é…ç½®æ–‡ä»¶ï¼Œå¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šï¼š
        python run_llm_experiment.py --config my_config.json

    é…ç½®æ–‡ä»¶ç»“æ„ï¼š
        {
            "llms": {
                "æ¨¡å‹å": {
                    "provider": "openai",
                    "model": "æ¨¡å‹ID",
                    "api_key": "ä½ çš„å¯†é’¥",
                    "enabled": true
                }
            },
            "experiment": {
                "few_shot_examples": 8,
                "sample_size": 976,
                "delay_between_calls": 0.5
            }
        }
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    """
    if not os.path.exists(config_file):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")

    with open(config_file, 'r', encoding='utf-8') as f:
        config = json.load(f)

    return config


def validate_api_keys(config: Dict) -> List[str]:
    """éªŒè¯APIå¯†é’¥ï¼Œè¿”å›å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    available_models = []

    for name, llm_config in config["llms"].items():
        if not llm_config.get("enabled", True):
            continue

        api_key = llm_config.get("api_key", "")
        if api_key and "YOUR_" not in api_key and len(api_key) > 10:
            available_models.append(name)

    return available_models


def run_single_experiment(
    model_name: str,
    config: Dict,
    test_titles: List[str],
    test_labels: List[int],
    sample_size: Optional[int] = None
) -> Dict:
    """
    è¿è¡Œå•ä¸ªæ¨¡å‹çš„å®éªŒ

    Returns:
        {
            "predictions": np.ndarray,
            "eval_result": dict,
            "details": list,
            "stats": dict
        }
    """
    # è·å–æ¨¡å‹é…ç½®
    llm_config = config["llms"][model_name]

    # é‡‡æ ·
    if sample_size and sample_size < len(test_titles):
        test_titles = test_titles[:sample_size]
        test_labels = test_labels[:sample_size]

    # åˆå§‹åŒ–åˆ†ç±»å™¨
    classifier = LLMClassifier(llm_config, model_name=model_name)

    # è®¾ç½®Few-shotç¤ºä¾‹
    few_shot_count = config["experiment"].get("few_shot_examples", 8)
    examples = get_default_few_shot_examples()[:few_shot_count]
    classifier.set_few_shot_examples(examples)

    # é¢„æµ‹
    delay = config["experiment"].get("delay_between_calls", 0.5)
    predictions, details = classifier.predict(
        test_titles,
        delay=delay,
        save_checkpoints=True,
        checkpoint_interval=100
    )

    # è¯„ä¼°
    evaluator = ModelEvaluator()
    eval_result = evaluator.evaluate_model(
        test_labels,
        predictions,
        model_name=model_name,
        verbose=True
    )

    return {
        "predictions": predictions,
        "eval_result": eval_result,
        "details": details,
        "stats": classifier.stats
    }


def save_experiment_results(
    results: Dict,
    model_name: str,
    output_dir: str = "output/llm_experiments"
):
    """ä¿å­˜å®éªŒç»“æœ"""
    os.makedirs(output_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
    def convert_to_serializable(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, dict):
            return {key: convert_to_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_serializable(item) for item in obj]
        else:
            return obj

    # 1. ä¿å­˜JSONæ ¼å¼è¯¦ç»†ç»“æœ
    json_file = os.path.join(output_dir, f"{model_name}_{timestamp}.json")
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump({
            "model": model_name,
            "timestamp": timestamp,
            "eval_metrics": convert_to_serializable(results["eval_result"]),
            "stats": results["stats"],
            "predictions": results["predictions"].tolist(),
            "details": convert_to_serializable(results["details"])
        }, f, ensure_ascii=False, indent=2)
    print(f"\nâœ“ è¯¦ç»†ç»“æœå·²ä¿å­˜: {json_file}")

    # 2. ä¿å­˜æ–‡æœ¬æ ¼å¼æŠ¥å‘Š
    report_file = os.path.join(output_dir, f"{model_name}_{timestamp}_report.txt")
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write(f"{model_name} å®éªŒæŠ¥å‘Š\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"å®éªŒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ¨¡å‹: {model_name}\n\n")

        f.write("=" * 80 + "\n")
        f.write("æ€§èƒ½æŒ‡æ ‡\n")
        f.write("=" * 80 + "\n")
        metrics = results["eval_result"]
        f.write(f"å‡†ç¡®ç‡ (Accuracy):  {metrics['accuracy']*100:.2f}%\n")
        f.write(f"ç²¾ç¡®ç‡ (Precision): {metrics['precision']*100:.2f}%\n")
        f.write(f"å¬å›ç‡ (Recall):    {metrics['recall']*100:.2f}%\n")
        f.write(f"F1åˆ†æ•° (F1):        {metrics['f1']*100:.2f}%\n")
        f.write(f"F1å®å¹³å‡:           {metrics['f1_macro']*100:.2f}%\n")
        f.write(f"F1å¾®å¹³å‡:           {metrics['f1_micro']*100:.2f}%\n\n")

        f.write("=" * 80 + "\n")
        f.write("è¿è¡Œç»Ÿè®¡\n")
        f.write("=" * 80 + "\n")
        stats = results["stats"]
        f.write(f"æ€»è°ƒç”¨æ¬¡æ•°: {stats['total_calls']}\n")
        f.write(f"æˆåŠŸæ¬¡æ•°: {stats['total_calls'] - stats['errors']}\n")
        f.write(f"å¤±è´¥æ¬¡æ•°: {stats['errors']}\n")
        f.write(f"æˆåŠŸç‡: {(stats['total_calls']-stats['errors'])/stats['total_calls']*100:.1f}%\n" if stats['total_calls'] > 0 else "æˆåŠŸç‡: N/A\n")
        f.write(f"æ€»Tokenæ¶ˆè€—: {stats['total_tokens']}\n")
        f.write(f"  è¾“å…¥Token: {stats['total_input_tokens']}\n")
        f.write(f"  è¾“å‡ºToken: {stats['total_output_tokens']}\n")
        f.write(f"æ€»æ—¶é—´: {stats['total_time']:.2f}ç§’\n")
        f.write(f"å¹³å‡æ—¶é—´: {stats['total_time']/stats['total_calls']:.2f}ç§’/æ ·æœ¬\n" if stats['total_calls'] > 0 else "å¹³å‡æ—¶é—´: N/A\n")

    print(f"âœ“ å®éªŒæŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    return json_file, report_file


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="çµæ´»çš„LLMåˆ†ç±»å®éªŒè„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä½¿ç”¨é»˜è®¤é…ç½®è¿è¡Œï¼ˆé€‰æ‹©ä¸€ä¸ªæ¨¡å‹ï¼‰
  python run_llm_experiment.py

  # è¿è¡ŒæŒ‡å®šæ¨¡å‹
  python run_llm_experiment.py --model glm-4.6

  # è¿è¡Œæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
  python run_llm_experiment.py --all

  # æŒ‡å®šæ ·æœ¬æ•°
  python run_llm_experiment.py --model deepseek --sample 100

  # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®æ–‡ä»¶
  python run_llm_experiment.py --config my_config.json
        """
    )

    parser.add_argument(
        "--model",
        type=str,
        help="æŒ‡å®šæ¨¡å‹åç§°ï¼ˆåœ¨é…ç½®æ–‡ä»¶ä¸­å®šä¹‰ï¼‰"
    )
    parser.add_argument(
        "--all",
        action="store_true",
        help="è¿è¡Œæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹"
    )
    parser.add_argument(
        "--config",
        type=str,
        default="llm_config.json",
        help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: llm_config.jsonï¼‰"
    )
    parser.add_argument(
        "--sample",
        type=int,
        help="æµ‹è¯•æ ·æœ¬æ•°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼‰"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="output/llm_experiments",
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: output/llm_experimentsï¼‰"
    )

    args = parser.parse_args()

    print("=" * 80)
    print(" " * 25 + "LLMåˆ†ç±»å®éªŒ")
    print("=" * 80)

    # 1. åŠ è½½é…ç½®
    print("\n[æ­¥éª¤ 1/4] åŠ è½½é…ç½®")
    print("-" * 80)

    try:
        config = load_config(args.config)
        print(f"âœ“ é…ç½®æ–‡ä»¶å·²åŠ è½½: {args.config}")
    except FileNotFoundError as e:
        print(f"âŒ {e}")
        sys.exit(1)

    # 2. éªŒè¯APIå¯†é’¥
    available_models = validate_api_keys(config)

    if not available_models:
        print("\nâŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼è¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®APIå¯†é’¥å¹¶å¯ç”¨æ¨¡å‹ã€‚")
        sys.exit(1)

    print(f"âœ“ å¯ç”¨æ¨¡å‹: {', '.join(available_models)}")

    # 3. é€‰æ‹©æ¨¡å‹
    if args.all:
        selected_models = available_models
        print(f"\nå°†è¿è¡Œæ‰€æœ‰ {len(selected_models)} ä¸ªæ¨¡å‹")
    elif args.model:
        if args.model not in available_models:
            print(f"\nâŒ æ¨¡å‹ '{args.model}' ä¸å¯ç”¨")
            print(f"å¯ç”¨æ¨¡å‹: {', '.join(available_models)}")
            sys.exit(1)
        selected_models = [args.model]
        print(f"\nå°†è¿è¡Œæ¨¡å‹: {args.model}")
    else:
        # äº¤äº’å¼é€‰æ‹©
        print("\nè¯·é€‰æ‹©è¦è¿è¡Œçš„æ¨¡å‹:")
        for i, model in enumerate(available_models, 1):
            model_info = config["llms"][model]
            print(f"  {i}. {model} ({model_info.get('comment', model_info['model'])})")
        print(f"  {len(available_models)+1}. è¿è¡Œæ‰€æœ‰æ¨¡å‹")

        try:
            choice = int(input("\nè¯·è¾“å…¥é€‰é¡¹ (1-{}): ".format(len(available_models)+1)))
            if choice == len(available_models) + 1:
                selected_models = available_models
            elif 1 <= choice <= len(available_models):
                selected_models = [available_models[choice-1]]
            else:
                print("æ— æ•ˆé€‰é¡¹")
                sys.exit(1)
        except (ValueError, KeyboardInterrupt):
            print("\nå·²å–æ¶ˆ")
            sys.exit(0)

    # 4. åŠ è½½æ•°æ®
    print("\n[æ­¥éª¤ 2/4] åŠ è½½æµ‹è¯•æ•°æ®")
    print("-" * 80)

    try:
        _, _, test_titles, test_labels = DataLoader.prepare_dataset(
            'data/positive.txt',
            'data/negative.txt',
            'data/testSet-1000.xlsx'
        )
        print(f"âœ“ æµ‹è¯•é›†: {len(test_titles)} æ ·æœ¬")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        sys.exit(1)

    # 5. ç¡®å®šæ ·æœ¬æ•°
    sample_size = args.sample or config["experiment"].get("sample_size", len(test_titles))
    if sample_size > len(test_titles):
        sample_size = len(test_titles)

    print(f"âœ“ å°†ä½¿ç”¨ {sample_size} ä¸ªæ ·æœ¬")

    # 6. è¿è¡Œå®éªŒ
    print("\n[æ­¥éª¤ 3/4] è¿è¡Œå®éªŒ")
    print("-" * 80)

    all_results = {}

    for model_name in selected_models:
        try:
            print(f"\n{'='*80}")
            print(f"å®éªŒ: {model_name}")
            print(f"{'='*80}")

            results = run_single_experiment(
                model_name,
                config,
                test_titles,
                test_labels,
                sample_size
            )

            all_results[model_name] = results

            # ä¿å­˜ç»“æœ
            save_experiment_results(results, model_name, args.output)

        except Exception as e:
            print(f"\nâŒ {model_name} å®éªŒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue

    # 7. æ€»ç»“
    print("\n[æ­¥éª¤ 4/4] å®éªŒæ€»ç»“")
    print("-" * 80)

    if len(all_results) == 0:
        print("\nâŒ æ‰€æœ‰å®éªŒéƒ½å¤±è´¥äº†")
        sys.exit(1)

    print(f"\n{'æ¨¡å‹':<20} {'å‡†ç¡®ç‡':>10} {'F1åˆ†æ•°':>10} {'Tokenæ¶ˆè€—':>12} {'å¹³å‡è€—æ—¶':>12}")
    print("-" * 70)

    for model_name, results in all_results.items():
        metrics = results["eval_result"]
        stats = results["stats"]
        avg_time = stats["total_time"] / stats["total_calls"] if stats["total_calls"] > 0 else 0

        print(f"{model_name:<20} {metrics['accuracy']*100:>9.2f}% {metrics['f1']*100:>9.2f}% "
              f"{stats['total_tokens']:>12} {avg_time:>11.2f}s")

    print("\n" + "=" * 80)
    print(" å®éªŒå®Œæˆ!")
    print("=" * 80)
    print(f"\nç»“æœå·²ä¿å­˜è‡³: {args.output}/")


if __name__ == "__main__":
    main()
