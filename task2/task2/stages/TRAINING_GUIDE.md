# Task2 åˆ†é˜¶æ®µè®­ç»ƒæŒ‡å—

## ğŸ“‹ æ€»è§ˆ

æœ¬é¡¹ç›®åˆ†ä¸º **6ä¸ªé˜¶æ®µ**ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æœ‰ç‹¬ç«‹çš„è®­ç»ƒè„šæœ¬å’Œæ–‡æ¡£ã€‚

### é˜¶æ®µæ¶æ„

```
baseline_simple (åŸºç¡€å®ç°)
    â†“
Stage1: Foundation (åŸºç¡€è®¾æ–½æµ‹è¯•)
    â†“
Stage2: Traditional Models (ä¸‰æ¨¡å‹baseline)
    â†“
Stage3: NaiveBayes Optimization (+5.74%)
    â†“
Stage4: BERT Optimization (+2-3%)
    â†“
Stage5: LLM Framework (Few-shotå­¦ä¹ ï¼Œ4ä¸ªå¤§æ¨¡å‹)
```

### æ€§èƒ½æ¼”è¿›

| é˜¶æ®µ | æœ´ç´ è´å¶æ–¯ | Word2Vec+SVM | BERT | ç‰¹ç‚¹ |
|------|------------|--------------|------|------|
| baseline_simple | 73% | 82% | 87% | æœ€ç®€å®ç° |
| Stage2 | 73% | 82% | 87% | å®Œæ•´baseline |
| Stage3 | **79%** | 82% | 87% | NBä¼˜åŒ– |
| Stage4 | 79% | 82% | **90%** | BERTä¼˜åŒ– |
| Stage5 | - | - | - | LLM (85-92%) |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®è¦æ±‚

```bash
# 1. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
cd /home/u2023312337/task2/task2
source .venv/bin/activate

# 2. ç¡®è®¤æ•°æ®æ–‡ä»¶å­˜åœ¨
ls -lh data/positive.txt data/negative.txt data/testSet-1000.xlsx

# 3. GPUæ£€æŸ¥ï¼ˆå¯é€‰ï¼ŒBERTè®­ç»ƒå¼ºçƒˆæ¨èï¼‰
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
```

### ä¸€é”®è¿è¡Œæ‰€æœ‰é˜¶æ®µ

```bash
# è¿è¡Œæ‰€æœ‰é˜¶æ®µï¼ˆéœ€è¦å¾ˆé•¿æ—¶é—´ï¼ï¼‰
cd stages

# Baseline
cd baseline_simple && python train.py && cd ..

# Stage 1-5
cd Stage1_Foundation && python test_infrastructure.py && cd ..
cd Stage2_Traditional_Models && python train.py && cd ..
cd Stage3_NaiveBayes_Optimization && python train.py && cd ..
cd Stage4_BERT_Optimization && python train.py --model bert --quick && cd ..
cd Stage5_LLM_Framework && python train.py --all && cd ..
```

## ğŸ“‚ å„é˜¶æ®µè¯¦ç»†æŒ‡å—

### Baseline Simple

**ç›®çš„**: æœ€ç®€å•çš„ä¸‰æ¨¡å‹å®ç°ï¼Œå¿«é€ŸéªŒè¯æƒ³æ³•

```bash
cd /home/u2023312337/task2/task2/stages/baseline_simple

# è®­ç»ƒæ‰€æœ‰æ¨¡å‹
python train.py

# ä»…è®­ç»ƒæŸä¸ªæ¨¡å‹
python train.py --model nb        # æœ´ç´ è´å¶æ–¯
python train.py --model w2v       # Word2Vec+SVM
python train.py --model bert      # BERT

# å¿«é€Ÿæµ‹è¯•ï¼ˆ5000æ ·æœ¬ï¼ŒBERT 1 epochï¼‰
python train.py --quick
```

**é¢„æœŸæ—¶é—´**: 1.5å°æ—¶ï¼ˆGPUï¼‰ / 7å°æ—¶ï¼ˆCPUï¼‰  
**è¾“å‡º**: `output/` å’Œ `models/`

---

### Stage1: Foundation

**ç›®çš„**: æµ‹è¯•åŸºç¡€è®¾æ–½ï¼ˆæ•°æ®åŠ è½½ã€è¯„ä¼°ã€å¯è§†åŒ–ï¼‰

```bash
cd /home/u2023312337/task2/task2/stages/Stage1_Foundation

# å®Œæ•´æµ‹è¯•
python test_infrastructure.py

# æµ‹è¯•ç‰¹å®šæ¨¡å—
python test_infrastructure.py --test data   # æ•°æ®åŠ è½½
python test_infrastructure.py --test viz    # å¯è§†åŒ–
python test_infrastructure.py --test env    # ç¯å¢ƒæ£€æŸ¥
```

**é¢„æœŸæ—¶é—´**: 2åˆ†é’Ÿ  
**è¾“å‡º**: `output/test_*.png`

---

### Stage2: Traditional Models

**ç›®çš„**: å»ºç«‹ä¸‰ç§æ–¹æ³•çš„å®Œæ•´baseline

```bash
cd /home/u2023312337/task2/task2/stages/Stage2_Traditional_Models

# è®­ç»ƒæ‰€æœ‰æ¨¡å‹
python train.py

# ä»…æŸä¸ªæ¨¡å‹
python train.py --model nb
python train.py --model w2v
python train.py --model bert

# å¿«é€Ÿæµ‹è¯•
python train.py --quick
```

**é¢„æœŸæ—¶é—´**: 1.5å°æ—¶ï¼ˆGPUï¼‰  
**é¢„æœŸæ€§èƒ½**: NB 73%, W2V 82%, BERT 87%

---

### Stage3: NaiveBayes Optimization

**ç›®çš„**: æ·±åº¦ä¼˜åŒ–æœ´ç´ è´å¶æ–¯ï¼Œä»73%æå‡è‡³79%

```bash
cd /home/u2023312337/task2/task2/stages/Stage3_NaiveBayes_Optimization

# è®­ç»ƒV2å¹¶ä¸V1å¯¹æ¯”
python train.py

# ä»…è®­ç»ƒV2ï¼ˆä¸å¯¹æ¯”ï¼‰
python train.py --no-compare

# V1 vs V2è¯¦ç»†å¯¹æ¯”
python test_optimized_nb.py
```

**é¢„æœŸæ—¶é—´**: 5åˆ†é’Ÿ  
**é¢„æœŸæ€§èƒ½**: 79.20% (+5.74%)

**æ ¸å¿ƒä¼˜åŒ–**:
- å¤šå±‚TF-IDF (è¯çº§+å­—ç¬¦çº§)
- 22ç»´ç»Ÿè®¡ç‰¹å¾
- ComplementNBç®—æ³•

---

### Stage4: BERT Optimization

**ç›®çš„**: BERTé«˜çº§ä¼˜åŒ–ï¼Œè¿½æ±‚90%+å‡†ç¡®ç‡

```bash
cd /home/u2023312337/task2/task2/stages/Stage4_BERT_Optimization

# ä½¿ç”¨ç»Ÿä¸€æ¥å£
python train.py --model bert         # BERT baseline
python train.py --model scibert      # SciBERT + Focal Loss
python train.py --model deberta      # DeBERTa (æœ€ä½³)

# å¿«é€Ÿæµ‹è¯•
python train.py --model bert --quick

# æ‰¹é‡å®éªŒï¼ˆ5ç»„ï¼Œ8-12å°æ—¶ï¼‰
python run_bert_experiments.py

# å•æ¨¡å‹å®Œæ•´è®­ç»ƒ
python train_bert_optimized_v2.py --model microsoft/deberta-v3-base
```

**é¢„æœŸæ—¶é—´**: 2-4å°æ—¶ï¼ˆå•æ¨¡å‹ï¼‰ / 8-12å°æ—¶ï¼ˆå…¨éƒ¨å®éªŒï¼‰  
**é¢„æœŸæ€§èƒ½**: BERT 87% â†’ DeBERTa 90%

**æ ¸å¿ƒä¼˜åŒ–**:
- é¢„è®­ç»ƒæ¨¡å‹é€‰æ‹© (SciBERT, DeBERTa)
- Focal Loss
- å¯¹æŠ—è®­ç»ƒ (FGM)
- æ—©åœæœºåˆ¶

---

### Stage5: LLM Framework

**ç›®çš„**: LLM In-Context Learningå®éªŒï¼ˆFew-shotå­¦ä¹ ï¼‰

```bash
cd /home/u2023312337/task2/task2/stages/Stage5_LLM_Framework

# ğŸ†• ä½¿ç”¨ç»Ÿä¸€train.pyè„šæœ¬
# äº¤äº’å¼é€‰æ‹©æ¨¡å‹
python train.py

# è¿è¡Œæ‰€æœ‰4ä¸ªLLM
python train.py --all

# è¿è¡Œå•ä¸ªæ¨¡å‹
python train.py --model glm-4.6      # æ™ºè°±AI GLM-4.6
python train.py --model deepseek     # DeepSeek (æ€§ä»·æ¯”ä¹‹ç‹)
python train.py --model qwen3        # é˜¿é‡Œäº‘é€šä¹‰åƒé—®
python train.py --model kimi         # Moonshot Kimi

# å¿«é€Ÿæµ‹è¯•ï¼ˆ100æ ·æœ¬ï¼‰
python train.py --model deepseek --sample 100

# ä½¿ç”¨åŸå§‹è„šæœ¬ï¼ˆé«˜çº§é€‰é¡¹ï¼‰
python run_llm_experiment.py --model deepseek
python run_llm_experiment.py --all

# æˆæœ¬ä¼°ç®—
python calculate_llm_cost.py --model deepseek --samples 1000
```

**æ”¯æŒçš„æ¨¡å‹**: GLM-4.6, Qwen3-Turbo, Kimi-K2-Turbo, DeepSeek-Chat
**é¢„æœŸæ—¶é—´**: 10-30åˆ†é’Ÿï¼ˆ100æ ·æœ¬ï¼‰ / 2-8å°æ—¶ï¼ˆå…¨éƒ¨976æ ·æœ¬ï¼‰
**é¢„æœŸæ€§èƒ½**: 85-92%ï¼ˆå–å†³äºæ¨¡å‹ï¼‰

**æ ¸å¿ƒç‰¹ç‚¹**:
- Few-shotå­¦ä¹ ï¼ˆ8ä¸ªç¤ºä¾‹ï¼‰
- é›¶è®­ç»ƒï¼ˆç›´æ¥æ¨ç†ï¼‰
- æˆæœ¬è¿½è¸ª
- APIè°ƒç”¨ç®¡ç†

---

## ğŸ’¡ å®ç”¨æŠ€å·§

### 1. å¿«é€Ÿæµ‹è¯•æµç¨‹

å¦‚æœæ—¶é—´æœ‰é™ï¼Œæ¨èè¿™ä¸ªå¿«é€Ÿæµç¨‹ï¼š

```bash
# 20åˆ†é’Ÿå®Œæˆæ‰€æœ‰é˜¶æ®µçš„å¿«é€ŸéªŒè¯
cd /home/u2023312337/task2/task2/stages

# Baseline (5000æ ·æœ¬ï¼ŒBERT 1 epoch) - 15åˆ†é’Ÿ
cd baseline_simple && python train.py --quick && cd ..

# Stage1 (åŸºç¡€è®¾æ–½æµ‹è¯•) - 2åˆ†é’Ÿ
cd Stage1_Foundation && python test_infrastructure.py && cd ..

# Stage3 (NBä¼˜åŒ–ï¼Œ10Kæ ·æœ¬) - 2åˆ†é’Ÿ
cd Stage3_NaiveBayes_Optimization && python train.py --quick && cd ..

# Stage5 (LLMå¿«é€Ÿæµ‹è¯•ï¼Œ100æ ·æœ¬) - 5åˆ†é’Ÿ
cd Stage5_LLM_Framework && python train.py --model deepseek --sample 100 && cd ..
```

### 2. æŸ¥çœ‹è¾“å‡ºæ–‡ä»¶

æ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºéƒ½åœ¨å„è‡ªçš„ `output/` ç›®å½•ï¼š

```bash
# æŸ¥çœ‹æŸä¸ªé˜¶æ®µçš„è¾“å‡º
ls -lh /home/u2023312337/task2/task2/stages/Stage3_NaiveBayes_Optimization/output/

# æŸ¥çœ‹æ‰€æœ‰é˜¶æ®µçš„æ¨¡å‹æ–‡ä»¶å¤§å°
du -sh */models/ | sort -h
```

### 3. å¯¹æ¯”ä¸åŒé˜¶æ®µçš„æ€§èƒ½

```bash
# æŸ¥çœ‹å„é˜¶æ®µçš„è¯„ä¼°ç»“æœ
grep -h "å‡†ç¡®ç‡\|Accuracy" */output/*.txt
```

## âš ï¸ å¸¸è§é—®é¢˜

### Q1: è™šæ‹Ÿç¯å¢ƒæ¿€æ´»å¤±è´¥

```bash
# ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•
cd /home/u2023312337/task2/task2
source .venv/bin/activate

# å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œæ£€æŸ¥.venvæ˜¯å¦å­˜åœ¨
ls -la .venv/
```

### Q2: CUDA out of memory

```bash
# é™ä½æ‰¹æ¬¡å¤§å°
python train.py --model bert --batch-size 8  # é»˜è®¤16
```

### Q3: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶

```bash
# ç¡®è®¤æ•°æ®æ–‡ä»¶ä½ç½®
ls -lh /home/u2023312337/task2/task2/data/

# åº”è¯¥æœ‰è¿™ä¸‰ä¸ªæ–‡ä»¶:
# - positive.txt (118K samples)
# - negative.txt (114K samples)
# - testSet-1000.xlsx (1000 samples)
```

### Q4: è®­ç»ƒæ—¶é—´å¤ªé•¿

```bash
# ä½¿ç”¨å¿«é€Ÿæµ‹è¯•æ¨¡å¼
python train.py --quick

# æˆ–é™åˆ¶æ ·æœ¬æ•°
python train.py --max-samples 10000

# æˆ–ä»…è®­ç»ƒå•ä¸ªæ¨¡å‹
python train.py --model nb  # æœ€å¿«ï¼Œ2åˆ†é’Ÿ
```

## ğŸ“Š æ€§èƒ½é¢„æœŸæ€»ç»“

| é˜¶æ®µ | å…³é”®æ¨¡å‹ | å‡†ç¡®ç‡ | è®­ç»ƒæ—¶é—´ | éš¾åº¦ |
|------|----------|--------|----------|------|
| baseline_simple | BERT | 87% | 1å°æ—¶ | â­ |
| Stage1 | - | - | 2åˆ†é’Ÿ | â­ |
| Stage2 | ä¸‰æ¨¡å‹ | 73-87% | 1.5å°æ—¶ | â­â­ |
| Stage3 | NB V2 | 79% | 5åˆ†é’Ÿ | â­â­ |
| Stage4 | DeBERTa | 90% | 4å°æ—¶ | â­â­â­â­ |
| Stage5 | LLM | 85-90% | 5åˆ†é’Ÿ | â­â­â­ |

## ğŸ“š ç›¸å…³æ–‡æ¡£

- å„é˜¶æ®µçš„ `IMPLEMENTATION.md` - è¯¦ç»†å®ç°è¯´æ˜
- å„é˜¶æ®µçš„ `README.md` - å¿«é€Ÿæ¦‚è¿°
- `../CLAUDE.md` - é¡¹ç›®æ€»ä½“æ–‡æ¡£
- `../VERSION_EVOLUTION.md` - å®Œæ•´æ¼”è¿›å†å²

---

**æœ€åæ›´æ–°**: 2024-12-05  
**æ–‡æ¡£ç»´æŠ¤**: ä¸ä»£ç åŒæ­¥æ›´æ–°
