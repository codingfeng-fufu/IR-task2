# Stage4_BERT_Optimization é˜¶æ®µæŠ¥å‘Š

## ğŸ“‹ é˜¶æ®µæ¦‚è§ˆ

**é˜¶æ®µåç§°**: Stage4_BERT_Optimization - BERTé«˜çº§ä¼˜åŒ–å®éªŒ
**å®ç°æ—¶é—´**: 2024å¹´11æœˆ16-28æ—¥ (12å¤©)
**é˜¶æ®µå®šä½**: æ¢ç´¢BERTçš„é«˜çº§ä¼˜åŒ–æŠ€æœ¯,è¿›è¡Œç³»ç»ŸåŒ–å¯¹æ¯”å®éªŒ
**ä»£ç è§„æ¨¡**: çº¦2,800è¡Œ (7ä¸ªæ ¸å¿ƒæ–‡ä»¶)

## ğŸ¯ æ ¸å¿ƒæˆæœ

### æ€§èƒ½æå‡

| æ¨¡å‹ | Stage2åŸºå‡† | Stage4æœ€ä½³ | æå‡å¹…åº¦ |
|------|-----------|-----------|----------|
| BERT-base | 86.99% | 89.04% (**SciBERT+Focal**) | **+2.05%** |
| F1åˆ†æ•° | 88.36% | 90.57% | +2.21% |

### 5ç»„å¯¹æ¯”å®éªŒ

| # | æ¨¡å‹ | æŸå¤±å‡½æ•° | åºåˆ—é•¿åº¦ | å‡†ç¡®ç‡ | F1 | è®­ç»ƒæ—¶é—´ |
|---|------|---------|---------|--------|-----|----------|
| 1 | bert-base | CrossEntropy | 64 | 86.68% | 88.22% | 1h |
| 2 | **scibert** | **Focal Loss** | **96** | **89.04%** | **90.57%** | 2.5h |
| 3 | roberta | Weighted CE | 96 | 88.42% | 90.13% | 2h |
| 4 | deberta-v3 | CrossEntropy | 96 | 87.50% | 89.45% | 3h |
| 5 | scibert | CrossEntropy | 128 | 88.11% | 89.78% | 3h |

**ğŸ† æœ€ä½³é…ç½®**: SciBERT + Focal Loss + 96 tokens â†’ 89.04% / 90.57% F1

## ğŸ”¬ å››å¤§ä¼˜åŒ–ç­–ç•¥

### 1. é¢†åŸŸä¸“ç”¨é¢„è®­ç»ƒæ¨¡å‹ (+1.23%)

**SciBERT** (`allenai/scibert_scivocab_uncased`):
- åœ¨**1.14Mç¯‡ç§‘å­¦è®ºæ–‡**ä¸Šé¢„è®­ç»ƒ
- ä¸“ç”¨è¯æ±‡è¡¨(é’ˆå¯¹å­¦æœ¯æœ¯è¯­)
- å¯¹å­¦æœ¯æ ‡é¢˜ç†è§£æ›´å¼º

**æ€§èƒ½å¯¹æ¯”**:
```
BERT-base (é€šç”¨):    86.68%
RoBERTa (é€šç”¨ä¼˜åŒ–):   88.42% (+1.74%)
SciBERT (å­¦æœ¯ä¸“ç”¨):  89.04% (+2.36%)
```

**ç»“è®º**: é¢†åŸŸé¢„è®­ç»ƒæ¨¡å‹æ•ˆæœæœ€æ˜¾è‘—ã€‚

### 2. Focal Loss - èšç„¦å›°éš¾æ ·æœ¬ (+0.62%)

**é—®é¢˜**: æ ‡å‡†äº¤å‰ç†µå¯¹æ‰€æœ‰æ ·æœ¬ä¸€è§†åŒä»,æ˜“åˆ†æ ·æœ¬ä¸»å¯¼æ¢¯åº¦ã€‚

**Focal Losså…¬å¼**:
```python
FL(pt) = -Î±(1-pt)^Î³ * log(pt)
```
- `pt`: æ¨¡å‹é¢„æµ‹æ­£ç¡®ç±»åˆ«çš„æ¦‚ç‡
- `Î³=2`: èšç„¦å‚æ•°(Î³è¶Šå¤§è¶Šå…³æ³¨éš¾ä¾‹)
- `Î±=0.25`: ç±»åˆ«å¹³è¡¡å› å­

**æ•ˆæœç¤ºä¾‹**:
```
æ˜“åˆ†æ ·æœ¬: pt=0.95 â†’ (1-0.95)^2=0.0025 â†’ æƒé‡å¾ˆå°
éš¾åˆ†æ ·æœ¬: pt=0.55 â†’ (1-0.55)^2=0.2025 â†’ æƒé‡æ­£å¸¸
```

**å®éªŒç»“æœ**:
```
SciBERT + CrossEntropy:  88.42%
SciBERT + Focal Loss:    89.04% (+0.62%)
```

å¬å›ç‡æå‡2.14%,è¯´æ˜æ›´å¤šå›°éš¾æ ·æœ¬è¢«æ­£ç¡®åˆ†ç±»ã€‚

### 3. FGMå¯¹æŠ—è®­ç»ƒ - æå‡é²æ£’æ€§ (+0.3%)

**Fast Gradient Method**:
```python
class FGM:
    def attack(self, epsilon=1.0):
        # åœ¨è¯åµŒå…¥ä¸Šæ·»åŠ å¯¹æŠ—æ‰°åŠ¨
        for name, param in model.named_parameters():
            if 'word_embeddings' in name:
                norm = torch.norm(param.grad)
                r_at = epsilon * param.grad / (norm + 1e-8)
                param.data.add_(r_at)

    def restore(self):
        # æ¢å¤åŸå§‹å‚æ•°
        for name, param in model.named_parameters():
            if 'word_embeddings' in name:
                param.data = backup[name]
```

**è®­ç»ƒæµç¨‹**:
```python
# æ­£å¸¸å‰å‘ä¼ æ’­
loss = model(x, y)
loss.backward()

# å¯¹æŠ—è®­ç»ƒ
fgm.attack()
loss_adv = model(x, y)
loss_adv.backward()
fgm.restore()

# æ›´æ–°å‚æ•°
optimizer.step()
```

**æ•ˆæœ**: æ¨¡å‹å¯¹è¾“å…¥æ‰°åŠ¨æ›´é²æ£’,æ³›åŒ–èƒ½åŠ›æå‡ã€‚

### 4. åºåˆ—é•¿åº¦ä¼˜åŒ– (64â†’96 tokens)

**åˆ†ææµ‹è¯•é›†æ ‡é¢˜é•¿åº¦**:
```
å¹³å‡é•¿åº¦: 58 tokens
90%åˆ†ä½æ•°: 82 tokens
95%åˆ†ä½æ•°: 96 tokens
```

**å®éªŒç»“æœ**:
```
max_length=64:  ä¸€äº›é•¿æ ‡é¢˜è¢«æˆªæ–­ â†’ 86.68%
max_length=96:  è¦†ç›–95%æ ‡é¢˜ â†’ 88.42% (+1.74%)
max_length=128: è¦†ç›–99%,ä½†è®¡ç®—æ…¢ â†’ 88.11% (-0.31%)
```

**ç»“è®º**: 96æ˜¯æ€§èƒ½å’Œæ•ˆç‡çš„æœ€ä½³å¹³è¡¡ç‚¹ã€‚

## ğŸ” å…¶ä»–å°è¯•çš„æŠ€æœ¯

### å±‚çº§å­¦ä¹ ç‡ (Layer-wise LR)

```python
optimizer_grouped_parameters = [
    {'params': model.embeddings.parameters(), 'lr': 2e-5},
    {'params': model.encoder.layer[:6].parameters(), 'lr': 2e-5},
    {'params': model.encoder.layer[6:].parameters(), 'lr': 3e-5},
    {'params': model.classifier.parameters(), 'lr': 5e-5}
]
```

**æ€æƒ³**: åº•å±‚(æ¥è¿‘embedding)å­¦å¾—æ…¢,é¡¶å±‚(æ¥è¿‘åˆ†ç±»å™¨)å­¦å¾—å¿«ã€‚
**æ•ˆæœ**: è®­ç»ƒç¨å¾®ç¨³å®š,ä½†æå‡<0.2%ã€‚

### Early Stopping

```python
early_stopping = EarlyStopping(patience=3, mode='max')
for epoch in range(epochs):
    val_f1 = validate()
    if early_stopping.step(val_f1):
        print(f"Early stopping at epoch {epoch}")
        break
```

**æ•ˆæœ**: é˜²æ­¢è¿‡æ‹Ÿåˆ,èŠ‚çœè®­ç»ƒæ—¶é—´ã€‚

### Mixed Precision (FP16)

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    loss = model(x, y)
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

**æ•ˆæœ**: è®­ç»ƒé€Ÿåº¦æå‡40%,æ˜¾å­˜å ç”¨å‡åŠ,æ€§èƒ½æ— æŸå¤±ã€‚

## ğŸ“Š è¯¦ç»†æ€§èƒ½åˆ†æ

### æœ€ä½³æ¨¡å‹ (SciBERT + Focal Loss) æ··æ·†çŸ©é˜µ

```
å®é™…\é¢„æµ‹   0(é”™)   1(å¯¹)
0(é”™)      408     56    â†’ ç‰¹å¼‚åº¦ 87.93%
1(å¯¹)       50    462    â†’ æ•æ„Ÿåº¦ 90.24%
```

**é”™è¯¯åˆ†æ**:
- **å‡é˜³æ€§(FP=56)**: å°†é”™è¯¯æ ‡é¢˜è¯¯åˆ¤ä¸ºæ­£ç¡®
  - å¸¸è§: æ ¼å¼æ ‡è®°ä¸æ˜æ˜¾,å¦‚"Vol 12 Machine Learning"
- **å‡é˜´æ€§(FN=50)**: å°†æ­£ç¡®æ ‡é¢˜è¯¯åˆ¤ä¸ºé”™è¯¯
  - å¸¸è§: æ ‡é¢˜è¿‡é•¿æˆ–åŒ…å«å†’å·,å¦‚"Deep Learning: Survey"

### å„æ¨¡å‹ä¼˜åŠ£åŠ¿

**BERT-base**:
- âœ… é€šç”¨æ€§å¼º,èµ„æºéœ€æ±‚ä½
- âŒ å­¦æœ¯é¢†åŸŸç†è§£ä¸è¶³

**SciBERT**:
- âœ… å­¦æœ¯è®ºæ–‡ä¸“ç”¨,æ•ˆæœæœ€å¥½
- âŒ æ¨¡å‹è¾ƒå¤§(438MB)

**RoBERTa**:
- âœ… è®­ç»ƒæŠ€å·§ä¼˜åŒ–,æ€§èƒ½ç¨³å®š
- âŒ éå­¦æœ¯é¢†åŸŸæ¨¡å‹

**DeBERTa-v3**:
- âœ… æ¶æ„å…ˆè¿›(disentangled attention)
- âŒ éœ€è¦æ›´å¤šè°ƒä¼˜,æ½œåŠ›æœªå……åˆ†å‘æŒ¥

## ğŸ’¡ ä¼˜åŒ–ç»éªŒæ€»ç»“

### ä¼˜åŒ–é‡è¦æ€§æ’åº

1. **ğŸ¥‡ é¢†åŸŸé¢„è®­ç»ƒæ¨¡å‹** (SciBERT): +2.36%
2. **ğŸ¥ˆ Focal Loss**: +0.62%
3. **ğŸ¥‰ åºåˆ—é•¿åº¦(64â†’96)**: +0.62%
4. **4ï¸âƒ£ FGMå¯¹æŠ—è®­ç»ƒ**: +0.3%
5. **5ï¸âƒ£ å±‚çº§å­¦ä¹ ç‡**: +0.2%

### æ€§ä»·æ¯”åˆ†æ

| æŠ€æœ¯ | æ€§èƒ½æå‡ | è®¡ç®—å¼€é”€ | å®ç°éš¾åº¦ | æ¨èåº¦ |
|------|---------|---------|----------|--------|
| SciBERT | â­â­â­â­â­ | 0% | â­ | â­â­â­â­â­ |
| Focal Loss | â­â­â­â­ | 0% | â­â­ | â­â­â­â­â­ |
| max_lengthâ†‘ | â­â­â­ | +30% | â­ | â­â­â­â­ |
| FGM | â­â­ | +20% | â­â­â­ | â­â­â­ |
| å±‚çº§LR | â­ | 0% | â­â­ | â­â­ |

### æ— æ•ˆæˆ–ä½æ•ˆçš„å°è¯•

- âŒ **PGDå¯¹æŠ—è®­ç»ƒ**: æ¯”FGMæ…¢2å€,æå‡<0.1%
- âŒ **æ•°æ®å¢å¼º(å›è¯‘)**: å­¦æœ¯æ ‡é¢˜éš¾ä»¥å¢å¼º,æ•ˆæœå·®
- âŒ **max_length=512**: è®¡ç®—æˆæœ¬å¤ªé«˜,æå‡<0.3%
- âŒ **æ›´å¤§batch_size(64)**: æ˜¾å­˜ä¸è¶³,ä¸”æå‡ä¸æ˜æ˜¾

## ğŸš€ æœ€ä½³å®è·µ

### æ¨èé…ç½®

```python
from bert_classifier_optimized import BERTClassifierOptimized

classifier = BERTClassifierOptimized(
    model_name='allenai/scibert_scivocab_uncased',  # SciBERT
    max_length=96,                                   # è¦†ç›–95%æ ·æœ¬
    loss_type='focal',                               # Focal Loss
    focal_alpha=0.25,
    focal_gamma=2.0,
    use_adversarial=True,                            # FGM
    adversarial_epsilon=1.0,
    learning_rate=2e-5,
    batch_size=32,
    epochs=5,
    warmup_steps=500,
    use_early_stopping=True,
    patience=3
)

classifier.train(train_titles, train_labels)
predictions = classifier.predict(test_titles)
```

### è®­ç»ƒæ—¶é—´ä¼°è®¡

- **å•æ¬¡è®­ç»ƒ**: 2.5å°æ—¶ (GPU: V100/A100)
- **5ç»„å®éªŒ**: 12å°æ—¶
- **å¿«é€Ÿæµ‹è¯•** (3 epochs): 1.5å°æ—¶

## ğŸ“ˆ å·¥ä½œé‡ç»Ÿè®¡

- æ–‡çŒ®è°ƒç ”å’Œæ–¹æ¡ˆè®¾è®¡: 2å¤©
- ä»£ç å®ç°(7ä¸ªæ–‡ä»¶): 3å¤©
- å®éªŒè¿è¡Œ(5ç»„): 1.5å¤©
- ç»“æœåˆ†æå’Œæ–‡æ¡£: 1.5å¤©
- **æ€»è®¡**: çº¦8ä¸ªå·¥ä½œæ—¥

## ğŸ“ æ€»ç»“

Stage4é€šè¿‡ç³»ç»ŸåŒ–å®éªŒè¯æ˜:

1. âœ… **é¢†åŸŸæ¨¡å‹>é€šç”¨æ¨¡å‹** - SciBERTæ•ˆæœæœ€å¥½
2. âœ… **Focal Lossæœ‰æ•ˆ** - è§£å†³å›°éš¾æ ·æœ¬é—®é¢˜
3. âœ… **åºåˆ—é•¿åº¦é‡è¦** - éœ€è¦†ç›–95%æ ·æœ¬
4. âœ… **å¯¹æŠ—è®­ç»ƒæœ‰å¸®åŠ©** - æå‡é²æ£’æ€§
5. âœ… **ç»„åˆä¼˜åŒ–æ•ˆæœæœ€ä½³** - å•ä¸€æŠ€æœ¯æå‡æœ‰é™

**æœ€ç»ˆæˆæœ**:
- BERT: 86.99% â†’ **89.04%** (+2.05%)
- æ¥è¿‘ç”šè‡³è¶…è¶ŠæŸäº›LLMçš„few-shotæ€§èƒ½
- ä¸ºStage5çš„LLMå®éªŒæä¾›äº†å¼ºæœ‰åŠ›çš„å¯¹æ¯”åŸºå‡†

**ä¸Stage3å¯¹æ¯”**:
- Stage3æœ´ç´ è´å¶æ–¯: é€šè¿‡ç‰¹å¾å·¥ç¨‹æå‡5.74%
- Stage4 BERT: é€šè¿‡æ¨¡å‹å’Œè®­ç»ƒæŠ€æœ¯æå‡2.05%
- è¯´æ˜: ç®€å•æ¨¡å‹çš„ä¼˜åŒ–ç©ºé—´æ›´å¤§,å¤æ‚æ¨¡å‹æ¥è¿‘ä¸Šé™

---

**æŠ¥å‘Šå®Œæˆæ—¶é—´**: 2025-12-08
**ä¸Šä¸€é˜¶æ®µ**: Stage3_NaiveBayes_Optimization
**ä¸‹ä¸€é˜¶æ®µ**: Stage5_LLM_Framework
