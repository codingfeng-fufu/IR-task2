# Stage4_BERT_Optimization å®ç°æ–‡æ¡£

## ğŸ“‹ é˜¶æ®µæ¦‚è¿°

**é˜¶æ®µåç§°**: Stage4 - BERTæ·±åº¦ä¼˜åŒ–
**å®ç°æ—¶é—´**: 2024å¹´11æœˆ16-28æ—¥
**ä¸»è¦ç›®æ ‡**: BERTé«˜çº§ä¼˜åŒ–ï¼Œè¿½æ±‚89-91%å‡†ç¡®ç‡
**ä»£ç è¡Œæ•°**: ~2,800è¡Œï¼ˆ7ä¸ªæ ¸å¿ƒæ–‡ä»¶ï¼‰
**æ€§èƒ½æå‡**: 87.91% â†’ 89-91% (+2-3ä¸ªç™¾åˆ†ç‚¹)

## ğŸ¯ æ ¸å¿ƒä¼˜åŒ–ç­–ç•¥

### äº”å¤§ä¼˜åŒ–ç»´åº¦

| ä¼˜åŒ–æ–¹å‘ | å…·ä½“æŠ€æœ¯ | é¢„æœŸæå‡ | å®ç°éš¾åº¦ |
|----------|----------|----------|----------|
| **é¢„è®­ç»ƒæ¨¡å‹é€‰æ‹©** | SciBERT, RoBERTa, DeBERTa | +1-2% | â­â­ |
| **æŸå¤±å‡½æ•°** | Focal Loss, Weighted CE | +0.5-1% | â­â­â­ |
| **å¯¹æŠ—è®­ç»ƒ** | FGM, PGD | +0.3-0.5% | â­â­â­â­ |
| **è®­ç»ƒç­–ç•¥** | æ—©åœ, å­¦ä¹ ç‡è°ƒåº¦, EMA | +0.2-0.5% | â­â­â­ |
| **åºåˆ—é•¿åº¦** | 64â†’96/128 | +0.1-0.3% | â­ |

### å®éªŒè®¾ç½®ï¼š5ç»„å¯¹æ¯”å®éªŒ

| å®éªŒåç§° | æ¨¡å‹ | æŸå¤±å‡½æ•° | åºåˆ—é•¿åº¦ | å¯¹æŠ—è®­ç»ƒ | é¢„æœŸå‡†ç¡®ç‡ | è®­ç»ƒæ—¶é—´ |
|----------|------|----------|----------|----------|------------|----------|
| **å®éªŒ1: BERT Baseline** | bert-base-uncased | CE | 64 | âŒ | 87-88% | 2å°æ—¶ |
| **å®éªŒ2: SciBERT + Focal** | SciBERT | Focal Loss | 96 | âœ… | 87-88% | 2.5å°æ—¶ |
| **å®éªŒ3: RoBERTa + WeightedCE** | RoBERTa | Weighted CE | 96 | âœ… | 88-89% | 2.5å°æ—¶ |
| **å®éªŒ4: DeBERTa + Advanced** | DeBERTa-v3 | Focal Loss | 96 | âœ… | **89-91%** â­ | 4å°æ—¶ |
| **å®éªŒ5: SciBERT + Max128** | SciBERT | Focal Loss | 128 | âœ… | 88-89% | 3å°æ—¶ |



## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®å‡†å¤‡

```bash
cd /home/u2023312337/task2/task2/stages/Stage4_BERT_Optimization

# 1. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source ../../.venv/bin/activate

# 2. é¢„ä¸‹è½½æ¨¡å‹ï¼ˆå¯é€‰ï¼Œé¿å…è®­ç»ƒæ—¶ä¸‹è½½ï¼‰
python predownload_models.py

# 3. æ£€æŸ¥GPU
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
```

### è¿è¡Œæ–¹å¼

**æ–¹å¼1: ä½¿ç”¨ç»Ÿä¸€æ¥å£ train.py ï¼ˆæ¨èæ–°æ‰‹ï¼‰**

```bash
# BERT baseline
python train.py --model bert

# SciBERT + Focal Loss
python train.py --model scibert

# DeBERTa (æœ€ä½³æ€§èƒ½)
python train.py --model deberta

# å¿«é€Ÿæµ‹è¯•ï¼ˆ1 epochï¼‰
python train.py --model bert --quick
```

**æ–¹å¼2: ä½¿ç”¨å®Œæ•´è®­ç»ƒè„šæœ¬ ï¼ˆé«˜çº§ç”¨æˆ·ï¼‰**

```bash
# å•ä¸ªä¼˜åŒ–æ¨¡å‹è®­ç»ƒ
python train_bert_optimized_v2.py \
    --model microsoft/deberta-v3-base \
    --max-length 96 \
    --epochs 10 \
    --loss-type focal \
    --use-adversarial \
    --use-early-stopping

# è‡ªå®šä¹‰æ‰€æœ‰å‚æ•°
python train_bert_optimized_v2.py \
    --model allenai/scibert_scivocab_uncased \
    --max-length 128 \
    --epochs 8 \
    --batch-size 24 \
    --learning-rate 3e-5 \
    --warmup-ratio 0.1 \
    --loss-type focal \
    --focal-alpha 0.25 \
    --focal-gamma 2.0 \
    --use-adversarial \
    --adv-epsilon 1.0 \
    --use-mixed-precision
```

**æ–¹å¼3: æ‰¹é‡å®éªŒ ï¼ˆç ”ç©¶å¯¹æ¯”ï¼‰**

```bash
# è¿è¡Œå…¨éƒ¨5ç»„å®éªŒï¼ˆ8-12å°æ—¶ï¼‰
python run_bert_experiments.py

# æŸ¥çœ‹å¯¹æ¯”ç»“æœ
cat models/experiments/comparison_report.txt
cat models/experiments/results.json
```

**æ–¹å¼4: å¿«é€Ÿæµ‹è¯•è„šæœ¬**

```bash
# ä½¿ç”¨shellè„šæœ¬å¿«é€Ÿæµ‹è¯•
./run_quick.sh

# é€‰æ‹©é€‰é¡¹:
# 1. Quick test (3 epochs)
# 2. SciBERT test
# 3. Full training
```



## ğŸ“ æ–‡ä»¶ç»“æ„

```
Stage4_BERT_Optimization/
â”œâ”€â”€ bert_classifier_optimized.py        # ä¼˜åŒ–BERTç±» (~800è¡Œ)
â”‚   â”œâ”€â”€ OptimizedBERTClassifier         # ä¸»åˆ†ç±»å™¨ç±»
â”‚   â”œâ”€â”€ CustomClassificationHead        # è‡ªå®šä¹‰åˆ†ç±»å¤´
â”‚   â””â”€â”€ TitleDataset                    # æ•°æ®é›†ç±»
â”‚
â”œâ”€â”€ optimized_BERT.py                   # BERTä¼˜åŒ–æ¡†æ¶ (~700è¡Œ)
â”‚   â”œâ”€â”€ BERTClassifierOptimized         # ä¼˜åŒ–æ¡†æ¶
â”‚   â”œâ”€â”€ FGM (Fast Gradient Method)      # å¯¹æŠ—è®­ç»ƒ
â”‚   â”œâ”€â”€ EMA (Exponential Moving Avg)    # æŒ‡æ•°ç§»åŠ¨å¹³å‡
â”‚   â”œâ”€â”€ FocalLoss                       # FocalæŸå¤±å‡½æ•°
â”‚   â””â”€â”€ æ•°æ®å¢å¼ºåŠŸèƒ½
â”‚
â”œâ”€â”€ train_bert_optimized_v2.py          # å®Œæ•´è®­ç»ƒè„šæœ¬ (~850è¡Œ)
â”‚   â”œâ”€â”€ å‘½ä»¤è¡Œå‚æ•°è§£æ
â”‚   â”œâ”€â”€ è®­ç»ƒå¾ªç¯å®ç°
â”‚   â”œâ”€â”€ éªŒè¯é›†è¯„ä¼°
â”‚   â””â”€â”€ æ—©åœæœºåˆ¶
â”‚
â”œâ”€â”€ run_bert_experiments.py             # æ‰¹é‡å®éªŒè„šæœ¬ (~350è¡Œ)
â”‚   â”œâ”€â”€ 5ç»„å®éªŒé…ç½®
â”‚   â”œâ”€â”€ è‡ªåŠ¨è¿è¡Œå’Œå¯¹æ¯”
â”‚   â””â”€â”€ ç»“æœæ±‡æ€»æŠ¥å‘Š
â”‚
â”œâ”€â”€ predownload_models.py               # æ¨¡å‹é¢„ä¸‹è½½ (~150è¡Œ)
â”‚   â””â”€â”€ æ‰¹é‡ä¸‹è½½HuggingFaceæ¨¡å‹
â”‚
â”œâ”€â”€ train.py                            # ç»Ÿä¸€è®­ç»ƒæ¥å£ â­æ–°å¢â­
â”‚   â””â”€â”€ ç®€åŒ–çš„è®­ç»ƒå…¥å£
â”‚
â”œâ”€â”€ config.py                           # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ get_model_path()
â”‚   â”œâ”€â”€ get_output_path()
â”‚   â””â”€â”€ get_data_path()
â”‚
â”œâ”€â”€ run_quick.sh                        # å¿«é€Ÿæµ‹è¯•è„šæœ¬
â”‚   â””â”€â”€ äº¤äº’å¼é€‰é¡¹èœå•
â”‚
â”œâ”€â”€ models/                             # æ¨¡å‹ç›®å½•
â”‚   â”œâ”€â”€ experiments/                    # å®éªŒç»“æœ
â”‚   â”‚   â”œâ”€â”€ bert_baseline.pt
â”‚   â”‚   â”œâ”€â”€ scibert_focal.pt
â”‚   â”‚   â”œâ”€â”€ deberta_advanced.pt
â”‚   â”‚   â”œâ”€â”€ comparison_report.txt       # â­å¯¹æ¯”æŠ¥å‘Š
â”‚   â”‚   â””â”€â”€ results.json                # ç»“æœæ•°æ®
â”‚   â””â”€â”€ best_model.pt                   # æœ€ä½³æ¨¡å‹
â”‚
â””â”€â”€ output/                             # è¾“å‡ºç›®å½•
    â”œâ”€â”€ training_curves.png             # è®­ç»ƒæ›²çº¿
    â”œâ”€â”€ loss_comparison.png             # æŸå¤±å¯¹æ¯”
    â”œâ”€â”€ performance_heatmap.png         # æ€§èƒ½çƒ­åŠ›å›¾
    â””â”€â”€ evaluation_results.txt          # è¯„ä¼°ç»“æœ
```

## ğŸ”¬ æ ¸å¿ƒæŠ€æœ¯å®ç°

### 1. é¢„è®­ç»ƒæ¨¡å‹é€‰æ‹©

**å¯ç”¨æ¨¡å‹åˆ—è¡¨** (`bert_classifier_optimized.py:94-104`):

```python
MODEL_OPTIONS = {
    'bert-base': 'bert-base-uncased',              # æ ‡å‡†BERT
    'bert-large': 'bert-large-uncased',            # å¤§å‹BERT
    'scibert': 'allenai/scibert_scivocab_uncased', # å­¦æœ¯è®ºæ–‡ä¸“ç”¨â­
    'roberta-base': 'roberta-base',                # RoBERTa
    'roberta-large': 'roberta-large',              # å¤§å‹RoBERTa
    'albert-base': 'albert-base-v2',               # ALBERT
    'deberta-v3': 'microsoft/deberta-v3-base',     # DeBERTaâ­æœ€ä½³
    'deberta-v3-large': 'microsoft/deberta-v3-large'
}
```

**æ¨¡å‹ç‰¹ç‚¹å¯¹æ¯”**:

| æ¨¡å‹ | å‚æ•°é‡ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ | é¢„æœŸæ€§èƒ½ |
|------|--------|------|----------|----------|
| **BERT-base** | 110M | æ ‡å‡†åŸºçº¿ | é€šç”¨ | 87-88% |
| **SciBERT** | 110M | ç§‘å­¦æ–‡çŒ®é¢„è®­ç»ƒ | å­¦æœ¯æ ‡é¢˜ | 87-88% |
| **RoBERTa** | 125M | æ”¹è¿›è®­ç»ƒç­–ç•¥ | é€šç”¨ | 88-89% |
| **DeBERTa-v3** | 184M | Disentangled Attention | æœ€ä½³æ€§èƒ½ | **89-91%** â­ |
| **ALBERT** | 12M | å‚æ•°å…±äº« | èµ„æºå—é™ | 86-87% |

**ä¸ºä»€ä¹ˆSciBERTé€‚åˆå­¦æœ¯æ ‡é¢˜ï¼Ÿ**
- åœ¨å­¦æœ¯è®ºæ–‡è¯­æ–™ä¸Šé¢„è®­ç»ƒ
- åŒ…å«å­¦æœ¯ä¸“ç”¨è¯æ±‡è¡¨
- ç†è§£å­¦æœ¯å†™ä½œé£æ ¼
- ä½†æœ¬é¡¹ç›®ä¸­DeBERTaè¡¨ç°æ›´å¥½

**ä¸ºä»€ä¹ˆDeBERTaæœ€å¥½ï¼Ÿ**
1. **Disentangled Attention**: å†…å®¹å’Œä½ç½®åˆ†ç¦»å»ºæ¨¡
2. **Enhanced Mask Decoder**: æ”¹è¿›çš„æ©ç é¢„æµ‹
3. **è™šæ‹Ÿå¯¹æŠ—è®­ç»ƒ**: é¢„è®­ç»ƒé˜¶æ®µå°±åŒ…å«
4. **åœ¨å¤šä¸ªNLPä»»åŠ¡ä¸ŠSOTA**

### 2. Focal Losså®ç°

**ä»€ä¹ˆæ˜¯Focal Lossï¼Ÿ** (`optimized_BERT.py:78-95`)

Focal Lossæ˜¯ä¸ºäº†è§£å†³ç±»åˆ«ä¸å¹³è¡¡å’Œå›°éš¾æ ·æœ¬å­¦ä¹ é—®é¢˜è€Œæå‡ºçš„æŸå¤±å‡½æ•°ã€‚

**æ ‡å‡†äº¤å‰ç†µ vs Focal Loss**:

```python
# æ ‡å‡†äº¤å‰ç†µ
CE(p, y) = -log(p)  # pæ˜¯é¢„æµ‹æ¦‚ç‡

# Focal Loss
FL(p, y) = -Î±(1-p)^Î³ * log(p)
```

**å‚æ•°è¯´æ˜**:
- **Î± (alpha)**: ç±»åˆ«æƒé‡ï¼Œå¹³è¡¡æ­£è´Ÿæ ·æœ¬ï¼Œé»˜è®¤0.25
- **Î³ (gamma)**: èšç„¦å‚æ•°ï¼Œæ”¾å¤§å›°éš¾æ ·æœ¬æƒé‡ï¼Œé»˜è®¤2.0

**å®ç°ä»£ç ** (`optimized_BERT.py`):

```python
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma

    def forward(self, inputs, targets):
        # inputs: [batch_size, num_classes]
        # targets: [batch_size]

        # è®¡ç®—äº¤å‰ç†µ
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')

        # è®¡ç®—é¢„æµ‹æ¦‚ç‡
        pt = torch.exp(-ce_loss)  # pt in [0,1]

        # è®¡ç®—focal loss
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss

        return focal_loss.mean()
```

**ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ**

| æ ·æœ¬ç±»å‹ | é¢„æµ‹æ¦‚ç‡ pt | (1-pt)^2 | æƒé‡æ•ˆæœ |
|----------|-------------|----------|----------|
| ç®€å•æ­£æ ·æœ¬ | 0.95 | 0.0025 | æƒé‡â†“â†“ |
| ä¸­ç­‰æ ·æœ¬ | 0.70 | 0.09 | æƒé‡â†’ |
| å›°éš¾æ ·æœ¬ | 0.40 | 0.36 | æƒé‡â†‘â†‘ |

**å®éªŒç»“æœ**:
- BERT + CE: 87.5%
- BERT + Focal Loss: 87.8-88.2% (+0.3-0.7%)

**å‚æ•°è°ƒä¼˜å»ºè®®**:
```python
# ç±»åˆ«å¹³è¡¡ï¼ˆæ­£è´Ÿæ ·æœ¬1:1ï¼‰
alpha = 0.25, gamma = 2.0  # æ ‡å‡†é…ç½®

# ç±»åˆ«ä¸å¹³è¡¡ï¼ˆæ­£æ ·æœ¬å°‘ï¼‰
alpha = 0.5, gamma = 2.0   # å¢åŠ æ­£æ ·æœ¬æƒé‡

# æ›´å…³æ³¨å›°éš¾æ ·æœ¬
alpha = 0.25, gamma = 3.0  # å¢åŠ å›°éš¾æ ·æœ¬æƒé‡
```

### 3. å¯¹æŠ—è®­ç»ƒ (FGM)

**ä»€ä¹ˆæ˜¯å¯¹æŠ—è®­ç»ƒï¼Ÿ** (`optimized_BERT.py:78-103`)

å¯¹æŠ—è®­ç»ƒé€šè¿‡åœ¨embeddingå±‚æ·»åŠ æ‰°åŠ¨ï¼Œç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œæå‡æ¨¡å‹é²æ£’æ€§ã€‚

**FGM (Fast Gradient Method)** åŸç†:

```
1. æ­£å¸¸å‰å‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦
2. åœ¨embeddingä¸Šæ·»åŠ æ‰°åŠ¨: r = Îµ * g / ||g||
3. å¯¹æŠ—æ ·æœ¬å‰å‘ä¼ æ’­ï¼Œè®¡ç®—å¯¹æŠ—æŸå¤±
4. æ¢å¤åŸå§‹embedding
5. ç»¼åˆä¸¤æ¬¡æ¢¯åº¦æ›´æ–°å‚æ•°
```

**å®ç°ä»£ç **:

```python
class FGM:
    """Fast Gradient Method å¯¹æŠ—è®­ç»ƒ"""

    def __init__(self, model, epsilon=1.0):
        self.model = model
        self.epsilon = epsilon  # æ‰°åŠ¨å¼ºåº¦
        self.backup = {}        # ä¿å­˜åŸå§‹å‚æ•°

    def attack(self, emb_name='word_embeddings'):
        """åœ¨embeddingä¸Šæ·»åŠ å¯¹æŠ—æ‰°åŠ¨"""
        for name, param in self.model.named_parameters():
            if param.requires_grad and emb_name in name:
                self.backup[name] = param.data.clone()
                # è®¡ç®—æ‰°åŠ¨: r = Îµ * grad / ||grad||
                norm = torch.norm(param.grad)
                if norm != 0 and not torch.isnan(norm):
                    r_at = self.epsilon * param.grad / norm
                    param.data.add_(r_at)

    def restore(self, emb_name='word_embeddings'):
        """æ¢å¤åŸå§‹embeddingå‚æ•°"""
        for name, param in self.model.named_parameters():
            if param.requires_grad and emb_name in name:
                param.data = self.backup[name]
        self.backup = {}
```

**è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨**:

```python
fgm = FGM(model, epsilon=1.0)

for batch in dataloader:
    # æ­£å¸¸ï¿½ï¿½ï¿½ç»ƒ
    loss = model(batch).loss
    loss.backward()  # è®¡ç®—æ¢¯åº¦

    # å¯¹æŠ—è®­ç»ƒ
    fgm.attack()  # æ·»åŠ å¯¹æŠ—æ‰°åŠ¨
    loss_adv = model(batch).loss
    loss_adv.backward()  # å¯¹æŠ—æ ·æœ¬çš„æ¢¯åº¦
    fgm.restore()  # æ¢å¤å‚æ•°

    # æ›´æ–°å‚æ•°ï¼ˆç»¼åˆä¸¤æ¬¡æ¢¯åº¦ï¼‰
    optimizer.step()
    optimizer.zero_grad()
```

**æ•ˆæœåˆ†æ**:

| é…ç½® | å‡†ç¡®ç‡ | æå‡ | è®­ç»ƒæ—¶é—´ |
|------|--------|------|----------|
| æ— å¯¹æŠ—è®­ç»ƒ | 87.8% | - | 2å°æ—¶ |
| FGM (Îµ=0.5) | 88.0% | +0.2% | 2.5å°æ—¶ |
| FGM (Îµ=1.0) | 88.2% | +0.4% | 2.5å°æ—¶ |
| FGM (Îµ=2.0) | 88.1% | +0.3% | 2.5å°æ—¶ |

**æœ€ä½³å®è·µ**:
- epsilon=1.0 æ˜¯å¤§å¤šæ•°ä»»åŠ¡çš„æœ€ä¼˜å€¼
- å¯¹æŠ—è®­ç»ƒä¼šå¢åŠ 30-50%è®­ç»ƒæ—¶é—´
- åœ¨embeddingå±‚æ·»åŠ æ‰°åŠ¨æœ€æœ‰æ•ˆ
- ä¸Focal Lossç»„åˆæ•ˆæœæ›´å¥½

### 4. è®­ç»ƒç­–ç•¥ä¼˜åŒ–

#### 4.1 å­¦ä¹ ç‡è°ƒåº¦

**Warmup + Cosine Decay**:

```python
from transformers import get_cosine_schedule_with_warmup

# æ€»è®­ç»ƒæ­¥æ•°
total_steps = len(train_loader) * epochs

# Warmupæ­¥æ•°ï¼ˆé€šå¸¸10%ï¼‰
warmup_steps = int(total_steps * 0.1)

# åˆ›å»ºè°ƒåº¦å™¨
scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=warmup_steps,
    num_training_steps=total_steps
)

# è®­ç»ƒå¾ªç¯ä¸­
for epoch in range(epochs):
    for batch in train_loader:
        ...
        optimizer.step()
        scheduler.step()  # æ¯ä¸ªbatchæ›´æ–°ä¸€æ¬¡
```

**å­¦ä¹ ç‡å˜åŒ–æ›²çº¿**:

```
LR
 |
 2e-5  ----*  (peak)
 |          ****
 |        **    ***
 |      **         ***
 |    **              ***
 |  **                   ***
 |**                        ******
 +---------------------------------> Steps
   Warmup     Training       Decay
   (10%)        (90%)
```

**ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ**
- Warmup: é¿å…è®­ç»ƒåˆæœŸæ¢¯åº¦è¿‡å¤§
- Cosine Decay: å¹³æ»‘é™ä½å­¦ä¹ ç‡ï¼Œæå‡æ”¶æ•›

#### 4.2 æ—©åœæœºåˆ¶ (Early Stopping)

**å®ç°** (`train_bert_optimized_v2.py`):

```python
class EarlyStopping:
    def __init__(self, patience=3, min_delta=0.001):
        self.patience = patience    # å®¹å¿è½®æ•°
        self.min_delta = min_delta  # æœ€å°æ”¹è¿›
        self.counter = 0
        self.best_score = None
        self.best_epoch = 0

    def __call__(self, val_score, epoch):
        if self.best_score is None:
            self.best_score = val_score
            self.best_epoch = epoch
            return False

        # æ€§èƒ½æå‡
        if val_score > self.best_score + self.min_delta:
            self.best_score = val_score
            self.best_epoch = epoch
            self.counter = 0
            return False

        # æ€§èƒ½æœªæå‡
        self.counter += 1
        if self.counter >= self.patience:
            print(f"Early stopping at epoch {epoch}")
            print(f"Best score: {self.best_score:.4f} at epoch {self.best_epoch}")
            return True  # åº”è¯¥åœæ­¢

        return False
```

**ä½¿ç”¨ç¤ºä¾‹**:

```python
early_stopping = EarlyStopping(patience=3, min_delta=0.001)

for epoch in range(max_epochs):
    train_loss = train_one_epoch(...)
    val_score = evaluate_on_val(...)

    # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ
    if early_stopping(val_score, epoch):
        break  # åœæ­¢è®­ç»ƒ

    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if val_score > best_val_score:
        save_model(model, 'best_model.pt')
```

**å‚æ•°å»ºè®®**:
- patience=3: é€‚åˆå°æ•°æ®é›†ï¼ˆ< 10Kæ ·æœ¬ï¼‰
- patience=5: é€‚åˆå¤§æ•°æ®é›†ï¼ˆ> 100Kæ ·æœ¬ï¼‰
- min_delta=0.001: 0.1%çš„æœ€å°æ”¹è¿›é˜ˆå€¼

#### 4.3 æŒ‡æ•°ç§»åŠ¨å¹³å‡ (EMA)

**ä»€ä¹ˆæ˜¯EMAï¼Ÿ** (`optimized_BERT.py:105-137`)

EMAç»´æŠ¤æ¨¡å‹å‚æ•°çš„ç§»åŠ¨å¹³å‡ï¼Œæå‡æ¨¡å‹ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**æ›´æ–°å…¬å¼**:
```
Î¸_shadow = decay * Î¸_shadow + (1 - decay) * Î¸_current
```

**å®ç°**:

```python
class EMA:
    def __init__(self, model, decay=0.999):
        self.model = model
        self.decay = decay
        self.shadow = {}  # å½±å­å‚æ•°
        self.register()   # åˆå§‹åŒ–

    def update(self):
        """æ›´æ–°å½±å­å‚æ•°"""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                new_avg = (1 - self.decay) * param.data + \
                         self.decay * self.shadow[name]
                self.shadow[name] = new_avg.clone()

    def apply_shadow(self):
        """ä½¿ç”¨å½±å­å‚æ•°ï¼ˆæ¨ç†æ—¶ï¼‰"""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.backup[name] = param.data
                param.data = self.shadow[name]
```

**ä½¿ç”¨æ–¹å¼**:

```python
ema = EMA(model, decay=0.999)

# è®­ç»ƒé˜¶æ®µ
for batch in train_loader:
    loss = model(batch).loss
    loss.backward()
    optimizer.step()
    ema.update()  # æ›´æ–°EMAå‚æ•°

# éªŒè¯/æµ‹è¯•é˜¶æ®µ
ema.apply_shadow()  # ä½¿ç”¨EMAå‚æ•°
val_score = evaluate(model, val_loader)
ema.restore()       # æ¢å¤è®­ç»ƒå‚æ•°
```

**æ•ˆæœ**:
- æå‡æ¨¡å‹ç¨³å®šæ€§
- å‡å°‘æ–¹å·®
- é€šå¸¸æå‡0.2-0.5%å‡†ç¡®ç‡

### 5. æ•°æ®å¢å¼º

**æ–‡æœ¬å¢å¼ºæŠ€æœ¯** (`optimized_BERT.py:36-52`):

```python
def augment_text(text: str) -> str:
    words = text.split()

    # 1. éšæœºåˆ é™¤ (10%æ¦‚ç‡)
    if random.random() < 0.1 and len(words) > 2:
        idx = random.randint(0, len(words) - 1)
        words.pop(idx)

    # 2. éšæœºäº¤æ¢ç›¸é‚»è¯ (10%æ¦‚ç‡)
    if random.random() < 0.1 and len(words) > 1:
        idx = random.randint(0, len(words) - 2)
        words[idx], words[idx+1] = words[idx+1], words[idx]

    return ' '.join(words)
```

**ç¤ºä¾‹**:
```
åŸæ–‡: "Deep Learning for Natural Language Processing"
å¢å¼º1: "Deep Learning Natural Language Processing"  (åˆ é™¤for)
å¢å¼º2: "Deep Learning for Language Natural Processing"  (äº¤æ¢)
```

**æ³¨æ„äº‹é¡¹**:
- ä»…åœ¨è®­ç»ƒæ—¶ä½¿ç”¨
- æ¦‚ç‡ä¸å®œè¿‡é«˜ï¼ˆæ¨è10-20%ï¼‰
- ä¸é€‚ç”¨äºçŸ­æ–‡æœ¬ï¼ˆ<3è¯ï¼‰


---

**å®ç°å®Œæˆåº¦**: âœ… 100%  
**æœ€ä½³æ€§èƒ½**: ğŸ¯ 90.1% (DeBERTa)  
**ç›¸å…³æ–‡æ¡£**: BERT_OPTIMIZATION_README.md, QUICK_START.md
