"""
train_bert_optimized_v2.py
==========================
BERTæ·±åº¦ä¼˜åŒ–è®­ç»ƒè„šæœ¬
å®žçŽ°å¤šç§ä¼˜åŒ–ç­–ç•¥ä»¥æå‡å¬å›žçŽ‡å’Œæ•´ä½“æ€§èƒ½

ä¼˜åŒ–å†…å®¹:
1. SciBERTæ¨¡åž‹ (å­¦æœ¯é¢†åŸŸä¸“ç”¨)
2. éªŒè¯é›†+Early Stopping
3. ç±»åˆ«æƒé‡/Focal Loss (æå‡å¬å›žçŽ‡)
4. è°ƒæ•´max_lengthåˆ°96
5. Cosineå­¦ä¹ çŽ‡è°ƒåº¦
6. Layer-wiseå­¦ä¹ çŽ‡è¡°å‡
7. å¯¹æŠ—è®­ç»ƒ(å¯é€‰)
8. æ··åˆç²¾åº¦è®­ç»ƒ
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import os
from typing import List, Dict, Optional, Tuple
from transformers import (
    AutoTokenizer,
    AutoModel,
    AutoModelForSequenceClassification,
    get_linear_schedule_with_warmup,
    get_cosine_schedule_with_warmup
)
from torch.utils.data import Dataset, DataLoader as TorchDataLoader
from torch.optim import AdamW
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm
# æ‰‹åŠ¨å®žçŽ°train_test_splité¿å…sklearnä¾èµ–é—®é¢˜
def manual_train_test_split(titles, labels, test_size=0.1, random_state=42):
    import random
    random.seed(random_state)
    indices = list(range(len(titles)))
    random.shuffle(indices)
    split_idx = int(len(indices) * (1 - test_size))
    train_idx = indices[:split_idx]
    test_idx = indices[split_idx:]
    return ([titles[i] for i in train_idx], [titles[i] for i in test_idx],
            [labels[i] for i in train_idx], [labels[i] for i in test_idx])

# æ‰‹åŠ¨å®žçŽ°metricsé¿å…sklearnä¾èµ–
def calculate_metrics(y_true, y_pred):
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    tp = np.sum((y_true == 1) & (y_pred == 1))
    tn = np.sum((y_true == 0) & (y_pred == 0))
    fp = np.sum((y_true == 0) & (y_pred == 1))
    fn = np.sum((y_true == 1) & (y_pred == 0))

    accuracy = (tp + tn) / len(y_true)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'confusion_matrix': [[tn, fp], [fn, tp]]
    }

import warnings
warnings.filterwarnings('ignore')


class TitleDataset(Dataset):
    """PyTorch dataset for BERT model"""

    def __init__(self, titles: List[str], labels: List[int], tokenizer, max_length=96):
        self.titles = titles
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.titles)

    def __getitem__(self, idx):
        title = self.titles[idx]
        label = self.labels[idx]

        encoding = self.tokenizer(
            title,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long)
        }


class FocalLoss(nn.Module):
    """
    Focal Loss for addressing class imbalance
    é‡ç‚¹å…³æ³¨éš¾åˆ†æ ·æœ¬,æå‡å¬å›žçŽ‡
    """
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class FGM:
    """
    Fast Gradient Method å¯¹æŠ—è®­ç»ƒ
    æé«˜æ¨¡åž‹é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›
    """
    def __init__(self, model, epsilon=1.0):
        self.model = model
        self.epsilon = epsilon
        self.backup = {}

    def attack(self, emb_name='embeddings'):
        for name, param in self.model.named_parameters():
            if param.requires_grad and emb_name in name:
                self.backup[name] = param.data.clone()
                norm = torch.norm(param.grad)
                if norm != 0 and not torch.isnan(norm):
                    r_at = self.epsilon * param.grad / norm
                    param.data.add_(r_at)

    def restore(self, emb_name='embeddings'):
        for name, param in self.model.named_parameters():
            if param.requires_grad and emb_name in name:
                if name in self.backup:
                    param.data = self.backup[name]
        self.backup = {}


class OptimizedBERTClassifier:
    """
    æ·±åº¦ä¼˜åŒ–çš„BERTåˆ†ç±»å™¨
    """

    MODEL_OPTIONS = {
        'scibert': 'allenai/scibert_scivocab_uncased',  # â­â­â­â­â­ å­¦æœ¯è®ºæ–‡ä¸“ç”¨
        'roberta': 'roberta-base',                       # â­â­â­â­ ä¼˜ç§€æ€§èƒ½
        'bert-base': 'bert-base-uncased',                # â­â­â­ Baseline
    }

    def __init__(
        self,
        model_name='scibert',
        max_length=96,  # å¢žåŠ åˆ°96ä»¥è¦†ç›–90%+çš„æ ‡é¢˜
        model_path='models/bert_optimized_v2.pt',
        dropout_rate=0.2
    ):
        """åˆå§‹åŒ–ä¼˜åŒ–çš„BERTåˆ†ç±»å™¨"""

        # è§£æžæ¨¡åž‹åç§°
        if model_name in self.MODEL_OPTIONS:
            self.model_name = self.MODEL_OPTIONS[model_name]
            print(f"âœ“ ä½¿ç”¨æ¨¡åž‹: {model_name} ({self.model_name})")
        else:
            self.model_name = model_name
            print(f"âœ“ ä½¿ç”¨è‡ªå®šä¹‰æ¨¡åž‹: {self.model_name}")

        self.max_length = max_length
        self.model_path = model_path
        self.dropout_rate = dropout_rate

        # è®¾ç½®è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        if torch.cuda.is_available():
            print(f"âœ“ GPU: {torch.cuda.get_device_name(0)}")
            print(f"âœ“ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        else:
            print(f"âš ï¸  ä½¿ç”¨CPU (è®­ç»ƒä¼šå¾ˆæ…¢)")

        # åŠ è½½tokenizer
        print(f"åŠ è½½tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)

        # åŠ è½½æ¨¡åž‹
        print(f"åŠ è½½åˆ†ç±»æ¨¡åž‹...")
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name,
            num_labels=2,
            hidden_dropout_prob=dropout_rate,
            attention_probs_dropout_prob=dropout_rate,
            output_attentions=False,
            output_hidden_states=False
        ).to(self.device)

        # ç‰¹å¾æå–æ¨¡åž‹
        self.feature_model = AutoModel.from_pretrained(self.model_name).to(self.device)

        self.is_trained = False
        self.training_history = {
            'train_loss': [], 'train_acc': [], 'train_f1': [],
            'val_loss': [], 'val_acc': [], 'val_f1': [], 'val_recall': [], 'val_precision': []
        }

    def get_layer_wise_optimizer(self, learning_rate=2e-5, weight_decay=0.01, layer_decay=0.95):
        """
        Layer-wiseå­¦ä¹ çŽ‡è¡°å‡
        åº•å±‚(é è¿‘è¾“å…¥)å­¦ä¹ çŽ‡å°,é¡¶å±‚(é è¿‘è¾“å‡º)å­¦ä¹ çŽ‡å¤§
        """
        no_decay = ["bias", "LayerNorm.weight"]
        optimizer_grouped_parameters = []

        # èŽ·å–encoderå±‚æ•°
        if hasattr(self.model, 'bert'):
            encoder = self.model.bert.encoder
            embeddings = self.model.bert.embeddings
        elif hasattr(self.model, 'roberta'):
            encoder = self.model.roberta.encoder
            embeddings = self.model.roberta.embeddings
        elif hasattr(self.model, 'deberta'):
            encoder = self.model.deberta.encoder
            embeddings = self.model.deberta.embeddings
        else:
            print("âš ï¸  æœªè¯†åˆ«çš„æ¨¡åž‹æž¶æž„,ä½¿ç”¨æ ‡å‡†ä¼˜åŒ–å™¨")
            return AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)

        num_layers = len(encoder.layer)

        # Embeddingså±‚
        optimizer_grouped_parameters.append({
            "params": [p for n, p in embeddings.named_parameters() if not any(nd in n for nd in no_decay)],
            "weight_decay": weight_decay,
            "lr": learning_rate * (layer_decay ** num_layers)
        })

        # Encoderå„å±‚
        for i in range(num_layers):
            optimizer_grouped_parameters.append({
                "params": [p for n, p in encoder.layer[i].named_parameters() if not any(nd in n for nd in no_decay)],
                "weight_decay": weight_decay,
                "lr": learning_rate * (layer_decay ** (num_layers - 1 - i))
            })
            optimizer_grouped_parameters.append({
                "params": [p for n, p in encoder.layer[i].named_parameters() if any(nd in n for nd in no_decay)],
                "weight_decay": 0.0,
                "lr": learning_rate * (layer_decay ** (num_layers - 1 - i))
            })

        # åˆ†ç±»å¤´(æœ€é«˜å­¦ä¹ çŽ‡)
        optimizer_grouped_parameters.append({
            "params": self.model.classifier.parameters(),
            "weight_decay": 0.0,
            "lr": learning_rate * 10
        })

        print(f"âœ“ ä½¿ç”¨Layer-wiseå­¦ä¹ çŽ‡è¡°å‡ (decay={layer_decay})")
        print(f"  - Embeddingså±‚: {learning_rate * (layer_decay ** num_layers):.2e}")
        print(f"  - é¡¶å±‚Encoder: {learning_rate:.2e}")
        print(f"  - åˆ†ç±»å¤´: {learning_rate * 10:.2e}")

        return AdamW(optimizer_grouped_parameters, lr=learning_rate)

    def train(
        self,
        train_titles: List[str],
        train_labels: List[int],
        val_ratio=0.1,
        epochs=10,
        batch_size=32,
        learning_rate=2e-5,
        warmup_ratio=0.1,
        scheduler_type='cosine',  # 'linear' or 'cosine'
        loss_type='focal',  # 'ce', 'weighted_ce', or 'focal'
        class_weight_positive=1.3,  # æ­£æ ·æœ¬æƒé‡(æå‡å¬å›žçŽ‡)
        focal_alpha=0.25,
        focal_gamma=2.0,
        early_stopping_patience=3,
        use_layer_wise_lr=True,
        layer_decay=0.95,
        use_adversarial=True,  # å¯¹æŠ—è®­ç»ƒ
        adv_epsilon=1.0,
        use_mixed_precision=True,  # æ··åˆç²¾åº¦
        max_grad_norm=1.0,
        save_model=True
    ):
        """
        è®­ç»ƒä¼˜åŒ–çš„BERTæ¨¡åž‹

        Args:
            train_titles: è®­ç»ƒæ ‡é¢˜åˆ—è¡¨
            train_labels: è®­ç»ƒæ ‡ç­¾åˆ—è¡¨
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            learning_rate: å­¦ä¹ çŽ‡
            warmup_ratio: warmupæ¯”ä¾‹
            scheduler_type: å­¦ä¹ çŽ‡è°ƒåº¦å™¨ç±»åž‹
            loss_type: æŸå¤±å‡½æ•°ç±»åž‹
            class_weight_positive: æ­£æ ·æœ¬æƒé‡
            focal_alpha: Focal Lossçš„alphaå‚æ•°
            focal_gamma: Focal Lossçš„gammaå‚æ•°
            early_stopping_patience: Early stoppingè€å¿ƒå€¼
            use_layer_wise_lr: æ˜¯å¦ä½¿ç”¨layer-wiseå­¦ä¹ çŽ‡
            layer_decay: Layer-wiseè¡°å‡çŽ‡
            use_adversarial: æ˜¯å¦ä½¿ç”¨å¯¹æŠ—è®­ç»ƒ
            adv_epsilon: å¯¹æŠ—è®­ç»ƒçš„epsilon
            use_mixed_precision: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
            max_grad_norm: æ¢¯åº¦è£å‰ªé˜ˆå€¼
            save_model: æ˜¯å¦ä¿å­˜æ¨¡åž‹
        """
        print("\n" + "="*80)
        print(" ðŸš€ BERTæ·±åº¦ä¼˜åŒ–è®­ç»ƒ")
        print("="*80)
        print(f"æ¨¡åž‹: {self.model_name}")
        print(f"æ€»æ ·æœ¬: {len(train_titles)}")
        print(f"éªŒè¯é›†æ¯”ä¾‹: {val_ratio}")
        print(f"Max Length: {self.max_length}")
        print(f"Epochs: {epochs}")
        print(f"Batch Size: {batch_size}")
        print(f"Learning Rate: {learning_rate}")
        print(f"Scheduler: {scheduler_type}")
        print(f"Loss Type: {loss_type}")
        if loss_type == 'focal':
            print(f"Focal Loss: alpha={focal_alpha}, gamma={focal_gamma}")
        elif loss_type == 'weighted_ce':
            print(f"Class Weight: [1.0, {class_weight_positive}]")
        print(f"Early Stopping: {early_stopping_patience}")
        print(f"Layer-wise LR: {use_layer_wise_lr}")
        print(f"å¯¹æŠ—è®­ç»ƒ: {use_adversarial}")
        print(f"æ··åˆç²¾åº¦: {use_mixed_precision}")
        print("="*80)

        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        train_X, val_X, train_y, val_y = manual_train_test_split(
            train_titles, train_labels,
            test_size=val_ratio,
            random_state=42
        )

        print(f"\næ•°æ®åˆ’åˆ†:")
        print(f"  è®­ç»ƒé›†: {len(train_X)} æ ·æœ¬")
        print(f"  éªŒè¯é›†: {len(val_X)} æ ·æœ¬")
        print(f"  è®­ç»ƒé›†æ­£è´Ÿæ¯”: {sum(train_y)}/{len(train_y)-sum(train_y)} = {sum(train_y)/(len(train_y)-sum(train_y)):.2f}")
        print(f"  éªŒè¯é›†æ­£è´Ÿæ¯”: {sum(val_y)}/{len(val_y)-sum(val_y)} = {sum(val_y)/(len(val_y)-sum(val_y)):.2f}")

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TitleDataset(train_X, train_y, self.tokenizer, self.max_length)
        val_dataset = TitleDataset(val_X, val_y, self.tokenizer, self.max_length)

        train_dataloader = TorchDataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_dataloader = TorchDataLoader(val_dataset, batch_size=batch_size, shuffle=False)

        # è®¾ç½®ä¼˜åŒ–å™¨
        if use_layer_wise_lr:
            optimizer = self.get_layer_wise_optimizer(learning_rate, weight_decay=0.01, layer_decay=layer_decay)
        else:
            optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)

        # è®¾ç½®å­¦ä¹ çŽ‡è°ƒåº¦å™¨
        total_steps = len(train_dataloader) * epochs
        warmup_steps = int(total_steps * warmup_ratio)

        if scheduler_type == 'cosine':
            scheduler = get_cosine_schedule_with_warmup(
                optimizer,
                num_warmup_steps=warmup_steps,
                num_training_steps=total_steps
            )
        else:
            scheduler = get_linear_schedule_with_warmup(
                optimizer,
                num_warmup_steps=warmup_steps,
                num_training_steps=total_steps
            )

        print(f"\nè®­ç»ƒé…ç½®:")
        print(f"  æ€»æ­¥æ•°: {total_steps}")
        print(f"  Warmupæ­¥æ•°: {warmup_steps}")

        # è®¾ç½®æŸå¤±å‡½æ•°
        if loss_type == 'focal':
            criterion = FocalLoss(alpha=focal_alpha, gamma=focal_gamma)
            print(f"  ä½¿ç”¨Focal Loss (alpha={focal_alpha}, gamma={focal_gamma})")
        elif loss_type == 'weighted_ce':
            class_weights = torch.tensor([1.0, class_weight_positive]).to(self.device)
            criterion = nn.CrossEntropyLoss(weight=class_weights)
            print(f"  ä½¿ç”¨åŠ æƒäº¤å‰ç†µ (æ­£æ ·æœ¬æƒé‡={class_weight_positive})")
        else:
            criterion = nn.CrossEntropyLoss()
            print(f"  ä½¿ç”¨æ ‡å‡†äº¤å‰ç†µ")

        # å¯¹æŠ—è®­ç»ƒ
        fgm = FGM(self.model, epsilon=adv_epsilon) if use_adversarial else None

        # æ··åˆç²¾åº¦
        scaler = GradScaler() if use_mixed_precision else None

        # Early stopping
        best_val_f1 = 0.0
        best_val_recall = 0.0
        patience_counter = 0

        # è®­ç»ƒå¾ªçŽ¯
        self.model.train()

        for epoch in range(epochs):
            print(f"\n{'='*80}")
            print(f"ðŸ“Š Epoch {epoch + 1}/{epochs}")
            print(f"{'='*80}")

            # è®­ç»ƒé˜¶æ®µ
            total_loss = 0
            all_predictions = []
            all_labels = []

            progress_bar = tqdm(train_dataloader, desc="Training")

            for batch_idx, batch in enumerate(progress_bar):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['label'].to(self.device)

                optimizer.zero_grad()

                # å‰å‘ä¼ æ’­(æ··åˆç²¾åº¦)
                if use_mixed_precision:
                    with autocast():
                        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                        logits = outputs.logits
                        loss = criterion(logits, labels)
                else:
                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                    logits = outputs.logits
                    loss = criterion(logits, labels)

                # åå‘ä¼ æ’­
                if use_mixed_precision:
                    scaler.scale(loss).backward()
                else:
                    loss.backward()

                # å¯¹æŠ—è®­ç»ƒ
                if use_adversarial and fgm:
                    fgm.attack()
                    if use_mixed_precision:
                        with autocast():
                            outputs_adv = self.model(input_ids=input_ids, attention_mask=attention_mask)
                            loss_adv = criterion(outputs_adv.logits, labels)
                        scaler.scale(loss_adv).backward()
                    else:
                        outputs_adv = self.model(input_ids=input_ids, attention_mask=attention_mask)
                        loss_adv = criterion(outputs_adv.logits, labels)
                        loss_adv.backward()
                    fgm.restore()

                # æ¢¯åº¦è£å‰ªå’Œä¼˜åŒ–
                if use_mixed_precision:
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)
                    optimizer.step()

                scheduler.step()

                # ç»Ÿè®¡
                total_loss += loss.item()
                predictions = torch.argmax(logits, dim=1).cpu().numpy()
                all_predictions.extend(predictions)
                all_labels.extend(labels.cpu().numpy())

                # æ›´æ–°è¿›åº¦æ¡
                current_acc = sum(np.array(all_predictions) == np.array(all_labels)) / len(all_labels)
                progress_bar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'acc': f'{current_acc:.4f}',
                    'lr': f'{scheduler.get_last_lr()[0]:.2e}'
                })

            # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
            avg_train_loss = total_loss / len(train_dataloader)
            train_metrics = calculate_metrics(all_labels, all_predictions)
            train_acc = train_metrics['accuracy']
            train_f1 = train_metrics['f1']

            self.training_history['train_loss'].append(avg_train_loss)
            self.training_history['train_acc'].append(train_acc)
            self.training_history['train_f1'].append(train_f1)

            print(f"\nè®­ç»ƒç»“æžœ:")
            print(f"  Loss: {avg_train_loss:.4f}")
            print(f"  Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)")
            print(f"  F1 Score: {train_f1:.4f}")

            # éªŒè¯é˜¶æ®µ
            val_loss, val_acc, val_f1, val_recall, val_precision = self._validate(
                val_dataloader, criterion
            )

            self.training_history['val_loss'].append(val_loss)
            self.training_history['val_acc'].append(val_acc)
            self.training_history['val_f1'].append(val_f1)
            self.training_history['val_recall'].append(val_recall)
            self.training_history['val_precision'].append(val_precision)

            print(f"\néªŒè¯ç»“æžœ:")
            print(f"  Loss: {val_loss:.4f}")
            print(f"  Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)")
            print(f"  Precision: {val_precision:.4f} ({val_precision*100:.2f}%)")
            print(f"  Recall: {val_recall:.4f} ({val_recall*100:.2f}%) â­")
            print(f"  F1 Score: {val_f1:.4f}")

            # Early stoppingæ£€æŸ¥
            if val_f1 > best_val_f1:
                best_val_f1 = val_f1
                best_val_recall = val_recall
                patience_counter = 0
                print(f"  âœ“ éªŒè¯F1æå‡! (best: {best_val_f1:.4f}, recall: {best_val_recall:.4f})")

                if save_model:
                    self.save_model(suffix='_best')
            else:
                patience_counter += 1
                print(f"  - æœªæå‡ (patience: {patience_counter}/{early_stopping_patience})")

                if patience_counter >= early_stopping_patience:
                    print(f"\nâš ï¸  è§¦å‘Early Stopping!")
                    print(f"æœ€ä½³éªŒè¯F1: {best_val_f1:.4f}")
                    print(f"æœ€ä½³éªŒè¯Recall: {best_val_recall:.4f}")
                    break

        self.is_trained = True
        print("\n" + "="*80)
        print(" âœ“ è®­ç»ƒå®Œæˆ!")
        print("="*80)
        print(f"æœ€ä½³éªŒè¯F1: {best_val_f1:.4f} ({best_val_f1*100:.2f}%)")
        print(f"æœ€ä½³éªŒè¯Recall: {best_val_recall:.4f} ({best_val_recall*100:.2f}%)")

        if save_model:
            self.save_model()

        return self.training_history

    def _validate(self, val_dataloader, criterion):
        """éªŒè¯"""
        self.model.eval()

        total_loss = 0
        all_predictions = []
        all_labels = []

        with torch.no_grad():
            for batch in tqdm(val_dataloader, desc="Validating"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['label'].to(self.device)

                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outputs.logits
                loss = criterion(logits, labels)

                total_loss += loss.item()
                predictions = torch.argmax(logits, dim=1).cpu().numpy()
                all_predictions.extend(predictions)
                all_labels.extend(labels.cpu().numpy())

        self.model.train()

        avg_loss = total_loss / len(val_dataloader)

        # Use manual metrics calculation
        metrics = calculate_metrics(all_labels, all_predictions)
        accuracy = metrics['accuracy']
        f1 = metrics['f1']
        recall = metrics['recall']
        precision = metrics['precision']

        return avg_loss, accuracy, f1, recall, precision

    def save_model(self, suffix=''):
        """ä¿å­˜æ¨¡åž‹"""
        save_path = self.model_path.replace('.pt', f'{suffix}.pt') if suffix else self.model_path
        os.makedirs(os.path.dirname(save_path), exist_ok=True)

        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'model_name': self.model_name,
            'max_length': self.max_length,
            'is_trained': self.is_trained,
            'training_history': self.training_history
        }

        torch.save(checkpoint, save_path)
        print(f"\nâœ“ æ¨¡åž‹å·²ä¿å­˜: {save_path}")

    def load_model(self):
        """åŠ è½½æ¨¡åž‹"""
        if not os.path.exists(self.model_path):
            print(f"âš ï¸  æ¨¡åž‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")
            return False

        print(f"åŠ è½½æ¨¡åž‹: {self.model_path}")

        try:
            checkpoint = torch.load(self.model_path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.is_trained = checkpoint.get('is_trained', True)
            self.training_history = checkpoint.get('training_history', {})
            print("âœ“ æ¨¡åž‹åŠ è½½æˆåŠŸ!")
            return True
        except Exception as e:
            print(f"âš ï¸  åŠ è½½å¤±è´¥: {str(e)}")
            return False

    def predict(self, titles: List[str], batch_size=32) -> np.ndarray:
        """é¢„æµ‹"""
        if not self.is_trained:
            raise ValueError("æ¨¡åž‹æœªè®­ç»ƒ!è¯·å…ˆè®­ç»ƒæˆ–åŠ è½½æ¨¡åž‹")

        self.model.eval()
        predictions = []

        dummy_labels = [0] * len(titles)
        dataset = TitleDataset(titles, dummy_labels, self.tokenizer, self.max_length)
        dataloader = TorchDataLoader(dataset, batch_size=batch_size, shuffle=False)

        with torch.no_grad():
            for batch in tqdm(dataloader, desc="Predicting"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)

                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outputs.logits
                batch_preds = torch.argmax(logits, dim=1).cpu().numpy()
                predictions.extend(batch_preds)

        return np.array(predictions)

    def predict_proba(self, titles: List[str], batch_size=32) -> np.ndarray:
        """é¢„æµ‹æ¦‚çŽ‡"""
        if not self.is_trained:
            raise ValueError("æ¨¡åž‹æœªè®­ç»ƒ!è¯·å…ˆè®­ç»ƒæˆ–åŠ è½½æ¨¡åž‹")

        self.model.eval()
        probabilities = []

        dummy_labels = [0] * len(titles)
        dataset = TitleDataset(titles, dummy_labels, self.tokenizer, self.max_length)
        dataloader = TorchDataLoader(dataset, batch_size=batch_size, shuffle=False)

        with torch.no_grad():
            for batch in tqdm(dataloader, desc="Computing probabilities"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)

                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outputs.logits
                probs = torch.softmax(logits, dim=1).cpu().numpy()
                probabilities.append(probs)

        return np.vstack(probabilities)

    def get_feature_vectors(self, titles: List[str], batch_size=32) -> np.ndarray:
        """èŽ·å–ç‰¹å¾å‘é‡"""
        if not self.is_trained:
            raise ValueError("æ¨¡åž‹æœªè®­ç»ƒ!è¯·å…ˆè®­ç»ƒæˆ–åŠ è½½æ¨¡åž‹")

        self.feature_model.eval()
        embeddings = []

        dummy_labels = [0] * len(titles)
        dataset = TitleDataset(titles, dummy_labels, self.tokenizer, self.max_length)
        dataloader = TorchDataLoader(dataset, batch_size=batch_size, shuffle=False)

        with torch.no_grad():
            for batch in tqdm(dataloader, desc="Extracting features"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)

                outputs = self.feature_model(input_ids=input_ids, attention_mask=attention_mask)
                cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                embeddings.append(cls_embeddings)

        return np.vstack(embeddings)


def main():
    """ä¸»å‡½æ•°: è®­ç»ƒä¼˜åŒ–çš„BERTæ¨¡åž‹"""
    from data_loader import DataLoader as TitleDataLoader

    print("\n" + "="*80)
    print(" ðŸš€ BERTæ·±åº¦ä¼˜åŒ–è®­ç»ƒè„šæœ¬")
    print("="*80)

    # èŽ·å–è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))

    # åŠ è½½æ•°æ®
    print("\nåŠ è½½æ•°æ®...")
    print(f"è„šæœ¬ç›®å½•: {script_dir}")
    train_titles, train_labels, test_titles, test_labels = TitleDataLoader.prepare_dataset(
        os.path.join(script_dir, 'data/positive.txt'),
        os.path.join(script_dir, 'data/negative.txt'),
        os.path.join(script_dir, 'data/testSet-1000.xlsx')
    )

    if len(train_titles) == 0:
        print("âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶!")
        return

    print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ")
    print(f"  è®­ç»ƒé›†: {len(train_titles)} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(test_titles)} æ ·æœ¬")

    # åˆ›å»ºä¼˜åŒ–çš„åˆ†ç±»å™¨
    classifier = OptimizedBERTClassifier(
        model_name='scibert',  # ä½¿ç”¨SciBERT (å­¦æœ¯é¢†åŸŸä¸“ç”¨)
        max_length=96,  # å¢žåŠ åˆ°96
        model_path=os.path.join(script_dir, 'models/bert_scibert_optimized.pt'),
        dropout_rate=0.2
    )

    # è®­ç»ƒ
    history = classifier.train(
        train_titles,
        train_labels,
        val_ratio=0.1,  # 10%éªŒè¯é›†
        epochs=10,
        batch_size=32,
        learning_rate=2e-5,
        warmup_ratio=0.1,
        scheduler_type='cosine',  # Cosineè°ƒåº¦å™¨
        loss_type='focal',  # Focal Loss (æå‡å¬å›žçŽ‡)
        focal_alpha=0.25,
        focal_gamma=2.0,
        early_stopping_patience=3,
        use_layer_wise_lr=True,  # Layer-wiseå­¦ä¹ çŽ‡
        layer_decay=0.95,
        use_adversarial=True,  # å¯¹æŠ—è®­ç»ƒ
        adv_epsilon=1.0,
        use_mixed_precision=True,  # æ··åˆç²¾åº¦
        save_model=True
    )

    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    print("\n" + "="*80)
    print(" ðŸ“Š æµ‹è¯•é›†è¯„ä¼°")
    print("="*80)

    predictions = classifier.predict(test_titles)

    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(test_labels, predictions,
                                target_names=['Incorrect', 'Correct'],
                                digits=4))

    print("\næ··æ·†çŸ©é˜µ:")
    cm = confusion_matrix(test_labels, predictions)
    print(cm)
    print(f"\nTN={cm[0,0]}, FP={cm[0,1]}, FN={cm[1,0]}, TP={cm[1,1]}")

    # å¯¹æ¯”baseline
    baseline_acc = 0.8525
    baseline_recall = 0.8116
    new_acc = sum(predictions == test_labels) / len(test_labels)
    new_recall = recall_score(test_labels, predictions)

    print("\n" + "="*80)
    print(" ðŸŽ¯ å¯¹æ¯”Baseline")
    print("="*80)
    print(f"Baseline Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)")
    print(f"New Accuracy:      {new_acc:.4f} ({new_acc*100:.2f}%) [{'+' if new_acc > baseline_acc else ''}{(new_acc-baseline_acc)*100:.2f}%]")
    print(f"\nBaseline Recall:   {baseline_recall:.4f} ({baseline_recall*100:.2f}%)")
    print(f"New Recall:        {new_recall:.4f} ({new_recall*100:.2f}%) [{'+' if new_recall > baseline_recall else ''}{(new_recall-baseline_recall)*100:.2f}%]")


if __name__ == "__main__":
    main()
