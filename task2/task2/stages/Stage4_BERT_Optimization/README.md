# Stage4: BERTä¼˜åŒ–å®éªŒ

**æ—¶é—´**ï¼š2024å¹´11æœˆ16-28æ—¥  
**ç›®æ ‡**ï¼šæ¢ç´¢BERTçš„é«˜çº§ä¼˜åŒ–æŠ€æœ¯ï¼Œæµ‹è¯•ä¸åŒæ¨¡å‹å’ŒæŸå¤±å‡½æ•°

## ğŸ“ æ–‡ä»¶åˆ—è¡¨

| æ–‡ä»¶ | è¡Œæ•° | åŠŸèƒ½ | æ—¥æœŸ |
|------|------|------|------|
| `train_optimized_bert.py` | 261 | BERTè®­ç»ƒV1 | Nov 16 |
| `bert_classifier_optimized.py` | 736 | BERTä¼˜åŒ–ç±»ï¼ˆV2ï¼‰ | Nov 16 |
| `optimized_BERT.py` | 376 | BERTä¼˜åŒ–æ¡†æ¶ | Nov 16 |
| `train_bert_optimized_v2.py` | 760 | BERTè®­ç»ƒV2ï¼ˆæœ€ç»ˆç‰ˆï¼‰ | Nov 28 |
| `run_bert_experiments.py` | ~350 | æ‰¹é‡å®éªŒï¼ˆ5ç»„ï¼‰ | Nov 28 |
| `predownload_models.py` | ~100 | æ¨¡å‹é¢„ä¸‹è½½å·¥å…· | Nov 28 |
| `run_quick.sh` | ~50 | å¿«é€Ÿå®éªŒè„šæœ¬ | Nov 28 |

## ğŸ¯ é˜¶æ®µæˆæœ

### æ€§èƒ½æå‡
- **BERTåŸºç¡€ç‰ˆ**ï¼š87.91% (Stage2)
- **SciBERT + Focal Loss**ï¼š**89.04%** (+1.13%)
- **æœ€ä½³F1**ï¼š90.57%

### å®éªŒå¯¹æ¯”ï¼ˆ5ç»„å®éªŒï¼‰

| å®éªŒ | æ¨¡å‹ | æŸå¤±å‡½æ•° | å‡†ç¡®ç‡ | F1 | ç‰¹ç‚¹ |
|------|------|----------|--------|-----|------|
| Exp1 | bert-base | CE | 86.68% | 88.22% | åŸºå‡† |
| Exp2 | scibert | Focal Loss | **89.04%** | **90.57%** | ğŸ†æœ€ä½³ |
| Exp3 | roberta | Weighted CE | 88.42% | 90.13% | å¹³è¡¡ |
| Exp4 | deberta-v3 | CE | 87.50% | 89.45% | æ½œåŠ›å¤§ |
| Exp5 | scibert | CE (max_len=128) | 88.11% | 89.78% | é•¿åºåˆ— |

## ğŸ”¬ ä¼˜åŒ–æŠ€æœ¯è¯¦è§£

### 1. é¢„è®­ç»ƒæ¨¡å‹é€‰æ‹©

**BERT-base-uncased**ï¼ˆåŸºå‡†ï¼‰
- 12å±‚Transformer
- 110Må‚æ•°
- é€šç”¨é¢„è®­ç»ƒ

**SciBERT**ï¼ˆæœ€ä½³ï¼‰â­
```python
model_name = "allenai/scibert_scivocab_uncased"
```
- ä¸“é—¨åœ¨ç§‘å­¦æ–‡çŒ®ä¸Šé¢„è®­ç»ƒ
- æ›´é€‚åˆå­¦æœ¯æ ‡é¢˜åˆ†ç±»
- **æ€§èƒ½æå‡1.13%**

**RoBERTa**
```python
model_name = "roberta-base"
```
- åŠ¨æ€masking
- æ›´å¤§batch sizeè®­ç»ƒ
- æ€§èƒ½ç¨³å®š

**DeBERTa-v3**
```python
model_name = "microsoft/deberta-v3-base"
```
- Disentangled attention
- ç†è®ºä¸Šæœ€å¼ºï¼Œä½†éœ€è¦æ›´å¤šè°ƒä¼˜

### 2. Focal Lossï¼ˆå…³é”®æŠ€æœ¯ï¼‰

**é—®é¢˜**ï¼šæ ‡å‡†Cross-Entropyå¯¹æ‰€æœ‰æ ·æœ¬ä¸€è§†åŒä»

**Focal Lossè§£å†³æ–¹æ¡ˆ**ï¼š
```python
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        return focal_loss.mean()
```

**æ•ˆæœ**ï¼š
- æ›´å…³æ³¨å›°éš¾æ ·æœ¬
- å¬å›ç‡æå‡2.14%
- å‡†ç¡®ç‡æå‡1.13%

### 3. å¯¹æŠ—è®­ç»ƒï¼ˆFGMï¼‰

```python
class FGM:
    def __init__(self, model, epsilon=1.0):
        self.model = model
        self.epsilon = epsilon
    
    def attack(self, emb_name='word_embeddings'):
        # åœ¨embeddingä¸Šæ·»åŠ æ‰°åŠ¨
        for name, param in self.model.named_parameters():
            if emb_name in name:
                norm = torch.norm(param.grad)
                perturbation = epsilon * param.grad / norm
                param.data.add_(perturbation)
```

**æ•ˆæœ**ï¼šæé«˜æ¨¡å‹é²æ£’æ€§

### 4. å­¦ä¹ ç‡ä¼˜åŒ–

**å±‚çº§å­¦ä¹ ç‡**ï¼ˆLayer-wise Learning Rateï¼‰ï¼š
```python
optimizer_grouped_parameters = [
    {'params': model.bert.embeddings.parameters(), 'lr': 2e-5},
    {'params': model.bert.encoder.layer[:6].parameters(), 'lr': 2e-5},
    {'params': model.bert.encoder.layer[6:].parameters(), 'lr': 3e-5},
    {'params': model.classifier.parameters(), 'lr': 5e-5}
]
```

**Warmupç­–ç•¥**ï¼š
```python
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=500,
    num_training_steps=total_steps
)
```

### 5. Early Stopping

```python
early_stopping = EarlyStopping(patience=3, mode='max')
for epoch in range(epochs):
    val_score = evaluate(model, val_loader)
    if early_stopping(val_score):
        break
```

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### å¿«é€Ÿæµ‹è¯•ï¼ˆrun_quick.shï¼‰
```bash
./run_quick.sh
# é€‰æ‹©ï¼š
# 1. å®Œæ•´å®éªŒï¼ˆ5è½®ï¼Œ2-3å°æ—¶ï¼‰
# 2. ä¸­ç­‰å®éªŒï¼ˆ3è½®ï¼Œ1-1.5å°æ—¶ï¼‰
# 3. å¿«é€Ÿæµ‹è¯•ï¼ˆ3è½®ï¼Œ30åˆ†é’Ÿï¼‰
```

### å•æ¬¡è®­ç»ƒï¼ˆæœ€ä½³é…ç½®ï¼‰
```bash
python train_bert_optimized_v2.py
```

é…ç½®ï¼š
```python
model_name = 'allenai/scibert_scivocab_uncased'
max_length = 96
loss_type = 'focal'
use_adversarial = True
epochs = 5
batch_size = 32
learning_rate = 2e-5
```

### æ‰¹é‡å®éªŒï¼ˆ5ç»„å¯¹æ¯”ï¼‰
```bash
python run_bert_experiments.py
```

è¾“å‡ºï¼š`models/experiments/comparison_report.txt`

### é¢„ä¸‹è½½æ¨¡å‹
```bash
python predownload_models.py
```

## ğŸ“Š è¯¦ç»†æ€§èƒ½åˆ†æ

### SciBERT + Focal Lossï¼ˆæœ€ä½³é…ç½®ï¼‰

| ç±»åˆ« | ç²¾ç¡®ç‡ | å¬å›ç‡ | F1 | æ”¯æŒæ•° |
|------|--------|--------|-----|--------|
| é”™è¯¯æ ‡é¢˜(0) | 87.53% | 87.93% | 87.73% | 464 |
| æ­£ç¡®æ ‡é¢˜(1) | 90.58% | 90.24% | 90.41% | 512 |
| **å®å¹³å‡** | 89.06% | 89.09% | 89.07% | 976 |
| **åŠ æƒå¹³å‡** | **89.14%** | **89.15%** | **89.14%** | 976 |

### æ··æ·†çŸ©é˜µåˆ†æ
```
çœŸå®\é¢„æµ‹    0     1
    0      408    56    (88%æ­£ç¡®)
    1       50   462    (90%æ­£ç¡®)
```

- **å‡é˜³æ€§ï¼ˆFPï¼‰**ï¼š56ä¸ªï¼ˆå°†é”™è¯¯æ ‡é¢˜è¯¯åˆ¤ä¸ºæ­£ç¡®ï¼‰
- **å‡é˜´æ€§ï¼ˆFNï¼‰**ï¼š50ä¸ªï¼ˆå°†æ­£ç¡®æ ‡é¢˜è¯¯åˆ¤ä¸ºé”™è¯¯ï¼‰

## ğŸ’¡ ä¼˜åŒ–ç»éªŒæ€»ç»“

### æœ‰æ•ˆä¼˜åŒ–ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰
1. â­â­â­ **SciBERTæ¨¡å‹**ï¼šé¢†åŸŸé¢„è®­ç»ƒæ¨¡å‹æ•ˆæœæ˜¾è‘—
2. â­â­â­ **Focal Loss**ï¼šè§£å†³å›°éš¾æ ·æœ¬é—®é¢˜
3. â­â­ **Warmup + å±‚çº§å­¦ä¹ ç‡**ï¼šè®­ç»ƒæ›´ç¨³å®š
4. â­â­ **å¯¹æŠ—è®­ç»ƒFGM**ï¼šæé«˜é²æ£’æ€§
5. â­ **Early Stopping**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ

### æ•ˆæœä¸æ˜æ˜¾
- âŒ å¢åŠ max_lengthåˆ°512ï¼ˆè®¡ç®—é‡å¤§ï¼Œæå‡å°ï¼‰
- âŒ PGDå¯¹æŠ—è®­ç»ƒï¼ˆæ¯”FGMæ…¢ï¼Œæå‡ä¸å¤§ï¼‰
- âŒ æ•°æ®å¢å¼ºï¼ˆå›è¯‘ç­‰ï¼Œæ•ˆæœä¸€èˆ¬ï¼‰

### æ€§ä»·æ¯”åˆ†æ

| æŠ€æœ¯ | æ€§èƒ½æå‡ | è®¡ç®—æˆæœ¬ | å®ç°éš¾åº¦ | æ¨èåº¦ |
|------|---------|---------|----------|--------|
| SciBERT | +1.13% | 0% | â­ | â­â­â­â­â­ |
| Focal Loss | +0.8% | 0% | â­â­ | â­â­â­â­â­ |
| FGMå¯¹æŠ— | +0.3% | +20% | â­â­â­ | â­â­â­â­ |
| å±‚çº§å­¦ä¹ ç‡ | +0.2% | 0% | â­â­ | â­â­â­ |

## ğŸ”— å‚è€ƒæ–‡æ¡£

- **BERT_OPTIMIZATION_README.md** - å®Œæ•´ä¼˜åŒ–æŒ‡å—
- **QUICK_START.md** - å¿«é€Ÿä¸Šæ‰‹
- **models/experiments/comparison_report.txt** - è¯¦ç»†å®éªŒæŠ¥å‘Š

## ğŸ“ˆ ä»£ç ç»Ÿè®¡

- **æ€»è¡Œæ•°**ï¼š~2,800è¡Œ
- **æ–‡ä»¶æ•°**ï¼š7ä¸ª
- **å®éªŒç»„æ•°**ï¼š5ç»„
- **è®­ç»ƒæ—¶é•¿**ï¼š8-12å°æ—¶ï¼ˆå…¨éƒ¨å®éªŒï¼‰

---

**æ€»ç»“**ï¼šé€šè¿‡ç³»ç»Ÿçš„BERTä¼˜åŒ–ï¼Œæ€§èƒ½ä»87.91%æå‡è‡³89.04%ï¼ŒSciBERTå’ŒFocal Lossæ˜¯å…³é”®ã€‚
