"""
run_bert_experiments.py
=======================
æ‰¹é‡è¿è¡ŒBERTä¼˜åŒ–å®éªŒ,å¯¹æ¯”ä¸åŒæ¨¡å‹å’Œé…ç½®

å¿«é€Ÿå¯¹æ¯”:
1. SciBERT vs RoBERTa vs DeBERTa vs BERT-base
2. Focal Loss vs Weighted CE vs Standard CE
3. ä¸åŒmax_length: 64 vs 96 vs 128
4. å¯¹æŠ—è®­ç»ƒ vs æ— å¯¹æŠ—è®­ç»ƒ
"""

import os
import sys
import json
import pandas as pd
from typing import Dict, List
from data_loader import DataLoader as TitleDataLoader
from train_bert_optimized_v2 import OptimizedBERTClassifier
from sklearn.metrics import classification_report, f1_score, recall_score, precision_score, accuracy_score
import numpy as np


def run_single_experiment(
    experiment_name: str,
    train_titles: List[str],
    train_labels: List[int],
    test_titles: List[str],
    test_labels: List[int],
    config: Dict
) -> Dict:
    """è¿è¡Œå•ä¸ªå®éªŒ"""

    print("\n" + "="*100)
    print(f" ğŸ”¬ å®éªŒ: {experiment_name}")
    print("="*100)
    print(f"é…ç½®: {json.dumps(config, indent=2, ensure_ascii=False)}")

    # è·å–è„šæœ¬ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))

    # åˆ›å»ºåˆ†ç±»å™¨
    classifier = OptimizedBERTClassifier(
        model_name=config.get('model_name', 'scibert'),
        max_length=config.get('max_length', 96),
        model_path=os.path.join(script_dir, f"models/experiments/{experiment_name}.pt"),
        dropout_rate=config.get('dropout_rate', 0.2)
    )

    # è®­ç»ƒ
    try:
        history = classifier.train(
            train_titles,
            train_labels,
            val_ratio=config.get('val_ratio', 0.1),
            epochs=config.get('epochs', 10),
            batch_size=config.get('batch_size', 32),
            learning_rate=config.get('learning_rate', 2e-5),
            warmup_ratio=config.get('warmup_ratio', 0.1),
            scheduler_type=config.get('scheduler_type', 'cosine'),
            loss_type=config.get('loss_type', 'focal'),
            class_weight_positive=config.get('class_weight_positive', 1.3),
            focal_alpha=config.get('focal_alpha', 0.25),
            focal_gamma=config.get('focal_gamma', 2.0),
            early_stopping_patience=config.get('early_stopping_patience', 3),
            use_layer_wise_lr=config.get('use_layer_wise_lr', True),
            layer_decay=config.get('layer_decay', 0.95),
            use_adversarial=config.get('use_adversarial', True),
            adv_epsilon=config.get('adv_epsilon', 1.0),
            use_mixed_precision=config.get('use_mixed_precision', True),
            save_model=True
        )
    except Exception as e:
        print(f"âŒ å®éªŒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return {
            'experiment_name': experiment_name,
            'status': 'failed',
            'error': str(e)
        }

    # æµ‹è¯•
    print(f"\næµ‹è¯•é›†è¯„ä¼°...")
    predictions = classifier.predict(test_titles)
    probabilities = classifier.predict_proba(test_titles)

    # è®¡ç®—æŒ‡æ ‡
    results = {
        'experiment_name': experiment_name,
        'status': 'success',
        'config': config,
        'accuracy': accuracy_score(test_labels, predictions),
        'precision': precision_score(test_labels, predictions),
        'recall': recall_score(test_labels, predictions),
        'f1': f1_score(test_labels, predictions),
        'training_history': history
    }

    # æ‰“å°ç»“æœ
    print(f"\n{'='*100}")
    print(f" ğŸ“Š {experiment_name} - ç»“æœ")
    print(f"{'='*100}")
    print(f"Accuracy:  {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)")
    print(f"Precision: {results['precision']:.4f} ({results['precision']*100:.2f}%)")
    print(f"Recall:    {results['recall']:.4f} ({results['recall']*100:.2f}%) â­")
    print(f"F1 Score:  {results['f1']:.4f} ({results['f1']*100:.2f}%)")

    return results


def main():
    """è¿è¡Œæ‰€æœ‰å®éªŒ"""

    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))

    # åŠ è½½æ•°æ®
    print("\n" + "="*100)
    print(" ğŸ“¦ åŠ è½½æ•°æ®")
    print("="*100)
    print(f"è„šæœ¬ç›®å½•: {script_dir}")

    train_titles, train_labels, test_titles, test_labels = TitleDataLoader.prepare_dataset(
        os.path.join(script_dir, 'data/positive.txt'),
        os.path.join(script_dir, 'data/negative.txt'),
        os.path.join(script_dir, 'data/testSet-1000.xlsx')
    )

    if len(train_titles) == 0:
        print("âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶!")
        return

    print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ")
    print(f"  è®­ç»ƒé›†: {len(train_titles)} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(test_titles)} æ ·æœ¬")

    # å®šä¹‰å®éªŒ
    experiments = [
        # å®éªŒ1: Baseline (BERT-base)
        {
            'name': 'exp1_bert_base_baseline',
            'config': {
                'model_name': 'bert-base',
                'max_length': 64,
                'epochs': 8,
                'batch_size': 32,
                'learning_rate': 2e-5,
                'loss_type': 'ce',  # æ ‡å‡†äº¤å‰ç†µ
                'use_layer_wise_lr': False,
                'use_adversarial': False,
                'use_mixed_precision': True
            }
        },

        # å®éªŒ2: SciBERT (å­¦æœ¯ä¸“ç”¨)
        {
            'name': 'exp2_scibert_focal',
            'config': {
                'model_name': 'scibert',
                'max_length': 96,
                'epochs': 10,
                'batch_size': 32,
                'learning_rate': 2e-5,
                'loss_type': 'focal',  # Focal Loss
                'focal_alpha': 0.25,
                'focal_gamma': 2.0,
                'use_layer_wise_lr': True,
                'layer_decay': 0.95,
                'use_adversarial': True,
                'use_mixed_precision': True
            }
        },

        # å®éªŒ3: RoBERTa
        {
            'name': 'exp3_roberta_weighted',
            'config': {
                'model_name': 'roberta',
                'max_length': 96,
                'epochs': 10,
                'batch_size': 32,
                'learning_rate': 2e-5,
                'loss_type': 'weighted_ce',  # åŠ æƒäº¤å‰ç†µ
                'class_weight_positive': 1.3,
                'use_layer_wise_lr': True,
                'use_adversarial': True,
                'use_mixed_precision': True
            }
        },

        # å®éªŒ4: SciBERT + max_length=128
        {
            'name': 'exp5_scibert_maxlen128',
            'config': {
                'model_name': 'scibert',
                'max_length': 128,  # æ›´é•¿åºåˆ—
                'epochs': 10,
                'batch_size': 24,  # å‡å°batch sizeé€‚åº”æ›´é•¿åºåˆ—
                'learning_rate': 2e-5,
                'loss_type': 'focal',
                'use_layer_wise_lr': True,
                'use_adversarial': True,
                'use_mixed_precision': True
            }
        },
    ]

    # è¿è¡Œå®éªŒ
    all_results = []
    os.makedirs(os.path.join(script_dir, 'models/experiments'), exist_ok=True)

    for exp in experiments:
        result = run_single_experiment(
            exp['name'],
            train_titles,
            train_labels,
            test_titles,
            test_labels,
            exp['config']
        )
        all_results.append(result)

        # ä¿å­˜ä¸­é—´ç»“æœ
        with open(os.path.join(script_dir, 'models/experiments/results.json'), 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False, default=str)

    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print("\n\n" + "="*100)
    print(" ğŸ† å®éªŒå¯¹æ¯”æŠ¥å‘Š")
    print("="*100)

    # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
    comparison_data = []
    for result in all_results:
        if result['status'] == 'success':
            comparison_data.append({
                'å®éªŒåç§°': result['experiment_name'],
                'æ¨¡å‹': result['config']['model_name'],
                'Max Length': result['config']['max_length'],
                'Loss Type': result['config']['loss_type'],
                'Layer-wise LR': 'âœ“' if result['config'].get('use_layer_wise_lr') else 'âœ—',
                'å¯¹æŠ—è®­ç»ƒ': 'âœ“' if result['config'].get('use_adversarial') else 'âœ—',
                'Accuracy': f"{result['accuracy']:.4f}",
                'Precision': f"{result['precision']:.4f}",
                'Recall': f"{result['recall']:.4f}",
                'F1': f"{result['f1']:.4f}"
            })

    df = pd.DataFrame(comparison_data)
    print("\n")
    print(df.to_string(index=False))

    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    successful_results = [r for r in all_results if r['status'] == 'success']
    if successful_results:
        best_f1 = max(successful_results, key=lambda x: x['f1'])
        best_recall = max(successful_results, key=lambda x: x['recall'])

        print(f"\n" + "="*100)
        print(f" ğŸ… æœ€ä½³æ¨¡å‹")
        print(f"="*100)
        print(f"\næœ€é«˜F1åˆ†æ•°: {best_f1['experiment_name']}")
        print(f"  - F1: {best_f1['f1']:.4f} ({best_f1['f1']*100:.2f}%)")
        print(f"  - Accuracy: {best_f1['accuracy']:.4f} ({best_f1['accuracy']*100:.2f}%)")
        print(f"  - Recall: {best_f1['recall']:.4f} ({best_f1['recall']*100:.2f}%)")

        print(f"\næœ€é«˜å¬å›ç‡: {best_recall['experiment_name']}")
        print(f"  - Recall: {best_recall['recall']:.4f} ({best_recall['recall']*100:.2f}%)")
        print(f"  - F1: {best_recall['f1']:.4f} ({best_recall['f1']*100:.2f}%)")
        print(f"  - Accuracy: {best_recall['accuracy']:.4f} ({best_recall['accuracy']*100:.2f}%)")

        # å¯¹æ¯”åŸå§‹baseline
        baseline_acc = 0.8525
        baseline_recall = 0.8116
        baseline_f1 = 0.8649

        print(f"\n" + "="*100)
        print(f" ğŸ“ˆ ç›¸æ¯”åŸå§‹BERTçš„æå‡")
        print(f"="*100)
        print(f"\nåŸå§‹BERT (bert-base-uncased, max_length=64):")
        print(f"  Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)")
        print(f"  Recall:   {baseline_recall:.4f} ({baseline_recall*100:.2f}%)")
        print(f"  F1:       {baseline_f1:.4f} ({baseline_f1*100:.2f}%)")

        print(f"\næœ€ä½³ä¼˜åŒ–æ¨¡å‹ ({best_f1['experiment_name']}):")
        print(f"  Accuracy: {best_f1['accuracy']:.4f} ({best_f1['accuracy']*100:.2f}%) "
              f"[{'+' if best_f1['accuracy'] > baseline_acc else ''}{(best_f1['accuracy']-baseline_acc)*100:.2f}%]")
        print(f"  Recall:   {best_f1['recall']:.4f} ({best_f1['recall']*100:.2f}%) "
              f"[{'+' if best_f1['recall'] > baseline_recall else ''}{(best_f1['recall']-baseline_recall)*100:.2f}%]")
        print(f"  F1:       {best_f1['f1']:.4f} ({best_f1['f1']*100:.2f}%) "
              f"[{'+' if best_f1['f1'] > baseline_f1 else ''}{(best_f1['f1']-baseline_f1)*100:.2f}%]")

    # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
    report_path = os.path.join(script_dir, 'models/experiments/comparison_report.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("="*100 + "\n")
        f.write(" BERTä¼˜åŒ–å®éªŒå¯¹æ¯”æŠ¥å‘Š\n")
        f.write("="*100 + "\n\n")
        f.write(df.to_string(index=False))
        f.write("\n\n")
        f.write("="*100 + "\n")
        f.write(f" æœ€ä½³æ¨¡å‹: {best_f1['experiment_name']}\n")
        f.write("="*100 + "\n")
        f.write(f"F1: {best_f1['f1']:.4f}\n")
        f.write(f"Accuracy: {best_f1['accuracy']:.4f}\n")
        f.write(f"Recall: {best_f1['recall']:.4f}\n")
        f.write(f"Precision: {best_f1['precision']:.4f}\n")

    print(f"\nâœ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
    print(f"âœ“ å®Œæ•´ç»“æœå·²ä¿å­˜è‡³: {os.path.join(script_dir, 'models/experiments/results.json')}")


if __name__ == "__main__":
    main()
