#!/usr/bin/env python3
"""Stage2 Traditional Models - è®­ç»ƒè„šæœ¬

æœ¬é˜¶æ®µå®ç°ä¸‰ç§åŸºç¡€æ¨¡å‹:
- æœ´ç´ è´å¶æ–¯ (73.46%)
- Word2Vec + SVM (82.99%)
- BERT (87.91%)

ç”¨æ³•:
    python train.py                      # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
    python train.py --model nb           # ä»…æœ´ç´ è´å¶æ–¯
    python train.py --model w2v          # ä»…Word2Vec+SVM
    python train.py --model bert         # ä»…BERT
    python train.py --quick              # å¿«é€Ÿæµ‹è¯•
"""

import os
import sys
import argparse
import warnings
import time
from datetime import datetime

warnings.filterwarnings('ignore')

# é¦–å…ˆå¯¼å…¥Stage2è‡ªå·±çš„configï¼ˆå¿…é¡»åœ¨å¯¼å…¥Stage1ä¹‹å‰ï¼‰
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)  # Stage2ç›®å½•ä¼˜å…ˆ
import config as stage2_config

# ç„¶åæ·»åŠ Stage1åˆ°è·¯å¾„ä»¥è®¿é—®åŸºç¡€è®¾æ–½
sys.path.insert(0, os.path.join(current_dir, '..', 'Stage1_Foundation'))

from data_loader import DataLoader
from evaluator import ModelEvaluator
from visualizer import ResultVisualizer
from naive_bayes_classifier import NaiveBayesClassifier
from word2vec_svm_classifier import Word2VecSVMClassifier
from bert_classifier import BERTClassifier

# ä½¿ç”¨Stage2çš„é…ç½®å‡½æ•°
get_data_path = stage2_config.get_data_path
get_model_path = stage2_config.get_model_path
get_output_path = stage2_config.get_output_path


def print_banner():
    print("\n" + "="*80)
    print(" " * 25 + "Stage2 Traditional Models - è®­ç»ƒ")
    print("="*80)
    print(f"è®­ç»ƒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80 + "\n")


def load_data(max_samples=None):
    print("[1] åŠ è½½æ•°æ®")
    print("-" * 80)

    train_titles, train_labels, test_titles, test_labels = DataLoader.prepare_dataset(
        get_data_path('positive.txt'),
        get_data_path('negative.txt'),
        get_data_path('testSet-1000.xlsx')
    )

    if max_samples and max_samples < len(train_titles):
        import random
        indices = list(range(len(train_titles)))
        random.shuffle(indices)
        indices = indices[:max_samples]
        train_titles = [train_titles[i] for i in indices]
        train_labels = [train_labels[i] for i in indices]
        print(f"âš  å¿«é€Ÿæµ‹è¯•æ¨¡å¼: {len(train_titles)} æ ·æœ¬")
    else:
        print(f"âœ“ è®­ç»ƒæ•°æ®: {len(train_titles)} æ ·æœ¬")

    print(f"âœ“ æµ‹è¯•æ•°æ®: {len(test_titles)} æ ·æœ¬\n")
    return train_titles, train_labels, test_titles, test_labels


def train_naive_bayes(train_titles, train_labels, test_titles, test_labels):
    print("[2] è®­ç»ƒæœ´ç´ è´å¶æ–¯ (V1 - åŸºç¡€ç‰ˆ)")
    print("-" * 80)

    start_time = time.time()

    classifier = NaiveBayesClassifier(
        max_features=5000,
        ngram_range=(1, 2),
        model_path=get_model_path('naive_bayes_model.pkl')
    )
    classifier.train(train_titles, train_labels)

    train_time = time.time() - start_time
    print(f"\nâœ“ è®­ç»ƒå®Œæˆ: {train_time:.2f}ç§’")

    predictions = classifier.predict(test_titles)
    evaluator = ModelEvaluator()
    result = evaluator.evaluate_model(test_labels, predictions, "Naive Bayes V1", verbose=False)

    print(f"\nğŸ“Š æ€§èƒ½ (é¢„æœŸ ~73%)")
    print(f"  å‡†ç¡®ç‡: {result['accuracy']:.4f}")
    print(f"  F1åˆ†æ•°: {result['f1']:.4f}\n")

    result['training_time'] = train_time
    return result


def train_word2vec_svm(train_titles, train_labels, test_titles, test_labels):
    print("[3] è®­ç»ƒ Word2Vec + SVM")
    print("-" * 80)

    start_time = time.time()

    classifier = Word2VecSVMClassifier(
        vector_size=100,
        window=5,
        model_path=get_model_path('word2vec_svm_model')
    )
    classifier.train(train_titles, train_labels)

    train_time = time.time() - start_time
    print(f"\nâœ“ è®­ç»ƒå®Œæˆ: {train_time:.2f}ç§’ ({train_time/60:.1f}åˆ†é’Ÿ)")

    predictions = classifier.predict(test_titles)
    evaluator = ModelEvaluator()
    result = evaluator.evaluate_model(test_labels, predictions, "Word2Vec + SVM", verbose=False)

    print(f"\nğŸ“Š æ€§èƒ½ (é¢„æœŸ ~83%)")
    print(f"  å‡†ç¡®ç‡: {result['accuracy']:.4f}")
    print(f"  F1åˆ†æ•°: {result['f1']:.4f}\n")

    result['training_time'] = train_time
    return result


def train_bert(train_titles, train_labels, test_titles, test_labels, epochs=3, batch_size=16):
    print("[4] è®­ç»ƒ BERT (åŸºç¡€ç‰ˆ)")
    print("-" * 80)
    print(f"é…ç½®: epochs={epochs}, batch_size={batch_size}\n")

    start_time = time.time()

    classifier = BERTClassifier(
        model_name='bert-base-uncased',
        max_length=64,
        model_path=get_model_path('best_bert_model.pt')
    )
    classifier.train(train_titles, train_labels, epochs=epochs, batch_size=batch_size)

    train_time = time.time() - start_time
    print(f"\nâœ“ è®­ç»ƒå®Œæˆ: {train_time:.2f}ç§’ ({train_time/60:.1f}åˆ†é’Ÿ)")

    predictions = classifier.predict(test_titles, batch_size=batch_size)
    evaluator = ModelEvaluator()
    result = evaluator.evaluate_model(test_labels, predictions, "BERT", verbose=False)

    print(f"\nğŸ“Š æ€§èƒ½ (é¢„æœŸ ~88%)")
    print(f"  å‡†ç¡®ç‡: {result['accuracy']:.4f}")
    print(f"  F1åˆ†æ•°: {result['f1']:.4f}\n")

    result['training_time'] = train_time
    return result


def main():
    parser = argparse.ArgumentParser(description='Stage2 Traditional Models è®­ç»ƒè„šæœ¬')
    parser.add_argument('--model', type=str, choices=['nb', 'w2v', 'bert', 'all'], default='all')
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿæµ‹è¯•æ¨¡å¼')
    parser.add_argument('--max-samples', type=int, default=None)
    parser.add_argument('--epochs', type=int, default=3, help='BERTè®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=16)
    args = parser.parse_args()

    if args.quick:
        args.max_samples = args.max_samples or 5000
        args.epochs = 1
        print("âš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼")

    print_banner()

    train_titles, train_labels, test_titles, test_labels = load_data(args.max_samples)

    all_results = []
    total_start = time.time()

    if args.model in ['nb', 'all']:
        result = train_naive_bayes(train_titles, train_labels, test_titles, test_labels)
        all_results.append(result)

    if args.model in ['w2v', 'all']:
        result = train_word2vec_svm(train_titles, train_labels, test_titles, test_labels)
        all_results.append(result)

    if args.model in ['bert', 'all']:
        result = train_bert(train_titles, train_labels, test_titles, test_labels,
                           epochs=args.epochs, batch_size=args.batch_size)
        all_results.append(result)

    total_time = time.time() - total_start

    if len(all_results) > 1:
        print("\n" + "="*80)
        print(" " * 30 + "æ¨¡å‹å¯¹æ¯”")
        print("="*80 + "\n")
        ModelEvaluator.compare_models(all_results)

        visualizer = ResultVisualizer()
        visualizer.plot_comparison(all_results, save_path=get_output_path('comparison.png'))
        visualizer.plot_confusion_matrices(all_results, save_path=get_output_path('confusion_matrices.png'))

    # ç”Ÿæˆ t-SNE å¯è§†åŒ–
    print("\n" + "="*80)
    print(" " * 28 + "ç”Ÿæˆ t-SNE å¯è§†åŒ–")
    print("="*80 + "\n")

    classifiers_dict = {}
    if args.model in ['nb', 'all']:
        nb_classifier = NaiveBayesClassifier(
            max_features=5000,
            ngram_range=(1, 2),
            model_path=get_model_path('naive_bayes_model.pkl')
        )
        nb_classifier.load_model()
        classifiers_dict['Naive_Bayes'] = nb_classifier

    if args.model in ['w2v', 'all']:
        w2v_classifier = Word2VecSVMClassifier(
            vector_size=100,
            window=5,
            model_path=get_model_path('word2vec_svm_model')
        )
        w2v_classifier.load_model()
        classifiers_dict['Word2Vec_SVM'] = w2v_classifier

    if args.model in ['bert', 'all']:
        bert_classifier = BERTClassifier(
            model_name='bert-base-uncased',
            max_length=64,
            model_path=get_model_path('best_bert_model.pt')
        )
        bert_classifier.load_model()
        classifiers_dict['BERT'] = bert_classifier

    # ä¸ºæ¯ä¸ªæ¨¡å‹ç”Ÿæˆ t-SNE å›¾
    visualizer = ResultVisualizer()
    for model_name, classifier in classifiers_dict.items():
        try:
            feature_vectors = classifier.get_feature_vectors(test_titles)
            visualizer.visualize_embeddings_tsne(
                feature_vectors,
                test_labels,
                model_name,
                save_path=get_output_path(f'tsne_{model_name}.png')
            )
        except Exception as e:
            print(f"âš ï¸  ä¸º {model_name} ç”Ÿæˆ t-SNE å›¾å¤±è´¥: {str(e)}")

    print("\n" + "="*80)
    print(" " * 30 + "è®­ç»ƒå®Œæˆ")
    print("="*80)
    print(f"\næ€»è€—æ—¶: {total_time:.2f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
    print(f"æ¨¡å‹ä¿å­˜: {get_model_path('')}")
    print(f"ç»“æœä¿å­˜: {get_output_path('')}")
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    if len(all_results) > 1:
        print(f"  - comparison.png (æ¨¡å‹å¯¹æ¯”)")
        print(f"  - confusion_matrices.png (æ··æ·†çŸ©é˜µ)")
    for model_name in classifiers_dict.keys():
        print(f"  - tsne_{model_name}.png (t-SNEå¯è§†åŒ–)")
    print("\n" + "="*80 + "\n")


if __name__ == "__main__":
    main()
