# Stage2_Traditional_Models å®ç°æ–‡æ¡£

## ğŸ“‹ é˜¶æ®µæ¦‚è¿°

**é˜¶æ®µåç§°**: Stage2 - ä¼ ç»Ÿæ¨¡å‹å®ç°
**å®ç°æ—¶é—´**: 2024å¹´11æœˆ15æ—¥
**ä¸»è¦ç›®æ ‡**: å®ç°ä¸‰ç§åŸºç¡€åˆ†ç±»æ–¹æ³•(æœ´ç´ è´å¶æ–¯ã€Word2Vec+SVMã€BERT)
**ä»£ç è¡Œæ•°**: ~1,400è¡Œ(3ä¸ªæ–‡ä»¶)

## ğŸ¯ å®ç°ç›®æ ‡

- âœ… æœ´ç´ è´å¶æ–¯V1: 73.46%å‡†ç¡®ç‡
- âœ… Word2Vec+SVM: 82.99%å‡†ç¡®ç‡
- âœ… BERTåŸºç¡€ç‰ˆ: 87.91%å‡†ç¡®ç‡
- âœ… å»ºç«‹ä¸‰ç§ä¸åŒæŠ€æœ¯è·¯çº¿çš„baseline

## ğŸ“ æ–‡ä»¶ç»“æ„

```
Stage2_Traditional_Models/
â”œâ”€â”€ naive_bayes_classifier.py      # æœ´ç´ è´å¶æ–¯ (~273è¡Œ)
â”œâ”€â”€ word2vec_svm_classifier.py     # Word2Vec+SVM (~450è¡Œ)
â”œâ”€â”€ bert_classifier.py             # BERTåŸºç¡€ç‰ˆ (~348è¡Œ)
â”œâ”€â”€ config.py                      # é…ç½®æ–‡ä»¶(è¾“å‡ºè·¯å¾„)
â”œâ”€â”€ output/                        # æœ¬é˜¶æ®µè¾“å‡ºç›®å½• â­
â”‚   â”œâ”€â”€ naive_bayes_evaluation.txt
â”‚   â”œâ”€â”€ word2vec_svm_evaluation.txt
â”‚   â”œâ”€â”€ bert_evaluation.txt
â”‚   â””â”€â”€ [å¯è§†åŒ–æ–‡ä»¶]
â”œâ”€â”€ models/                        # æœ¬é˜¶æ®µæ¨¡å‹ç›®å½• â­
â”‚   â”œâ”€â”€ naive_bayes_model.pkl
â”‚   â”œâ”€â”€ word2vec_svm_model_w2v.model
â”‚   â”œâ”€â”€ word2vec_svm_model_svm.pkl
â”‚   â””â”€â”€ best_bert_model.pt
â””â”€â”€ README.md
```

## ğŸ”§ æ ¸å¿ƒå®ç°

### 1. æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨

**æŠ€æœ¯æ ˆ**: TF-IDF + MultinomialNB

**å…³é”®ä»£ç **:
```python
from config import get_model_path
from naive_bayes_classifier import NaiveBayesClassifier

# åˆå§‹åŒ–(ä½¿ç”¨configè·å–è·¯å¾„)
classifier = NaiveBayesClassifier(
    max_features=5000,
    ngram_range=(1, 2),
    model_path=get_model_path('naive_bayes_model.pkl')  # â­æ­£ç¡®è·¯å¾„
)

# è®­ç»ƒ
classifier.train(train_titles, train_labels)

# é¢„æµ‹
predictions = classifier.predict(test_titles)
```

**æ€§èƒ½**: 73.46%å‡†ç¡®ç‡,F1 78.82%

### 2. Word2Vec + SVMåˆ†ç±»å™¨

**æŠ€æœ¯æ ˆ**: Gensim Word2Vec + LinearSVC

**å…³é”®ä»£ç **:
```python
from config import get_model_path
from word2vec_svm_classifier import Word2VecSVMClassifier

# åˆå§‹åŒ–
classifier = Word2VecSVMClassifier(
    vector_size=100,
    window=5,
    model_path=get_model_path('word2vec_svm_model')  # ä¸å«æ‰©å±•å
)

# è®­ç»ƒ
classifier.train(train_titles, train_labels)
```

**æ€§èƒ½**: 82.99%å‡†ç¡®ç‡,F1 85.74%

### 3. BERTåˆ†ç±»å™¨

**æŠ€æœ¯æ ˆ**: bert-base-uncased + PyTorch

**å…³é”®ä»£ç **:
```python
from config import get_model_path
from bert_classifier import BERTClassifier

# åˆå§‹åŒ–
classifier = BERTClassifier(
    model_name='bert-base-uncased',
    max_length=64,
    model_path=get_model_path('best_bert_model.pt')  # â­æ­£ç¡®è·¯å¾„
)

# è®­ç»ƒ
classifier.train(
    train_titles,
    train_labels,
    epochs=3,
    batch_size=16
)
```

**æ€§èƒ½**: 87.91%å‡†ç¡®ç‡,F1 89.59%

## ğŸ“‚ è¾“å‡ºä½ç½®è¯´æ˜

### è¾“å‡ºç›®å½•ç»“æ„

```
Stage2_Traditional_Models/
â”œâ”€â”€ output/                    # â­æ‰€æœ‰è¾“å‡ºä¿å­˜åœ¨æ­¤
â”‚   â”œâ”€â”€ [è¯„ä¼°ç»“æœ.txt]
â”‚   â””â”€â”€ [å¯è§†åŒ–å›¾è¡¨.png]
â”‚
â”œâ”€â”€ models/                    # â­æ‰€æœ‰æ¨¡å‹ä¿å­˜åœ¨æ­¤
â”‚   â”œâ”€â”€ naive_bayes_model.pkl       # ~11 MB
â”‚   â”œâ”€â”€ word2vec_svm_model_w2v.model  # ~25 MB
â”‚   â”œâ”€â”€ word2vec_svm_model_svm.pkl    # ~114 MB
â”‚   â””â”€â”€ best_bert_model.pt             # ~438 MB
```

### å¦‚ä½•ä½¿ç”¨config.py

**åœ¨ä»£ç ä¸­å¯¼å…¥config**:
```python
from config import get_output_path, get_model_path, get_data_path

# è·å–æ¨¡å‹ä¿å­˜è·¯å¾„
model_path = get_model_path('my_model.pkl')
# â†’ .../Stage2_Traditional_Models/models/my_model.pkl

# è·å–è¾“å‡ºæ–‡ä»¶è·¯å¾„
output_file = get_output_path('evaluation.txt')
# â†’ .../Stage2_Traditional_Models/output/evaluation.txt
```

### æ£€æŸ¥è¾“å‡º

```bash
# æŸ¥çœ‹æ¨¡å‹æ–‡ä»¶
ls -lh /home/u2023312337/task2/task2/stages/Stage2_Traditional_Models/models/

# æŸ¥çœ‹è¾“å‡ºæ–‡ä»¶
ls -lh /home/u2023312337/task2/task2/stages/Stage2_Traditional_Models/output/
```

## ğŸš€ è¿è¡Œç¤ºä¾‹

### è®­ç»ƒå•ä¸ªæ¨¡å‹

```bash
cd /home/u2023312337/task2/task2/stages/Stage2_Traditional_Models

# è®­ç»ƒæœ´ç´ è´å¶æ–¯
python naive_bayes_classifier.py

# è®­ç»ƒWord2Vec+SVM
python word2vec_svm_classifier.py

# è®­ç»ƒBERT
python bert_classifier.py
```

### ä»Stage1å¯¼å…¥å·¥å…·

```python
# éœ€è¦è®¿é—®Stage1çš„å·¥å…·æ¨¡å—
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'Stage1_Foundation'))

from data_loader import DataLoader
from evaluator import ModelEvaluator
from visualizer import ResultVisualizer
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | å‡†ç¡®ç‡ | ç²¾ç¡®ç‡ | å¬å›ç‡ | F1 | è®­ç»ƒæ—¶é—´ | æ¨¡å‹å¤§å° |
|------|--------|--------|--------|-----|----------|----------|
| æœ´ç´ è´å¶æ–¯V1 | 73.46% | 73.59% | 84.86% | 78.82% | ~2åˆ†é’Ÿ | 11 MB |
| Word2Vec+SVM | 82.99% | 85.84% | 85.58% | 85.74% | ~10åˆ†é’Ÿ | 139 MB |
| BERTåŸºç¡€ç‰ˆ | 87.91% | 90.29% | 88.35% | 89.59% | ~1å°æ—¶ | 438 MB |

## ğŸ”— ä¸å…¶ä»–é˜¶æ®µçš„å…³ç³»

- **ä¾èµ–**: Stage1_Foundation(æ•°æ®åŠ è½½ã€è¯„ä¼°ã€å¯è§†åŒ–)
- **è¢«ä¾èµ–**: Stage3(ä¼˜åŒ–æœ´ç´ è´å¶æ–¯)ã€Stage4(ä¼˜åŒ–BERT)
- **å¯¹æ¯”**: ä¸ºåç»­ä¼˜åŒ–æä¾›baseline

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **è·¯å¾„é…ç½®**: æ‰€æœ‰æ¨¡å‹ä¿å­˜è·¯å¾„éƒ½åº”ä½¿ç”¨`config.get_model_path()`
2. **ä¾èµ–Stage1**: éœ€è¦Stage1çš„data_loader, evaluator, visualizer
3. **GPUæ”¯æŒ**: BERTè®­ç»ƒéœ€è¦GPU,å¦åˆ™ææ…¢
4. **å†…å­˜éœ€æ±‚**: BERTè®­ç»ƒè‡³å°‘éœ€è¦8GBå†…å­˜

## ğŸ“ ä¿®æ”¹è®°å½•

- **2024-11-15**: å®ç°ä¸‰ä¸ªåŸºç¡€åˆ†ç±»å™¨
- **2024-12-05**: æ·»åŠ config.py,å®ç°ç‹¬ç«‹è¾“å‡ºç›®å½•

## ğŸ“š ç›¸å…³æ–‡æ¡£

- **README.md** - é˜¶æ®µæ¦‚è¿°
- **IMPLEMENTATION.md** (æœ¬æ–‡æ¡£) - è¯¦ç»†å®ç°è¯´æ˜
- **../Stage1_Foundation/IMPLEMENTATION.md** - åŸºç¡€è®¾æ–½è¯´æ˜

---

**å®ç°å®Œæˆåº¦**: âœ… 100%
**è¾“å‡ºä½ç½®**: âœ… å·²é…ç½®åˆ°æœ¬é˜¶æ®µç›®å½•
