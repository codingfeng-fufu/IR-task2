#!/usr/bin/env python3
"""è®­ç»ƒè„šæœ¬ - Baseline Simple ç‰ˆæœ¬

ç”¨æ³•:
    # è®­ç»ƒæ‰€æœ‰æ¨¡å‹ï¼ˆé»˜è®¤ï¼‰
    python train.py

    # ä»…è®­ç»ƒç‰¹å®šæ¨¡å‹
    python train.py --model nb           # æœ´ç´ è´å¶æ–¯
    python train.py --model w2v          # Word2Vec+SVM
    python train.py --model bert         # BERT

    # å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼ˆå‡å°‘è®­ç»ƒæ•°æ®ï¼‰
    python train.py --quick --max-samples 5000

    # BERTå¿«é€Ÿæµ‹è¯•ï¼ˆä»…1ä¸ªepochï¼‰
    python train.py --model bert --quick --epochs 1
"""

import os
import sys
import argparse
import warnings
import time
from datetime import datetime

warnings.filterwarnings('ignore')

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from data_loader import load_training_data, load_test_data
from naive_bayes import SimpleNaiveBayes
from word2vec_svm import SimpleWord2VecSVM
from bert_classifier import SimpleBERT
from evaluator import evaluate_model, compare_models
from visualizer import plot_model_comparison, plot_confusion_matrices, plot_tsne
from config import get_data_path, get_model_path, get_output_path


def print_banner():
    """æ‰“å°æ¨ªå¹…."""
    print("\n" + "="*80)
    print(" " * 25 + "Baseline Simple - è®­ç»ƒè„šæœ¬")
    print(" " * 20 + "Academic Title Classification")
    print("="*80)
    print(f"\nè®­ç»ƒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80 + "\n")


def load_data(max_samples=None):
    """åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®.

    Args:
        max_samples: å¦‚æœæŒ‡å®šï¼Œé™åˆ¶è®­ç»ƒæ ·æœ¬æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰

    Returns:
        train_titles, train_labels, test_titles, test_labels
    """
    print("[1] åŠ è½½æ•°æ®")
    print("-" * 80)

    # åŠ è½½è®­ç»ƒæ•°æ®
    train_titles, train_labels = load_training_data(
        get_data_path('positive.txt'),
        get_data_path('negative.txt')
    )

    # å¦‚æœæŒ‡å®šäº†æœ€å¤§æ ·æœ¬æ•°ï¼Œè¿›è¡Œéšæœºé‡‡æ ·
    if max_samples and max_samples < len(train_titles):
        import random
        indices = list(range(len(train_titles)))
        random.shuffle(indices)
        indices = indices[:max_samples]
        train_titles = [train_titles[i] for i in indices]
        train_labels = [train_labels[i] for i in indices]
        print(f"âš ï¸  å¿«é€Ÿæµ‹è¯•æ¨¡å¼: ä½¿ç”¨ {len(train_titles)} ä¸ªè®­ç»ƒæ ·æœ¬")
    else:
        print(f"âœ“ åŠ è½½è®­ç»ƒæ•°æ®: {len(train_titles)} ä¸ªæ ·æœ¬")

    # åŠ è½½æµ‹è¯•æ•°æ®
    test_titles, test_labels = load_test_data(
        get_data_path('testSet-1000.xlsx')
    )
    print(f"âœ“ åŠ è½½æµ‹è¯•æ•°æ®: {len(test_titles)} ä¸ªæ ·æœ¬")
    print()

    return train_titles, train_labels, test_titles, test_labels


def train_naive_bayes(train_titles, train_labels, test_titles, test_labels):
    """è®­ç»ƒæœ´ç´ è´å¶æ–¯æ¨¡å‹.

    Returns:
        classifier, predictions, results, training_time
    """
    print("[2] è®­ç»ƒæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨")
    print("-" * 80)

    start_time = time.time()

    # åˆå§‹åŒ–å’Œè®­ç»ƒ
    classifier = SimpleNaiveBayes()
    classifier.train(train_titles, train_labels)

    training_time = time.time() - start_time
    print(f"\nâœ“ è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f} ç§’")

    # ä¿å­˜æ¨¡å‹
    model_path = get_model_path('naive_bayes.pkl')
    classifier.save_model(model_path)
    print(f"âœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")

    # è¯„ä¼°
    print("\nè¯„ä¼°æ¨¡å‹...")
    predictions = classifier.predict(test_titles)
    results = evaluate_model(test_labels, predictions, "Naive Bayes")

    print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
    print(f"  å‡†ç¡®ç‡: {results['accuracy']:.4f}")
    print(f"  ç²¾ç¡®ç‡: {results['precision']:.4f}")
    print(f"  å¬å›ç‡: {results['recall']:.4f}")
    print(f"  F1åˆ†æ•°: {results['f1']:.4f}")
    print()

    return classifier, predictions, results, training_time


def train_word2vec_svm(train_titles, train_labels, test_titles, test_labels):
    """è®­ç»ƒWord2Vec+SVMæ¨¡å‹.

    Returns:
        classifier, predictions, results, training_time
    """
    print("[3] è®­ç»ƒ Word2Vec + SVM åˆ†ç±»å™¨")
    print("-" * 80)

    start_time = time.time()

    # åˆå§‹åŒ–å’Œè®­ç»ƒ
    classifier = SimpleWord2VecSVM(vector_size=100, window=5)
    classifier.train(train_titles, train_labels)

    training_time = time.time() - start_time
    print(f"\nâœ“ è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f} ç§’ ({training_time/60:.1f} åˆ†é’Ÿ)")

    # ä¿å­˜æ¨¡å‹
    model_path_prefix = get_model_path('word2vec_svm')
    classifier.save_model(model_path_prefix)
    print(f"âœ“ æ¨¡å‹å·²ä¿å­˜: {model_path_prefix}_*")

    # è¯„ä¼°
    print("\nè¯„ä¼°æ¨¡å‹...")
    predictions = classifier.predict(test_titles)
    results = evaluate_model(test_labels, predictions, "Word2Vec + SVM")

    print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
    print(f"  å‡†ç¡®ç‡: {results['accuracy']:.4f}")
    print(f"  ç²¾ç¡®ç‡: {results['precision']:.4f}")
    print(f"  å¬å›ç‡: {results['recall']:.4f}")
    print(f"  F1åˆ†æ•°: {results['f1']:.4f}")
    print()

    return classifier, predictions, results, training_time


def train_bert(train_titles, train_labels, test_titles, test_labels,
               epochs=3, batch_size=16, max_length=64):
    """è®­ç»ƒBERTæ¨¡å‹.

    Args:
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        max_length: æœ€å¤§åºåˆ—é•¿åº¦

    Returns:
        classifier, predictions, results, training_time
    """
    print("[4] è®­ç»ƒ BERT åˆ†ç±»å™¨")
    print("-" * 80)
    print(f"é…ç½®: epochs={epochs}, batch_size={batch_size}, max_length={max_length}")
    print()

    start_time = time.time()

    # åˆå§‹åŒ–å’Œè®­ç»ƒ
    classifier = SimpleBERT(model_name='bert-base-uncased', max_length=max_length)
    classifier.train(train_titles, train_labels, epochs=epochs, batch_size=batch_size)

    training_time = time.time() - start_time
    print(f"\nâœ“ è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f} ç§’ ({training_time/60:.1f} åˆ†é’Ÿ)")

    # ä¿å­˜æ¨¡å‹
    model_path = get_model_path('bert.pt')
    classifier.save_model(model_path)
    print(f"âœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")

    # è¯„ä¼°
    print("\nè¯„ä¼°æ¨¡å‹...")
    predictions = classifier.predict(test_titles, batch_size=batch_size)
    results = evaluate_model(test_labels, predictions, "BERT")

    print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
    print(f"  å‡†ç¡®ç‡: {results['accuracy']:.4f}")
    print(f"  ç²¾ç¡®ç‡: {results['precision']:.4f}")
    print(f"  å¬å›ç‡: {results['recall']:.4f}")
    print(f"  F1åˆ†æ•°: {results['f1']:.4f}")
    print()

    return classifier, predictions, results, training_time


def save_results_summary(all_results, output_dir):
    """ä¿å­˜ç»“æœæ‘˜è¦åˆ°æ–‡æœ¬æ–‡ä»¶.

    Args:
        all_results: åŒ…å«æ‰€æœ‰æ¨¡å‹ç»“æœçš„å­—å…¸åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
    """
    output_file = os.path.join(output_dir, 'training_results.txt')

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write(" " * 25 + "Baseline Simple - è®­ç»ƒç»“æœ\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"è®­ç»ƒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        for result in all_results:
            f.write("-" * 80 + "\n")
            f.write(f"æ¨¡å‹: {result['model_name']}\n")
            f.write("-" * 80 + "\n")
            f.write(f"å‡†ç¡®ç‡: {result['accuracy']:.4f}\n")
            f.write(f"ç²¾ç¡®ç‡: {result['precision']:.4f}\n")
            f.write(f"å¬å›ç‡: {result['recall']:.4f}\n")
            f.write(f"F1åˆ†æ•°: {result['f1']:.4f}\n")
            if 'training_time' in result:
                f.write(f"è®­ç»ƒæ—¶é—´: {result['training_time']:.2f} ç§’\n")
            f.write("\n")

    print(f"âœ“ ç»“æœå·²ä¿å­˜: {output_file}")


def main():
    """ä¸»å‡½æ•°."""
    parser = argparse.ArgumentParser(
        description='Baseline Simple è®­ç»ƒè„šæœ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python train.py                          # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
  python train.py --model nb               # ä»…è®­ç»ƒæœ´ç´ è´å¶æ–¯
  python train.py --model bert --quick     # BERTå¿«é€Ÿæµ‹è¯•
  python train.py --max-samples 10000      # ä½¿ç”¨10Kæ ·æœ¬è®­ç»ƒ
        """
    )

    parser.add_argument(
        '--model',
        type=str,
        choices=['nb', 'w2v', 'bert', 'all'],
        default='all',
        help='é€‰æ‹©è¦è®­ç»ƒçš„æ¨¡å‹ (nb=æœ´ç´ è´å¶æ–¯, w2v=Word2Vec+SVM, bert=BERT, all=å…¨éƒ¨)'
    )

    parser.add_argument(
        '--quick',
        action='store_true',
        help='å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼ˆå‡å°‘è®­ç»ƒæ ·æœ¬å’Œepochsï¼‰'
    )

    parser.add_argument(
        '--max-samples',
        type=int,
        default=None,
        help='æœ€å¤§è®­ç»ƒæ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰'
    )

    parser.add_argument(
        '--epochs',
        type=int,
        default=3,
        help='BERTè®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤3ï¼‰'
    )

    parser.add_argument(
        '--batch-size',
        type=int,
        default=16,
        help='BERTæ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤16ï¼‰'
    )

    args = parser.parse_args()

    # æ‰“å°æ¨ªå¹…
    print_banner()

    # å¿«é€Ÿæµ‹è¯•æ¨¡å¼é…ç½®
    if args.quick:
        if args.max_samples is None:
            args.max_samples = 5000
        if args.epochs == 3:  # å¦‚æœæ²¡æœ‰æ‰‹åŠ¨æŒ‡å®šepochs
            args.epochs = 1
        print("âš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
        print(f"  æœ€å¤§æ ·æœ¬æ•°: {args.max_samples}")
        print(f"  BERT epochs: {args.epochs}")
        print()

    # åŠ è½½æ•°æ®
    train_titles, train_labels, test_titles, test_labels = load_data(
        max_samples=args.max_samples
    )

    # åˆ›å»ºè¾“å‡ºç›®å½•
    from config import OUTPUT_DIR, MODELS_DIR
    output_dir = OUTPUT_DIR
    models_dir = MODELS_DIR

    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = []
    total_start_time = time.time()

    # è®­ç»ƒæ¨¡å‹
    if args.model in ['nb', 'all']:
        _, _, results, train_time = train_naive_bayes(
            train_titles, train_labels, test_titles, test_labels
        )
        results['training_time'] = train_time
        all_results.append(results)

    if args.model in ['w2v', 'all']:
        _, _, results, train_time = train_word2vec_svm(
            train_titles, train_labels, test_titles, test_labels
        )
        results['training_time'] = train_time
        all_results.append(results)

    if args.model in ['bert', 'all']:
        _, _, results, train_time = train_bert(
            train_titles, train_labels, test_titles, test_labels,
            epochs=args.epochs,
            batch_size=args.batch_size
        )
        results['training_time'] = train_time
        all_results.append(results)

    total_time = time.time() - total_start_time

    # å¯¹æ¯”æ‰€æœ‰æ¨¡å‹ï¼ˆå¦‚æœè®­ç»ƒäº†å¤šä¸ªï¼‰
    if len(all_results) > 1:
        print("\n" + "="*80)
        print(" " * 30 + "æ¨¡å‹å¯¹æ¯”")
        print("="*80 + "\n")
        compare_models(all_results)

        # ç”Ÿæˆå¯è§†åŒ–
        print("\nç”Ÿæˆå¯è§†åŒ–...")
        plot_model_comparison(all_results, output_dir)
        plot_confusion_matrices(all_results, output_dir)
        print(f"âœ“ å¯è§†åŒ–å·²ä¿å­˜åˆ°: {output_dir}/")

    # ç”Ÿæˆ t-SNE å¯è§†åŒ–ï¼ˆä¸ºæ¯ä¸ªè®­ç»ƒçš„æ¨¡å‹ï¼‰
    print("\n" + "="*80)
    print(" " * 28 + "ç”Ÿæˆ t-SNE å¯è§†åŒ–")
    print("="*80 + "\n")

    classifiers_dict = {}
    if args.model in ['nb', 'all']:
        nb_classifier = SimpleNaiveBayes()
        nb_classifier.load_model(get_model_path('naive_bayes.pkl'))
        classifiers_dict['Naive_Bayes'] = nb_classifier

    if args.model in ['w2v', 'all']:
        w2v_classifier = SimpleWord2VecSVM(vector_size=100, window=5)
        w2v_classifier.load_model(get_model_path('word2vec_svm'))
        classifiers_dict['Word2Vec_SVM'] = w2v_classifier

    if args.model in ['bert', 'all']:
        bert_classifier = SimpleBERT(model_name='bert-base-uncased', max_length=64)
        bert_classifier.load_model(get_model_path('bert.pt'))
        classifiers_dict['BERT'] = bert_classifier

    # ä¸ºæ¯ä¸ªæ¨¡å‹ç”Ÿæˆ t-SNE å›¾
    for model_name, classifier in classifiers_dict.items():
        try:
            feature_vectors = classifier.get_feature_vectors(test_titles)
            plot_tsne(feature_vectors, test_labels, model_name, output_dir)
        except Exception as e:
            print(f"âš ï¸  ä¸º {model_name} ç”Ÿæˆ t-SNE å›¾å¤±è´¥: {str(e)}")

    # ä¿å­˜ç»“æœæ‘˜è¦
    save_results_summary(all_results, output_dir)

    # æ‰“å°æ€»ç»“
    print("\n" + "="*80)
    print(" " * 30 + "è®­ç»ƒå®Œæˆï¼")
    print("="*80)
    print(f"\næ€»è€—æ—¶: {total_time:.2f} ç§’ ({total_time/60:.1f} åˆ†é’Ÿ)")
    print(f"\næ¨¡å‹ä¿å­˜ä½ç½®: {models_dir}/")
    print(f"ç»“æœä¿å­˜ä½ç½®: {output_dir}/")
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    if len(all_results) > 1:
        print(f"  - {output_dir}/model_comparison.png")
        print(f"  - {output_dir}/confusion_matrices.png")
    for model_name in classifiers_dict.keys():
        print(f"  - {output_dir}/tsne_{model_name}.png")
    print(f"  - {output_dir}/training_results.txt")
    print("\n" + "="*80 + "\n")


if __name__ == "__main__":
    main()
