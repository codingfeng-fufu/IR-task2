#!/usr/bin/env python3
"""Stage3 NaiveBayes Optimization - è®­ç»ƒè„šæœ¬

æœ¬é˜¶æ®µä¼˜åŒ–æœ´ç´ è´å¶æ–¯æ¨¡å‹å¹¶è®­ç»ƒæ‰€æœ‰æ¨¡å‹è¿›è¡ŒéªŒè¯:
- æœ´ç´ è´å¶æ–¯ V1: 73.46%
- æœ´ç´ è´å¶æ–¯ V2 (ä¼˜åŒ–): 79.20% (+5.74%)
- Word2Vec + SVM: 82.99%
- BERT: 87.91%

ç”¨æ³•:
    python train.py                  # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
    python train.py --model nb       # ä»…è®­ç»ƒæœ´ç´ è´å¶æ–¯(V1+V2)
    python train.py --model w2v      # ä»…è®­ç»ƒWord2Vec+SVM
    python train.py --model bert     # ä»…è®­ç»ƒBERT
    python train.py --quick          # å¿«é€Ÿæµ‹è¯•
"""

import os
import sys
import argparse
import time
import warnings
from datetime import datetime

warnings.filterwarnings('ignore')

# é¦–å…ˆå¯¼å…¥Stage3è‡ªå·±çš„configï¼ˆå¿…é¡»åœ¨å¯¼å…¥Stage1ä¹‹å‰ï¼‰
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)  # Stage3ç›®å½•ä¼˜å…ˆ
import config as stage3_config

# ç„¶åæ·»åŠ Stage1åˆ°è·¯å¾„ä»¥è®¿é—®åŸºç¡€è®¾æ–½
sys.path.insert(0, os.path.join(current_dir, '..', 'Stage1_Foundation'))

from data_loader import DataLoader
from evaluator import ModelEvaluator
from visualizer import ResultVisualizer
from naive_bayes_classifier_optimized import NaiveBayesClassifierOptimized

# ä½¿ç”¨Stage3çš„é…ç½®å‡½æ•°
get_data_path = stage3_config.get_data_path
get_model_path = stage3_config.get_model_path
get_output_path = stage3_config.get_output_path

# å¯¼å…¥Stage2çš„æ¨¡å‹ç”¨äºè®­ç»ƒå’Œå¯¹æ¯”
try:
    sys.path.insert(0, os.path.join(current_dir, '..', 'Stage2_Traditional_Models'))
    from naive_bayes_classifier import NaiveBayesClassifier
    from word2vec_svm_classifier import Word2VecSVMClassifier
    from bert_classifier import BERTClassifier
    HAS_STAGE2_MODELS = True
except Exception as e:
    print(f"âš ï¸ æ— æ³•å¯¼å…¥Stage2æ¨¡å‹: {e}")
    HAS_STAGE2_MODELS = False


def print_banner():
    print("\n" + "="*80)
    print(" " * 18 + "Stage3 NaiveBayes Optimization + å…¨æ¨¡å‹éªŒè¯")
    print(" " * 20 + "æœ´ç´ è´å¶æ–¯ä»73.46%æå‡è‡³79.20% (+5.74%)")
    print("="*80 + "\n")


def train_naive_bayes_v1(train_titles, train_labels, test_titles, test_labels):
    """è®­ç»ƒæœ´ç´ è´å¶æ–¯V1ä½œä¸ºå¯¹æ¯”åŸºå‡†."""
    print("[2] è®­ç»ƒæœ´ç´ è´å¶æ–¯ V1 (åŸºç¡€ç‰ˆ)")
    print("-" * 80)

    start_time = time.time()

    classifier = NaiveBayesClassifier(
        max_features=5000,
        ngram_range=(1, 2),
        model_path=get_model_path('naive_bayes_v1_model.pkl')
    )
    classifier.train(train_titles, train_labels)

    train_time = time.time() - start_time
    predictions = classifier.predict(test_titles)

    evaluator = ModelEvaluator()
    result = evaluator.evaluate_model(test_labels, predictions, "Naive Bayes V1", verbose=False)

    print(f"\nâœ“ è®­ç»ƒå®Œæˆ: {train_time:.2f}ç§’")
    print(f"ğŸ“Š æ€§èƒ½ (é¢„æœŸ ~73%)")
    print(f"  å‡†ç¡®ç‡: {result['accuracy']:.4f}")
    print(f"  F1åˆ†æ•°: {result['f1']:.4f}\n")

    result['training_time'] = train_time
    return classifier, result


def train_naive_bayes_v2(train_titles, train_labels, test_titles, test_labels):
    """è®­ç»ƒæœ´ç´ è´å¶æ–¯V2ä¼˜åŒ–ç‰ˆæœ¬."""
    print("[3] è®­ç»ƒæœ´ç´ è´å¶æ–¯ V2 (ä¼˜åŒ–ç‰ˆ)")
    print("-" * 80)
    print("ä¼˜åŒ–ç­–ç•¥:")
    print("  1. å¤šå±‚TF-IDF: è¯çº§(10K) + å­—ç¬¦çº§(5K)")
    print("  2. ç»Ÿè®¡ç‰¹å¾: 22ç»´ä¸“é—¨ç‰¹å¾")
    print("  3. ComplementNBç®—æ³•")
    print()

    start_time = time.time()

    classifier = NaiveBayesClassifierOptimized(
        max_features_word=10000,
        max_features_char=5000,
        word_ngram_range=(1, 3),
        char_ngram_range=(3, 5),
        alpha=0.5,
        model_path=get_model_path('naive_bayes_v2_optimized_model.pkl')
    )
    classifier.train(train_titles, train_labels)

    train_time = time.time() - start_time
    predictions = classifier.predict(test_titles)

    evaluator = ModelEvaluator()
    result = evaluator.evaluate_model(test_labels, predictions, "Naive Bayes V2 (Optimized)", verbose=False)

    print(f"\nâœ“ è®­ç»ƒå®Œæˆ: {train_time:.2f}ç§’")
    print(f"ğŸ“Š æ€§èƒ½ (é¢„æœŸ ~79%)")
    print(f"  å‡†ç¡®ç‡: {result['accuracy']:.4f}")
    print(f"  F1åˆ†æ•°: {result['f1']:.4f}\n")

    result['training_time'] = train_time
    return classifier, result


def train_word2vec_svm(train_titles, train_labels, test_titles, test_labels):
    """è®­ç»ƒWord2Vec+SVMæ¨¡å‹."""
    print("[4] è®­ç»ƒ Word2Vec + SVM")
    print("-" * 80)

    start_time = time.time()

    classifier = Word2VecSVMClassifier(
        vector_size=100,
        window=5,
        use_linear_svm=False,
        add_features=True,
        model_path=get_model_path('word2vec_svm')
    )
    classifier.train(train_titles, train_labels)

    train_time = time.time() - start_time
    predictions = classifier.predict(test_titles)

    evaluator = ModelEvaluator()
    result = evaluator.evaluate_model(test_labels, predictions, "Word2Vec + SVM", verbose=False)

    print(f"\nâœ“ è®­ç»ƒå®Œæˆ: {train_time:.2f}ç§’ ({train_time/60:.1f}åˆ†é’Ÿ)")
    print(f"ğŸ“Š æ€§èƒ½ (é¢„æœŸ ~83%)")
    print(f"  å‡†ç¡®ç‡: {result['accuracy']:.4f}")
    print(f"  F1åˆ†æ•°: {result['f1']:.4f}\n")

    result['training_time'] = train_time
    return classifier, result


def train_bert(train_titles, train_labels, test_titles, test_labels, epochs=3, batch_size=16):
    """è®­ç»ƒBERTæ¨¡å‹."""
    print("[5] è®­ç»ƒ BERT")
    print("-" * 80)
    print(f"é…ç½®: epochs={epochs}, batch_size={batch_size}")
    print()

    start_time = time.time()

    classifier = BERTClassifier(
        model_name='bert-base-uncased',
        max_length=64,
        model_path=get_model_path('best_bert_model.pt')
    )
    classifier.train(
        train_titles,
        train_labels,
        epochs=epochs,
        batch_size=batch_size,
        learning_rate=2e-5,
        warmup_steps=500
    )

    train_time = time.time() - start_time
    predictions = classifier.predict(test_titles, batch_size=batch_size)

    evaluator = ModelEvaluator()
    result = evaluator.evaluate_model(test_labels, predictions, "BERT", verbose=False)

    print(f"\nâœ“ è®­ç»ƒå®Œæˆ: {train_time:.2f}ç§’ ({train_time/60:.1f}åˆ†é’Ÿ)")
    print(f"ğŸ“Š æ€§èƒ½ (é¢„æœŸ ~88%)")
    print(f"  å‡†ç¡®ç‡: {result['accuracy']:.4f}")
    print(f"  F1åˆ†æ•°: {result['f1']:.4f}\n")

    result['training_time'] = train_time
    return classifier, result


def main():
    parser = argparse.ArgumentParser(
        description='Stage3 NaiveBayes Optimization + å…¨æ¨¡å‹è®­ç»ƒ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python train.py                  # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
  python train.py --model nb       # ä»…è®­ç»ƒæœ´ç´ è´å¶æ–¯
  python train.py --model w2v      # ä»…è®­ç»ƒWord2Vec+SVM
  python train.py --model bert     # ä»…è®­ç»ƒBERT
  python train.py --quick          # å¿«é€Ÿæµ‹è¯•
        """
    )
    parser.add_argument(
        '--model',
        type=str,
        choices=['nb', 'w2v', 'bert', 'all'],
        default='all',
        help='é€‰æ‹©è¦è®­ç»ƒçš„æ¨¡å‹'
    )
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿæµ‹è¯•æ¨¡å¼')
    parser.add_argument('--max-samples', type=int, default=None, help='æœ€å¤§è®­ç»ƒæ ·æœ¬æ•°')
    parser.add_argument('--epochs', type=int, default=3, help='BERTè®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=16, help='BERTæ‰¹æ¬¡å¤§å°')
    args = parser.parse_args()

    if args.quick and args.max_samples is None:
        args.max_samples = 10000
        if args.epochs == 3:
            args.epochs = 1

    print_banner()

    # æ£€æŸ¥Stage2æ¨¡å‹æ˜¯å¦å¯ç”¨
    if not HAS_STAGE2_MODELS and args.model in ['w2v', 'bert', 'all']:
        print("âŒ æ— æ³•å¯¼å…¥Stage2æ¨¡å‹ï¼Œè¯·æ£€æŸ¥Stage2_Traditional_Modelsç›®å½•")
        sys.exit(1)

    # åŠ è½½æ•°æ®
    print("[1] åŠ è½½æ•°æ®")
    print("-" * 80)
    train_titles, train_labels, test_titles, test_labels = DataLoader.prepare_dataset(
        get_data_path('positive.txt'),
        get_data_path('negative.txt'),
        get_data_path('testSet-1000.xlsx')
    )

    if args.max_samples and args.max_samples < len(train_titles):
        import random
        indices = list(range(len(train_titles)))
        random.shuffle(indices)
        indices = indices[:args.max_samples]
        train_titles = [train_titles[i] for i in indices]
        train_labels = [train_labels[i] for i in indices]
        print(f"âš  å¿«é€Ÿæµ‹è¯•: {len(train_titles)} æ ·æœ¬")

    print(f"âœ“ è®­ç»ƒæ•°æ®: {len(train_titles)} æ ·æœ¬")
    print(f"âœ“ æµ‹è¯•æ•°æ®: {len(test_titles)} æ ·æœ¬\n")

    # è®­ç»ƒæ¨¡å‹
    all_results = []
    classifiers_dict = {}
    total_start = time.time()

    # è®­ç»ƒæœ´ç´ è´å¶æ–¯
    if args.model in ['nb', 'all']:
        nb_v1_clf, nb_v1_result = train_naive_bayes_v1(train_titles, train_labels, test_titles, test_labels)
        all_results.append(nb_v1_result)
        classifiers_dict['Naive_Bayes_V1'] = nb_v1_clf

        nb_v2_clf, nb_v2_result = train_naive_bayes_v2(train_titles, train_labels, test_titles, test_labels)
        all_results.append(nb_v2_result)
        classifiers_dict['Naive_Bayes_V2_Optimized'] = nb_v2_clf

        # æ˜¾ç¤ºæœ´ç´ è´å¶æ–¯ä¼˜åŒ–æå‡
        improvement = (nb_v2_result['accuracy'] - nb_v1_result['accuracy']) * 100
        print("\n" + "="*80)
        print(" " * 25 + "æœ´ç´ è´å¶æ–¯ä¼˜åŒ–æ•ˆæœ")
        print("="*80)
        print(f"\nğŸ¯ å‡†ç¡®ç‡æå‡: +{improvement:.2f}ä¸ªç™¾åˆ†ç‚¹ ({nb_v1_result['accuracy']:.4f} â†’ {nb_v2_result['accuracy']:.4f})")
        print(f"ç›®æ ‡æå‡: +5.74ä¸ªç™¾åˆ†ç‚¹ (è¾¾æˆç‡: {improvement/5.74*100:.1f}%)\n")

    # è®­ç»ƒWord2Vec+SVM
    if args.model in ['w2v', 'all']:
        w2v_clf, w2v_result = train_word2vec_svm(train_titles, train_labels, test_titles, test_labels)
        all_results.append(w2v_result)
        classifiers_dict['Word2Vec_SVM'] = w2v_clf

    # è®­ç»ƒBERT
    if args.model in ['bert', 'all']:
        bert_clf, bert_result = train_bert(
            train_titles, train_labels, test_titles, test_labels,
            epochs=args.epochs,
            batch_size=args.batch_size
        )
        all_results.append(bert_result)
        classifiers_dict['BERT'] = bert_clf

    total_time = time.time() - total_start

    # å¯¹æ¯”æ‰€æœ‰æ¨¡å‹
    if len(all_results) > 1:
        print("\n" + "="*80)
        print(" " * 30 + "æ¨¡å‹å¯¹æ¯”")
        print("="*80 + "\n")
        ModelEvaluator.compare_models(all_results)

        # ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–
        visualizer = ResultVisualizer()
        visualizer.plot_comparison(all_results, save_path=get_output_path('model_comparison.png'))
        visualizer.plot_confusion_matrices(all_results, save_path=get_output_path('confusion_matrices.png'))
        print(f"\nâœ“ å¯è§†åŒ–å·²ä¿å­˜åˆ°: {get_output_path('')}")

    # ç”Ÿæˆ t-SNE å¯è§†åŒ–
    print("\n" + "="*80)
    print(" " * 28 + "ç”Ÿæˆ t-SNE å¯è§†åŒ–")
    print("="*80 + "\n")

    visualizer = ResultVisualizer()
    for model_name, classifier in classifiers_dict.items():
        try:
            feature_vectors = classifier.get_feature_vectors(test_titles)
            visualizer.visualize_embeddings_tsne(
                feature_vectors,
                test_labels,
                model_name,
                save_path=get_output_path(f'tsne_{model_name}.png')
            )
        except Exception as e:
            print(f"âš ï¸  ä¸º {model_name} ç”Ÿæˆ t-SNE å›¾å¤±è´¥: {str(e)}")

    # ä¿å­˜ç»“æœæ‘˜è¦
    results_file = get_output_path('training_results.txt')
    with open(results_file, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write(" "*20 + "Stage3 NaiveBayes Optimization - è®­ç»ƒç»“æœ\n")
        f.write("="*80 + "\n\n")
        f.write(f"è®­ç»ƒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        for result in all_results:
            f.write("-"*80 + "\n")
            f.write(f"æ¨¡å‹: {result['model']}\n")
            f.write("-"*80 + "\n")
            f.write(f"å‡†ç¡®ç‡: {result['accuracy']:.4f}\n")
            f.write(f"ç²¾ç¡®ç‡: {result['precision']:.4f}\n")
            f.write(f"å¬å›ç‡: {result['recall']:.4f}\n")
            f.write(f"F1åˆ†æ•°: {result['f1']:.4f}\n")
            if 'training_time' in result:
                f.write(f"è®­ç»ƒæ—¶é—´: {result['training_time']:.2f}ç§’\n")
            f.write("\n")

    print("\n" + "="*80)
    print(" " * 30 + "è®­ç»ƒå®Œæˆ")
    print("="*80)
    print(f"\næ€»è€—æ—¶: {total_time:.2f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
    print(f"æ¨¡å‹ä¿å­˜: {get_model_path('')}")
    print(f"ç»“æœä¿å­˜: {results_file}")
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    if len(all_results) > 1:
        print(f"  - model_comparison.png")
        print(f"  - confusion_matrices.png")
    for model_name in classifiers_dict.keys():
        print(f"  - tsne_{model_name}.png")
    print(f"  - training_results.txt")
    print("\n" + "="*80 + "\n")


if __name__ == "__main__":
    main()
