#!/usr/bin/env python3
"""
Task2 å®Œæ•´è¯„ä¼° - æ‰€æœ‰é˜¶æ®µæ¨¡å‹å¯¹æ¯”
=====================================
è¯„ä¼°Stage1åˆ°Stage4çš„æ‰€æœ‰è®­ç»ƒæ¨¡å‹
"""

import os
import sys
from pathlib import Path

# æ·»åŠ æ‰€æœ‰é˜¶æ®µè·¯å¾„
base_dir = Path(__file__).parent
for stage in ['Stage1_Foundation', 'Stage2_Traditional_Models',
              'Stage3_NaiveBayes_Optimization', 'Stage4_BERT_Optimization']:
    sys.path.insert(0, str(base_dir / 'stages' / stage))

from data_loader import DataLoader
from evaluator import ModelEvaluator
from visualizer import ResultVisualizer
import config

print("="*80)
print(" " * 25 + "Task2 å®Œæ•´è¯„ä¼°")
print(" " * 20 + "æ‰€æœ‰é˜¶æ®µæ¨¡å‹æ€§èƒ½å¯¹æ¯”")
print("="*80)

# åŠ è½½æµ‹è¯•æ•°æ®
print("\n[1] åŠ è½½æµ‹è¯•æ•°æ®")
print("-" * 80)

train_titles, train_labels, test_titles, test_labels = DataLoader.prepare_dataset(
    config.get_data_path('positive.txt'),
    config.get_data_path('negative.txt'),
    config.get_data_path('testSet-1000.xlsx')
)

print(f"âœ“ æµ‹è¯•é›†: {len(test_titles)} æ ·æœ¬")
print(f"  - æ­£æ ·æœ¬: {sum(test_labels)} ({sum(test_labels)/len(test_labels)*100:.1f}%)")
print(f"  - è´Ÿæ ·æœ¬: {len(test_labels)-sum(test_labels)} ({(len(test_labels)-sum(test_labels))/len(test_labels)*100:.1f}%)")

# å®šä¹‰æ‰€æœ‰æ¨¡å‹
models_to_evaluate = [
    # Stage2/3 - ä¼ ç»Ÿæ¨¡å‹
    {
        'name': 'Naive Bayes (åŸå§‹)',
        'stage': 'Stage3',
        'type': 'nb_original',
        'path': base_dir / 'stages/Stage3_NaiveBayes_Optimization/models/naive_bayes_original_model.pkl'
    },
    {
        'name': 'Naive Bayes (ä¼˜åŒ–)',
        'stage': 'Stage3',
        'type': 'nb_optimized',
        'path': base_dir / 'stages/Stage3_NaiveBayes_Optimization/models/naive_bayes_optimized_model.pkl'
    },
    {
        'name': 'Word2Vec + SVM',
        'stage': 'Stage3',
        'type': 'word2vec_svm',
        'path': base_dir / 'stages/Stage3_NaiveBayes_Optimization/models/word2vec_svm_svm.pkl'
    },
    # Stage4 - BERTæ¨¡å‹
    {
        'name': 'BERT Baseline',
        'stage': 'Stage4',
        'type': 'bert',
        'path': base_dir / 'stages/Stage4_BERT_Optimization/models/bert_model.pt'
    },
    {
        'name': 'SciBERT + Focal Loss',
        'stage': 'Stage4',
        'type': 'scibert',
        'path': base_dir / 'stages/Stage4_BERT_Optimization/models/scibert_optimized_model.pt'
    },
    {
        'name': 'DeBERTa-v3',
        'stage': 'Stage4',
        'type': 'deberta',
        'path': base_dir / 'stages/Stage4_BERT_Optimization/models/deberta_optimized_model.pt'
    }
]

# è¯„ä¼°æ‰€æœ‰æ¨¡å‹
print("\n[2] è¯„ä¼°æ‰€æœ‰æ¨¡å‹")
print("-" * 80 + "\n")

results = []
evaluator = ModelEvaluator()

for i, model_info in enumerate(models_to_evaluate, 1):
    print(f"\n[{i}/{len(models_to_evaluate)}] è¯„ä¼°: {model_info['name']}")
    print(f"{'='*70}")

    model_path = model_info['path']

    # æ£€æŸ¥æ–‡ä»¶
    if not model_path.exists():
        print(f"âš ï¸  æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        continue

    model_size_mb = model_path.stat().st_size / (1024 * 1024)
    print(f"æ¨¡å‹æ–‡ä»¶: {model_path.name}")
    print(f"æ¨¡å‹å¤§å°: {model_size_mb:.1f} MB")
    print(f"æ‰€å±é˜¶æ®µ: {model_info['stage']}")

    try:
        # æ ¹æ®æ¨¡å‹ç±»å‹åŠ è½½
        if model_info['type'] == 'nb_original':
            from naive_bayes_classifier import NaiveBayesClassifier
            classifier = NaiveBayesClassifier(model_path=str(model_path))
            classifier.load_model()

        elif model_info['type'] == 'nb_optimized':
            sys.path.insert(0, str(base_dir / 'stages/Stage3_NaiveBayes_Optimization'))
            from naive_bayes_classifier_optimized import NaiveBayesClassifierOptimized
            classifier = NaiveBayesClassifierOptimized(model_path=str(model_path))
            classifier.load_model()

        elif model_info['type'] == 'word2vec_svm':
            from word2vec_svm_classifier import Word2VecSVMClassifier
            base_path = str(model_path).replace('_svm.pkl', '')
            classifier = Word2VecSVMClassifier(model_path=base_path)
            classifier.load_model()

        elif model_info['type'] in ['bert', 'scibert', 'deberta']:
            sys.path.insert(0, str(base_dir / 'stages/Stage4_BERT_Optimization'))

            if model_info['type'] == 'bert':
                from bert_classifier import BERTClassifier
                classifier = BERTClassifier(
                    model_name='bert-base-uncased',
                    max_length=64,
                    model_path=str(model_path)
                )
            else:
                from bert_classifier_optimized import OptimizedBERTClassifier
                if model_info['type'] == 'scibert':
                    model_name = 'allenai/scibert_scivocab_uncased'
                else:  # deberta
                    model_name = 'microsoft/deberta-v3-base'

                classifier = OptimizedBERTClassifier(
                    model_name=model_name,
                    model_path=str(model_path)
                )

            classifier.load_model()

        # é¢„æµ‹
        print("\nå¼€å§‹é¢„æµ‹...")
        predictions = classifier.predict(test_titles)

        # è¯„ä¼°
        result = evaluator.evaluate_model(
            test_labels,
            predictions,
            model_info['name'],
            verbose=False
        )

        # æ·»åŠ é¢å¤–ä¿¡æ¯
        result['stage'] = model_info['stage']
        result['model_size_mb'] = model_size_mb
        results.append(result)

        print(f"\nâœ“ è¯„ä¼°å®Œæˆ")
        print(f"  å‡†ç¡®ç‡: {result['accuracy']:.4f} ({result['accuracy']*100:.2f}%)")
        print(f"  F1åˆ†æ•°: {result['f1']:.4f}")

    except Exception as e:
        print(f"\nâš ï¸  è¯„ä¼°å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        continue

# ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
if not results:
    print("\nâš ï¸  æ²¡æœ‰æˆåŠŸè¯„ä¼°ä»»ä½•æ¨¡å‹")
    sys.exit(1)

print("\n" + "="*80)
print(" " * 25 + "æ‰€æœ‰æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
print("="*80 + "\n")

# æŒ‰å‡†ç¡®ç‡æ’åº
results_sorted = sorted(results, key=lambda x: x['accuracy'], reverse=True)

# æ‰“å°è¡¨æ ¼
header = f"{'æ’å':<6} {'æ¨¡å‹':<30} {'é˜¶æ®µ':<10} {'å‡†ç¡®ç‡':>10} {'F1åˆ†æ•°':>10} {'å¤§å°(MB)':>12}"
print(header)
print("-" * 80)

for rank, result in enumerate(results_sorted, 1):
    row = f"{rank:<6} "
    row += f"{result['model']:<30} "
    row += f"{result['stage']:<10} "
    row += f"{result['accuracy']:>9.4f} "
    row += f"{result['f1']:>9.4f} "
    row += f"{result['model_size_mb']:>11.1f}"
    print(row)

print("-" * 80)

# æœ€ä½³æ¨¡å‹
best_model = results_sorted[0]
print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model['model']}")
print(f"   å‡†ç¡®ç‡: {best_model['accuracy']:.4f} ({best_model['accuracy']*100:.2f}%)")
print(f"   F1åˆ†æ•°: {best_model['f1']:.4f}")
print(f"   æ‰€å±é˜¶æ®µ: {best_model['stage']}")

# æŒ‰é˜¶æ®µåˆ†ç»„ç»Ÿè®¡
print(f"\n{'='*80}")
print(" " * 25 + "å„é˜¶æ®µæœ€ä½³æ¨¡å‹")
print("="*80 + "\n")

stages = {}
for result in results:
    stage = result['stage']
    if stage not in stages or result['accuracy'] > stages[stage]['accuracy']:
        stages[stage] = result

for stage in sorted(stages.keys()):
    result = stages[stage]
    print(f"{stage}:")
    print(f"  æ¨¡å‹: {result['model']}")
    print(f"  å‡†ç¡®ç‡: {result['accuracy']:.4f} ({result['accuracy']*100:.2f}%)")
    print(f"  F1åˆ†æ•°: {result['f1']:.4f}")
    print()

# ä¿å­˜ç»“æœ
output_dir = base_dir / 'output'
output_dir.mkdir(exist_ok=True)

# JSONæ ¼å¼
import json
json_path = output_dir / 'all_stages_comparison.json'
with open(json_path, 'w', encoding='utf-8') as f:
    json.dump(results, f, indent=2, ensure_ascii=False, default=str)
print(f"âœ“ JSONç»“æœå·²ä¿å­˜: {json_path}")

# æ–‡æœ¬æŠ¥å‘Š
report_path = output_dir / 'all_stages_comparison_report.txt'
with open(report_path, 'w', encoding='utf-8') as f:
    f.write("="*80 + "\n")
    f.write(" " * 25 + "Task2 å®Œæ•´è¯„ä¼°æŠ¥å‘Š\n")
    f.write(" " * 20 + "æ‰€æœ‰é˜¶æ®µæ¨¡å‹æ€§èƒ½å¯¹æ¯”\n")
    f.write("="*80 + "\n\n")

    f.write(f"è¯„ä¼°æ—¥æœŸ: 2025-12-08\n")
    f.write(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_titles)}\n")
    f.write(f"è¯„ä¼°æ¨¡å‹æ•°: {len(results)}\n\n")

    f.write("## å®Œæ•´æ’å\n\n")
    f.write(f"{'æ’å':<6} {'æ¨¡å‹':<35} {'é˜¶æ®µ':<12} {'å‡†ç¡®ç‡':>12} {'F1åˆ†æ•°':>12}\n")
    f.write("-" * 85 + "\n")

    for rank, result in enumerate(results_sorted, 1):
        f.write(f"{rank:<6} ")
        f.write(f"{result['model']:<35} ")
        f.write(f"{result['stage']:<12} ")
        f.write(f"{result['accuracy']:>11.4f} ")
        f.write(f"{result['f1']:>11.4f}\n")

    f.write("\n## æœ€ä½³æ¨¡å‹\n\n")
    f.write(f"**{best_model['model']}** ({best_model['stage']})\n")
    f.write(f"- å‡†ç¡®ç‡: {best_model['accuracy']:.4f} ({best_model['accuracy']*100:.2f}%)\n")
    f.write(f"- F1åˆ†æ•°: {best_model['f1']:.4f}\n")
    f.write(f"- æ¨¡å‹å¤§å°: {best_model['model_size_mb']:.1f} MB\n\n")

    f.write("## å„é˜¶æ®µæœ€ä½³\n\n")
    for stage in sorted(stages.keys()):
        result = stages[stage]
        f.write(f"### {stage}\n\n")
        f.write(f"æ¨¡å‹: {result['model']}\n")
        f.write(f"- å‡†ç¡®ç‡: {result['accuracy']:.4f}\n")
        f.write(f"- F1åˆ†æ•°: {result['f1']:.4f}\n\n")

print(f"âœ“ æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

# ç”Ÿæˆå¯è§†åŒ–
print(f"\n{'='*80}")
print(" " * 30 + "ç”Ÿæˆå¯è§†åŒ–")
print("="*80 + "\n")

visualizer = ResultVisualizer()

# æ¨¡å‹å¯¹æ¯”å›¾
comparison_path = output_dir / 'all_models_comparison.png'
visualizer.plot_comparison(results, save_path=str(comparison_path))
print(f"âœ“ æ¨¡å‹å¯¹æ¯”å›¾: {comparison_path}")

# æ··æ·†çŸ©é˜µ
confusion_path = output_dir / 'all_models_confusion_matrices.png'
visualizer.plot_confusion_matrices(results[:4], save_path=str(confusion_path))  # åªæ˜¾ç¤ºå‰4ä¸ª
print(f"âœ“ æ··æ·†çŸ©é˜µ (å‰4ä¸ªæ¨¡å‹): {confusion_path}")

print("\n" + "="*80)
print(" " * 30 + "è¯„ä¼°å®Œæˆ")
print("="*80)
print("\nç”Ÿæˆçš„æ–‡ä»¶:")
print(f"  - all_stages_comparison.json")
print(f"  - all_stages_comparison_report.txt")
print(f"  - all_models_comparison.png")
print(f"  - all_models_confusion_matrices.png")
print("\n" + "="*80 + "\n")
